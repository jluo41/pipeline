{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "from pprint import pprint\n",
    "KEY = 'utils'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0]\n",
    "print(WORKSPACE_PATH); os.chdir(WORKSPACE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(WORKSPACE_PATH)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 optional\n",
    "import os \n",
    "\n",
    "AWS_ACCESS_KEY_ID=\"ASIAYPXMS72EPD3IPMX5\"\n",
    "AWS_SECRET_ACCESS_KEY=\"i6DbEiWADy+9U/MypOJwe+HV+w61f6SSmTk8BlZb\"\n",
    "AWS_SESSION_TOKEN=\"IQoJb3JpZ2luX2VjEHgaCXVzLWVhc3QtMSJHMEUCIHbXPte7WTWZu2mCDOTdFHLlNLX7cRWRr0qvOjDrsThtAiEA/FdQ3P+ATtlFFdyhri4WBheWIH4wa1k3Yv2ardt861wq5gIIIRAAGgw1ODM1Mzc5ODMxMTIiDF8r8wtLMieu7Qo5hSrDAs/VcuGC1Gh5xBMohclS9OhyY4vIN1eTi3OW9LlpzONrAw0fFzCKLdhLiq/4lbaGu9Z7y632zoBK1Gk7W7HNqDidlOHGE6gt/8/F4uCcksbaSGl8SR3DSRp+MeMhnSeIwcGri46yMlVZCKroWYajJIZbClu9Z9SN2R8x1k9pjsuAtWUHbjHzG3KozuPa20YCYp5lOiF/CNbgjHwv1GWbY4qE91UvENr4SdhVBc5FAMDzBW09TLQffRhAd4HLX8jEdMxgtrSya8xbwarzTjPhzwurbi9znOCtEC1Qtjr6wvSZlcdCyKsNkfSaRi8eolQFw2wEDM3gilRdDPET5G+ZXBQMOnTOLNxmkYriiOHq7FjL2haGkUgD7wa8X7GBi8CLhtV+C8/pllAzc7o4HtuuLny9vXC5rhNUZcDQTAcSAs3xvLXAMKr638AGOqcBpuy8mYLVuRNZshHDwzBB5Ub7YKAusxI0kPrKI956gQuKMICGG76hnF08edwXVaKP9fHEfjZRlN8M9u6RQRNtFPcKz4/gacIvtZvDLFgb79qOpgqnUkFTH0b9y0f6OQDwdDhcFdPNXcFsGzMgC/PI009wIOJPzq6sdMeUBocD2dmbpEb+DMK1SHw5Gi0sEnWSnkalOCCn4xnvjvOAJ2MnVp6GMYb7TQI=\"\n",
    "\n",
    "try:\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "    os.environ[\"AWS_SESSION_TOKEN\"] = AWS_SESSION_TOKEN\n",
    "\n",
    "    import boto3\n",
    "    import sagemaker\n",
    "    from sagemaker import get_execution_role\n",
    "    from sagemaker.session import Session\n",
    "\n",
    "    sess = sagemaker.Session()\n",
    "    S3_CLIENT = boto3.client('s3')\n",
    "    account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "    region = sess.boto_session.region_name\n",
    "    role = get_execution_role()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'S3 is not available: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = {\n",
    "    'DATA_RAW': f'_Data/0-Data_Raw',\n",
    "    'DATA_RFT': f'_Data/1-Data_RFT',\n",
    "    'DATA_CASE': f'_Data/2-Data_CASE',\n",
    "    'DATA_AIDATA': f'_Data/3-Data_AIDATA',\n",
    "    'DATA_EXTERNAL': f'code/external',\n",
    "    'CODE_FN': f'code/pipeline', \n",
    "}\n",
    "\n",
    "import sys \n",
    "assert os.path.exists(SPACE['CODE_FN']), f'{SPACE[\"CODE_FN\"]} not found'\n",
    "\n",
    "print(SPACE['CODE_FN'])\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "\n",
    "from recfldtkn.check import update_and_assert_CaseInfo\n",
    "from recfldtkn.check import retrive_pipeline_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import cleanup_folder, upload_s3_folder_or_file, delete_s3_folder_or_file\n",
    "\n",
    "########################### take as given ###########################\n",
    "BUCKET_NAME = 'rxinform-analytics-personalization'\n",
    "S3_BASE_PATH = '000-SAGEMAKER-TRAINING-PIPELINE/REMOTE_REPO/'\n",
    "########################### take as given ###########################\n",
    "\n",
    "LOCAL_CODE_FODER = 'code'\n",
    "\n",
    "\n",
    "file_counts = delete_s3_folder_or_file(\n",
    "    S3_CLIENT=S3_CLIENT,\n",
    "    BUCKET_NAME=BUCKET_NAME,\n",
    "    S3_BASE_PATH = S3_BASE_PATH,\n",
    "    local_folder_or_file_path=LOCAL_CODE_FODER,\n",
    ")\n",
    "\n",
    "file_counts = cleanup_folder(LOCAL_CODE_FODER)\n",
    "\n",
    "file_counts = upload_s3_folder_or_file(\n",
    "    S3_CLIENT=S3_CLIENT,\n",
    "    BUCKET_NAME=BUCKET_NAME,\n",
    "    S3_BASE_PATH = S3_BASE_PATH,\n",
    "    local_folder_or_file_path=LOCAL_CODE_FODER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0-Data_Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import cleanup_folder, upload_s3_folder_or_file\n",
    "\n",
    "########################### take as given ###########################\n",
    "BUCKET_NAME = 'rxinform-analytics-personalization'\n",
    "S3_BASE_PATH = '000-SAGEMAKER-TRAINING-PIPELINE/REMOTE_REPO/'\n",
    "########################### take as given ###########################\n",
    "\n",
    "LOCAL_CODE_FODER = '_Data/0-Data_Raw/20250408_Jardiance'\n",
    "file_counts = cleanup_folder(LOCAL_CODE_FODER)\n",
    "\n",
    "file_counts = upload_s3_folder_or_file(\n",
    "    S3_CLIENT=S3_CLIENT,\n",
    "    BUCKET_NAME=BUCKET_NAME,\n",
    "    S3_BASE_PATH = S3_BASE_PATH,\n",
    "    local_folder_or_file_path=LOCAL_CODE_FODER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import cleanup_folder, upload_s3_folder_or_file\n",
    "\n",
    "\n",
    "#################################\n",
    "# import pandas as pd\n",
    "# LOCAL_CODE_FODER = './_Data/0-Data_Raw/20250408_Jardiance'\n",
    "# path = os.path.join(LOCAL_CODE_FODER, 'combined_deidentified_20250404144405.parquet')\n",
    "# df = pd.read_parquet(path)\n",
    "# df = df.iloc[:5000]\n",
    "# df.to_parquet(path.replace('.parquet', '_small.parquet'))\n",
    "#################################\n",
    "\n",
    "\n",
    "\n",
    "########################### take as given ###########################\n",
    "BUCKET_NAME = 'rxinform-analytics-personalization'\n",
    "S3_BASE_PATH = '000-SAGEMAKER-TRAINING-PIPELINE/REMOTE_REPO/'\n",
    "########################### take as given ###########################\n",
    "\n",
    "LOCAL_CODE_FODER = '_Data/0-Data_Raw/202505_Demo'\n",
    "file_counts = cleanup_folder(LOCAL_CODE_FODER)\n",
    "\n",
    "file_counts = upload_s3_folder_or_file(\n",
    "    S3_CLIENT=S3_CLIENT,\n",
    "    BUCKET_NAME=BUCKET_NAME,\n",
    "    S3_BASE_PATH = S3_BASE_PATH,\n",
    "    local_folder_or_file_path=LOCAL_CODE_FODER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_CODE_FODER = '_Data/0-Data_Raw/20250416_AprBanditSMS'\n",
    "file_counts = cleanup_folder(LOCAL_CODE_FODER)\n",
    "\n",
    "file_counts = upload_s3_folder_or_file(\n",
    "    S3_CLIENT=S3_CLIENT,\n",
    "    BUCKET_NAME=BUCKET_NAME,\n",
    "    S3_BASE_PATH = S3_BASE_PATH,\n",
    "    local_folder_or_file_path=LOCAL_CODE_FODER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from recfldtkn.base import cleanup_folder, upload_s3_folder_or_file\n",
    "\n",
    "# ########################### take as given ###########################\n",
    "# BUCKET_NAME = 'rxinform-analytics-personalization'\n",
    "# S3_BASE_PATH = '000-SAGEMAKER-TRAINING-PIPELINE/REMOTE_REPO/'\n",
    "# ########################### take as given ###########################\n",
    "\n",
    "# LOCAL_CODE_FODER = './_Data/0-Data_Raw/20250408_Jardiance'\n",
    "# file_counts = cleanup_folder(LOCAL_CODE_FODER)\n",
    "\n",
    "# file_counts = upload_s3_folder_or_file(\n",
    "#     S3_CLIENT=S3_CLIENT,\n",
    "#     BUCKET_NAME=BUCKET_NAME,\n",
    "#     S3_BASE_PATH = S3_BASE_PATH,\n",
    "#     local_folder_or_file_path=LOCAL_CODE_FODER,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Data_RFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KEY_DATA_FOLDER = '000-Sagemaker-Pipeline/_Data_from_s3/_Data'\n",
    "KEY_DATA_RAW_FOLDER = os.path.join(PROJECT_FODLER, '_Data', '1-Data_RFT') # f'{KEY_DATA_FOLDER}/0-Data_Raw'\n",
    "\n",
    "LOCAL_FOLDER = '_Data/1-Data_RFT'\n",
    "\n",
    "print(KEY_DATA_RAW_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KEY_DATA_RAW_FOLDER)\n",
    "\n",
    "###### maybe do not need to delete the folder for next time. \n",
    "delete_s3_folder(s3_client, CUSTOM_BUCKET, KEY_DATA_RAW_FOLDER) \n",
    "######\n",
    "\n",
    "\n",
    "upload_s3_folder(s3_client, LOCAL_FOLDER, CUSTOM_BUCKET, KEY_DATA_RAW_FOLDER)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Data_CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_BUCKET = \"rxinform-analytics-personalization\"\n",
    "PROJECT_FODLER = '000-Sagemaker-Pipeline/_Data_from_s3'\n",
    "\n",
    "# KEY_DATA_FOLDER = '000-Sagemaker-Pipeline/_Data_from_s3/_Data'\n",
    "KEY_DATA_RAW_FOLDER = os.path.join(PROJECT_FODLER, '_Data', '2-Data_CASE') # f'{KEY_DATA_FOLDER}/0-Data_Raw'\n",
    "\n",
    "LOCAL_FOLDER = '_Data/2-Data_CASE'\n",
    "\n",
    "print(KEY_DATA_RAW_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KEY_DATA_RAW_FOLDER)\n",
    "\n",
    "###### maybe do not need to delete the folder for next time. \n",
    "delete_s3_folder(s3_client, CUSTOM_BUCKET, KEY_DATA_RAW_FOLDER) \n",
    "######\n",
    "\n",
    "\n",
    "upload_s3_folder(s3_client, LOCAL_FOLDER, CUSTOM_BUCKET, KEY_DATA_RAW_FOLDER)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Data_AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_BUCKET = \"rxinform-analytics-personalization\"\n",
    "PROJECT_FODLER = '000-Sagemaker-Pipeline/_Data_from_s3'\n",
    "\n",
    "\n",
    "# KEY_DATA_FOLDER = '000-Sagemaker-Pipeline/_Data_from_s3/_Data'\n",
    "KEY_DATA_RAW_FOLDER = os.path.join(PROJECT_FODLER, '_Data', '3-Data_AIData') # f'{KEY_DATA_FOLDER}/0-Data_Raw'\n",
    "\n",
    "LOCAL_FOLDER = '_Data/3-Data_AIData'\n",
    "\n",
    "print(KEY_DATA_RAW_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KEY_DATA_RAW_FOLDER)\n",
    "\n",
    "###### maybe do not need to delete the folder for next time. \n",
    "delete_s3_folder(s3_client, CUSTOM_BUCKET, KEY_DATA_RAW_FOLDER) \n",
    "######\n",
    "\n",
    "\n",
    "upload_s3_folder(s3_client, LOCAL_FOLDER, CUSTOM_BUCKET, KEY_DATA_RAW_FOLDER)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_CODE_FODER = './code'\n",
    "cleanup_folder(LOCAL_CODE_FODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY_CODE_FOLDER = '000-Sagemaker-Pipeline/_Data_from_s3/code'\n",
    "KEY_CODE_FOLDER = os.path.join(PROJECT_FODLER, 'code')\n",
    "print(KEY_CODE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KEY_CODE_FOLDER)\n",
    "delete_s3_folder(s3_client, CUSTOM_BUCKET, KEY_CODE_FOLDER) \n",
    "upload_s3_folder(s3_client, LOCAL_CODE_FODER, CUSTOM_BUCKET, KEY_CODE_FOLDER)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_FODLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_FODER = './_Model'\n",
    "cleanup_folder(LOCAL_MODEL_FODER)\n",
    "\n",
    "# KEY_CODE_FOLDER = '000-Sagemaker-Pipeline/_Data_from_s3/code'\n",
    "KEY_MODEL_FOLDER = os.path.join(PROJECT_FODLER, '_Model')\n",
    "print(KEY_MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_MODEL_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_FODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(KEY_MODEL_FOLDER)\n",
    "\n",
    "# ###### maybe do not need to delete the folder for next time. \n",
    "# delete_s3_folder(s3_client, CUSTOM_BUCKET, KEY_MODEL_FOLDER) \n",
    "# ######\n",
    "\n",
    "\n",
    "# upload_s3_folder(s3_client, LOCAL_MODEL_FODER, CUSTOM_BUCKET, KEY_MODEL_FOLDER)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ENDPOINT_PATH = '_Model/vSMSBanditModel'\n",
    "# MODEL_ENDPOINT_PATH = '_Model/vSMSBanditModel'\n",
    "# MODEL_ENDPOINT_PATH = '_Model/vSMSXgboostCTRModel' # change this to the new model version \n",
    "MODEL_ENDPOINT_PATH = '_Model/sms.bandit.0.0.1' # change this to the new model version \n",
    "\n",
    "clean_and_archive_model(MODEL_ENDPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model tar.gz to S3\n",
    "model_tar_path = MODEL_ENDPOINT_PATH + '.tar.gz'\n",
    "model_tar_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_model_path = os.path.join(PROJECT_FODLER, model_tar_path)\n",
    "key_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Deleting existing model at s3://{CUSTOM_BUCKET}/{key_model_path}\")\n",
    "    s3_client.delete_object(\n",
    "        Bucket=CUSTOM_BUCKET,\n",
    "        Key=key_model_path\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Note: No existing model found or error during deletion: {str(e)}\")\n",
    "\n",
    "\n",
    "print(f\"Uploading model to s3://{CUSTOM_BUCKET}/{key_model_path}\")\n",
    "s3_client.upload_file(\n",
    "    model_tar_path,\n",
    "    CUSTOM_BUCKET, \n",
    "    key_model_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}