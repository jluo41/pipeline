{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af9e5f6-12ce-4ac4-95d8-aa091997589e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Space Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753d321c-8789-48cb-ad8e-cf23f78f8d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is the file that implements a flask server to do inferences. It's the file that you will modify to\n",
    "# implement the scoring for your own algorithm.\n",
    "\n",
    "from __future__ import print_function\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "# import flask\n",
    "import logging\n",
    "import datasets\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime \n",
    "# from flask import Flask, request, jsonify, Response\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e485a37-59c4-4326-afdd-fa3f5c9a187f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# parent_directory = '/dbfs/FileStore/Sid_Files/Deployment-v1119'\n",
    "parent_directory = '.'\n",
    "\n",
    "os.chdir(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9c96ac-16ab-492f-bbe5-419884775c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_inference_SPACE(SPACE, MODEL_ENDPOINT):\n",
    "\n",
    "    assert 'MODEL_ROOT' in SPACE, \"Invalid SPACE: missing MODEL_ROOT\"   \n",
    "    \n",
    "    # pipeline from ModelVersion/pipeline\n",
    "    SPACE['CODE_FN'] = os.path.join(SPACE['MODEL_ROOT'], MODEL_ENDPOINT, 'pipeline')\n",
    "    assert os.path.exists(SPACE['CODE_FN']), f\"Invalid CODE_FN: {SPACE['CODE_FN']}\"\n",
    "    # external from ModelVersion/external\n",
    "    SPACE['DATA_EXTERNAL'] = os.path.join(SPACE['MODEL_ROOT'], MODEL_ENDPOINT, 'external')\n",
    "    assert os.path.exists(SPACE['DATA_EXTERNAL']), f\"Invalid DATA_EXTERNAL: {SPACE['DATA_EXTERNAL']}\"\n",
    "\n",
    "    SPACE['DATA_RAW'] = os.path.join(SPACE['MODEL_ROOT'], MODEL_ENDPOINT)\n",
    "    assert os.path.exists(SPACE['DATA_RAW']), f\"Invalid DATA_EXTERNAL: {SPACE['DATA_RAW']}\"\n",
    "\n",
    "    SPACE['DATA_INFERENCE'] = os.path.join(SPACE['MODEL_ROOT'], MODEL_ENDPOINT, 'inference')\n",
    "    assert os.path.exists(SPACE['DATA_INFERENCE']), f\"Invalid DATA_EXTERNAL: {SPACE['DATA_INFERENCE']}\"\n",
    "\n",
    "    SPACE['MODEL_ENDPOINT'] = MODEL_ENDPOINT\n",
    "    return SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "279949ba-10ba-4e52-882f-4f38b761b6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ############################\n",
    "# # ----------- environment for Estimator.deploy() -----------\n",
    "# MODEL_ROOT          = '../../../_Model'           # '/opt/ml/model' in sagemaker\n",
    "# MODEL_ENDPOINT      = 'vTestWeight' # 'vTestCGMFull'\n",
    "# INF_CohortName      = '20241013_InferencePttSampleV0'\n",
    "# INF_OneCohortArgs   = {'CohortLabel': 9,\n",
    "#                        'CohortName': '20241013_InferencePttSampleV0',\n",
    "#                        'FolderPath': '$DATA_RAW$/inference/',\n",
    "#                        'SourcePath': 'patient_sample',\n",
    "#                        'Source2CohortName': 'InferencePttSampleV0'}\n",
    "# INF_CFArgs          = None \n",
    "# INF_Args            = None \n",
    "\n",
    "# PostFnName = \"PostFn_NaiveForUniLabelPred\" # \"EngagementPredToLabel\"\n",
    "# TrigFnName = 'TriggerFn_WeightEntry_v1211' \n",
    "# MetaFnName = 'MetaFn_None'\n",
    "\n",
    "# POST_PROCESS_SCRIPT = None # 'pipeline/inference/post_process.py' # by default, use this script\n",
    "# LoggerLevel         = \"INFO\"\n",
    "# ############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "MODEL_ROOT          = '../../../_Model'           # '/opt/ml/model' in sagemaker\n",
    "MODEL_ENDPOINT      = 'vTestCGMFull' # 'vTestWeight' # \n",
    "INF_CohortName      = '20241013_InferencePttSampleV0'\n",
    "INF_OneCohortArgs   = {'CohortLabel': 9,\n",
    "                       'CohortName': '20241013_InferencePttSampleV0',\n",
    "                       'FolderPath': '$DATA_RAW$/inference/',\n",
    "                       'SourcePath': 'patient_sample',\n",
    "                       'Source2CohortName': 'InferencePttSampleV0'}\n",
    "INF_CFArgs          = ['cf.TargetCGM_Bf24H'] \n",
    "INF_Args            = {'GEN_Args': {\n",
    "                            'num_first_tokens_for_gen': 289,\n",
    "                            'max_new_tokens': 24,\n",
    "                            'do_sample': False,\n",
    "                            'items_list': ['hist', 'pred', 'logit_scores']}\n",
    "                      } \n",
    "MetaFnName = 'MetaFn_None'\n",
    "TrigFnName = 'TriggerFn_CGM5MinEntry_v1211' \n",
    "PostFnName = \"PostFn_WithCGMPred_v1210\" # \"EngagementPredToLabel\"\n",
    "POST_PROCESS_SCRIPT = None # 'pipeline/inference/post_process.py' # by default, use this script\n",
    "LoggerLevel         = \"INFO\"\n",
    "###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# # image your are in the sagemaker container\n",
    "MODEL_ROOT        = os.environ.get('MODEL_ROOT', MODEL_ROOT)\n",
    "MODEL_ENDPOINT    = os.environ.get('MODEL_ENDPOINT', MODEL_ENDPOINT)\n",
    "INF_CohortName    = os.environ.get('INF_COHORT_NAME', INF_CohortName)\n",
    "INF_CohortArgs    = os.environ.get('INF_COHORT_ARGS', INF_OneCohortArgs)\n",
    "InputCFArgs_ForInference = os.environ.get('INF_CFArgs', INF_CFArgs)\n",
    "InferenceArgs     = os.environ.get('INF_Args', INF_Args)   \n",
    "\n",
    "PostFnName = os.environ.get('PostFnName', PostFnName)\n",
    "TrigFnName = os.environ.get('TrigFnName', TrigFnName)\n",
    "MetaFnName = os.environ.get('MetaFnName', MetaFnName)\n",
    "\n",
    "LoggerLevel       = os.environ.get('LOGGER_LEVEL', LoggerLevel)\n",
    "#############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = {'MODEL_ROOT': MODEL_ROOT}  \n",
    "SPACE = process_inference_SPACE(SPACE, MODEL_ENDPOINT)\n",
    "if SPACE['CODE_FN'] not in sys.path:\n",
    "    sys.path.append(SPACE['CODE_FN'])\n",
    "    sys.path = list(set(sys.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92851f08-7316-406c-8d71-fe4d5831480d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MlFlow Databrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import CohortFn, Cohort\n",
    "from recfldtkn.case_base.caseutils import get_ROCOGammePhiInfo_from_CFList\n",
    "from recfldtkn.aidata_base.aidata_base import AIData_Base \n",
    "from recfldtkn.record_base.record_base import Record_Base\n",
    "from recfldtkn.case_base.case_base import Case_Base\n",
    "from recfldtkn.model_base.model_base import Model_Base\n",
    "from recfldtkn.base import fill_missing_keys\n",
    "\n",
    "\n",
    "from nn import load_model_instance_from_nn\n",
    "\n",
    "from inference.utils_inference import (\n",
    "    load_AIData_Model_InfoSettings,\n",
    "    load_Inference_Entry_Example,\n",
    "    pipeline_inference_for_modelbase,\n",
    "    Record_Proc_Config,\n",
    "    Case_Proc_Config,\n",
    "    OneEntryArgs_items_for_inference,\n",
    ")\n",
    "\n",
    "from inference.post_process import NAME_TO_FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e7c26c-51fc-4990-8c15-ca7c13ee379e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaFn = NAME_TO_FUNCTION[MetaFnName]\n",
    "TrigFn = NAME_TO_FUNCTION[TrigFnName]\n",
    "PostFn = NAME_TO_FUNCTION[PostFnName]\n",
    "\n",
    "\n",
    "# # --------- meta_results ---------\n",
    "# meta_results = MetaFn(SPACE)\n",
    "# if meta_results is None:\n",
    "#     print('No meta_results')\n",
    "# else:\n",
    "#     metadata_response = meta_results.get('metadata_response', None)\n",
    "#     pprint('metadata_response:', metadata_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- load context ---------\n",
    "\n",
    "ModelEndpoint_Path = os.path.join(SPACE['MODEL_ROOT'], SPACE['MODEL_ENDPOINT'])\n",
    "assert os.path.exists(ModelEndpoint_Path), f\"Invalid ModelEndpoint_Path: {ModelEndpoint_Path}\"\n",
    "\n",
    "CohortName_to_OneCohortArgs = {INF_CohortName: INF_OneCohortArgs}\n",
    "\n",
    "Package_Settings = {\n",
    "    'INF_CohortName': INF_CohortName,\n",
    "    'INF_OneCohortArgs': INF_OneCohortArgs,\n",
    "    'Record_Proc_Config': Record_Proc_Config,\n",
    "    'Case_Proc_Config': Case_Proc_Config,\n",
    "    'OneEntryArgs_items_for_inference': OneEntryArgs_items_for_inference,\n",
    "    'get_ROCOGammePhiInfo_from_CFList': get_ROCOGammePhiInfo_from_CFList,\n",
    "    'load_model_instance_from_nn': load_model_instance_from_nn,\n",
    "    'Model_Base': Model_Base,\n",
    "    'AIData_Base': AIData_Base,\n",
    "}\n",
    "\n",
    "Context = load_AIData_Model_InfoSettings(\n",
    "    ModelEndpoint_Path = ModelEndpoint_Path,\n",
    "    InputCFArgs_ForInference = InputCFArgs_ForInference, \n",
    "    InferenceArgs = InferenceArgs, \n",
    "    SPACE = SPACE,\n",
    "    **Package_Settings,\n",
    ")\n",
    "\n",
    "model_base = Context['model_base']\n",
    "aidata_base = Context['aidata_base']\n",
    "InfoSettings = Context['InfoSettings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inference_Entry_Example = load_Inference_Entry_Example(INF_CohortName, \n",
    "                                                            CohortName_to_OneCohortArgs,\n",
    "                                                            Cohort,\n",
    "                                                            CohortFn,\n",
    "                                                            SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# json_payload_path = 'data_weight.json'\n",
    "json_payload_path = 'data_cgm.json'\n",
    "#####################\n",
    "\n",
    "with open(json_payload_path, 'r') as f:\n",
    "    json_payload = json.load(f)\n",
    "\n",
    "\n",
    "df_model_input = pd.DataFrame(json_payload['dataframe_records'])\n",
    "df_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = df_model_input.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TriggerName_to_CaseTriggerList = model_input['TriggerName_to_CaseTriggerList']          \n",
    "TriggerName_to_dfCaseTrigger = {k: pd.DataFrame(v) for k, v in TriggerName_to_CaseTriggerList.items()}\n",
    "# TriggerName_to_dfCaseTrigger\n",
    "\n",
    "for TriggerName, df in TriggerName_to_dfCaseTrigger.items():\n",
    "    # CaseTriggerList = TrigFn(dfCaseTrigger, model_input, Context)\n",
    "\n",
    "    if 'ObsDT' not in df.columns:\n",
    "        df['ObsDT'] = pd.to_datetime(df['ObsDT_UTC']) + pd.to_timedelta(df['TimezoneOffset'], 'm')\n",
    "        \n",
    "    TriggerName_to_dfCaseTrigger[TriggerName] = df\n",
    "    # TriggerName_to_CaseTriggerList[TriggerName] = CaseTriggerList\n",
    "\n",
    "TriggerName_to_dfCaseTrigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Inference_Entry_Final = {}\n",
    "Inference_Entry_Final['TriggerName_to_dfCaseTrigger'] = TriggerName_to_dfCaseTrigger\n",
    "Inference_Entry_Final['inference_form'] = model_input['inference_form']\n",
    "Inference_Entry_Final['template_form'] = Inference_Entry_Example['template_form']\n",
    "# pipeline_inference_for_modelbase = pipeline_inference_for_modelbase\n",
    "\n",
    "inference_results = pipeline_inference_for_modelbase(\n",
    "    Inference_Entry = Inference_Entry_Final,\n",
    "    Record_Base = Record_Base, \n",
    "    Case_Base = Case_Base,\n",
    "    aidata_base = aidata_base, \n",
    "    model_base = model_base,\n",
    "    InfoSettings = InfoSettings, \n",
    "    SPACE = SPACE\n",
    ")\n",
    "\n",
    "print([i for i in inference_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelArtifactName_to_Inference = inference_results['ModelArtifactName_to_Inference']\n",
    "pprint(ModelArtifactName_to_Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_base = inference_results['record_base']\n",
    "CohortName = INF_CohortName\n",
    "onecohort_recordbase = record_base.CohortName_to_OneCohortRecordBase[CohortName]\n",
    "Name_to_HRF = onecohort_recordbase.Name_to_HRF\n",
    "\n",
    "for Name, HRF in Name_to_HRF.items():\n",
    "    if len(Name) == 2:\n",
    "        print(f\"RecordName: {Name}\")\n",
    "\n",
    "        df = HRF.df_RecAttr\n",
    "        print(df.shape)\n",
    "        display(df.head())\n",
    "        print('===========\\n')\n",
    "\n",
    "    elif len(Name) == 3:\n",
    "        print(f\"RecFeatName: {Name}\")\n",
    "        df = HRF.df_RecFeat\n",
    "        print(df.shape)\n",
    "        display(df.head())\n",
    "        print('===========\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_base = inference_results['case_base']\n",
    "\n",
    "TriggerCaseBaseName = [i for i in case_base.TriggerCaseBaseName_to_CaseSetNameToCaseset][0]\n",
    "\n",
    "print(TriggerCaseBaseName)\n",
    "\n",
    "CaseSetNameToCaseSet = case_base.TriggerCaseBaseName_to_CaseSetNameToCaseset[TriggerCaseBaseName]\n",
    "CaseSetNameToCaseSet\n",
    "\n",
    "CaseSetName = [i for i in CaseSetNameToCaseSet][0]\n",
    "caseset = CaseSetNameToCaseSet[CaseSetName]\n",
    "\n",
    "\n",
    "df_case = caseset.df_case\n",
    "display(df_case.head())\n",
    "\n",
    "ds_case = caseset.ds_case\n",
    "display(ds_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # ----------------------------------------------------\n",
    "# du1 = inference_results['du1']\n",
    "# du2 = inference_results['du2']\n",
    "# du3 = inference_results['du3']\n",
    "# du4 = inference_results['du4']\n",
    "# total_time = inference_results['total_time']\n",
    "\n",
    "# logger.info(f\"record_base: {du1}\")\n",
    "# logger.info(f\"case_base: {du2}\")\n",
    "# logger.info(f\"aidata_base and model_base update: {du3}\")\n",
    "# logger.info(f\"model_infernece: {du4}\")\n",
    "# logger.info(f\"total_time: {total_time}\")\n",
    "\n",
    "# print(inference_results)\n",
    "\n",
    "# ModelCheckpointName_to_InferenceInfo = inference_results['ModelCheckpointName_to_InferenceInfo']\n",
    "        \n",
    "# # for k, v in ModelCheckpointName_to_InferenceInfo.items():\n",
    "# #     # print(k)\n",
    "# #     v = {k1: list(v1) for k1, v1 in v.items()}\n",
    "    \n",
    "# ModelCheckpointName_to_InferenceInfo = {\n",
    "#     k: {k1: [round(float(i), 4) for i in list(v1)] for k1, v1 in v.items()} for k, v in ModelCheckpointName_to_InferenceInfo.items()\n",
    "# }\n",
    "\n",
    "# self.logger.info(\"Successfully ran prediction\")\n",
    "# return ModelCheckpointName_to_InferenceInfo\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "# self.logger.error(f\"Prediction failed: {str(e)}\")\n",
    "# raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Final_Databrick-Inference_final_V1sb_v1107_15Days",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}