{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "KEY = 'WorkSpace'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY\n",
    "# print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "import sys\n",
    "from proj_space import SPACE\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "SPACE['MODEL_ENDPOINT'] = 'vTest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.aidata import AIData\n",
    "\n",
    "DATA_AIDATA = SPACE['DATA_AIDATA']\n",
    "OneAIDataName = 'CgmLhm_Bf24Af2Af2t8H_5Min_3Cohort_EventFlt_Sample'\n",
    "\n",
    "aidata = AIData.load_aidata(DATA_AIDATA, OneAIDataName, SPACE)\n",
    "aidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaskType = 'MLUniLabel'\n",
    "SeriesName  = 'Bf24.Af2H'\n",
    "OneTaskName = 'cgm_lhm_bf24h_af2h_5min'\n",
    "OneEntryArgs = {\n",
    "    # ----------------- Task Part -----------------\n",
    "    'Task_Part': {\n",
    "\n",
    "        'Tagging': {\n",
    "            # 'TagName_to_TaggingMethod': {\n",
    "            #     # TagName: TaggingMethod {Rules: [(x,x,x)], Op: and or}\n",
    "            # },\n",
    "            # 'ColumnsAddToDsCase': [],\n",
    "            'TagFilter': True, # <--- still need to add Fitlter Tag, as we need to do the RandomDownSample.\n",
    "            'TagSplit': False, # <--- do not need to add Split Tag anymore, as we already have. \n",
    "        },\n",
    "\n",
    "        'Filtering': {\n",
    "            # 'FilterTagging': None,\n",
    "            'FilterTagging': {\n",
    "                \"Rules\": [\n",
    "                    ['RandDownSample', '<=', 0.5],\n",
    "                    ['co.Bf24H_Food_recnum:recnum', '>=', 1], \n",
    "                    ], \n",
    "                'Op': 'and',\n",
    "            }\n",
    "        }, \n",
    "        \n",
    "        'Splitting': {\n",
    "            # 'SplitTagging': { # <----- for the Tagging part.\n",
    "            #     'RANDOM_SEED': 32,\n",
    "            #     'out_ratio': 0.1,\n",
    "            #     'test_ratio': 'tail0.1',\n",
    "            #     'valid_ratio': 0.1\n",
    "            # },\n",
    "            'TrainEvals': {\n",
    "                'TrainSetName': 'In-Train', \n",
    "                'EvalSetNames': ['In-Test', 'In-Valid', 'Out']\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'Mto1Period_MultiTknInStep',\n",
    "        'CF_list': [\n",
    "            'cf.TargetCGM_Bf24H',\n",
    "            'cf.TargetCGM_Af2H',\n",
    "\n",
    "            'cf.ActivitySparse_Bf24H',\n",
    "            'cf.ActivitySparse_Af2H',\n",
    "\n",
    "            # 'cf.TimeSparse_Bf24H', \n",
    "            # 'cf.TimeSparse_Af2H',\n",
    "\n",
    "\n",
    "            'cf.DietSparse_Bf24H',\n",
    "            'cf.DietSparse_Af2H',\n",
    "        ],\n",
    "        'TargetField': 'TargetCGM',\n",
    "        # 'TimeField':   'Time',\n",
    "        'EventFields': [\n",
    "            'Activity',\n",
    "            'Diet',\n",
    "        ],\n",
    "        'BeforePeriods': ['Bf24H'],\n",
    "        'AfterPeriods': ['Af2H'],\n",
    "        'InferenceMode': False, # 'WithFutureEvent' #  # 'NoFutureEvent', 'WithFutureEvent', \n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'EventPrediction',\n",
    "        'MaskingRate': 0,\n",
    "        'Task_Label': 'Diet',\n",
    "        #other parameters toward X and Y value\n",
    "        'agg_function':None,\n",
    "        'label_process': None, \n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "aidata.update_NameToData_with_OneEntryArgs(OneEntryArgs)\n",
    "dataset = aidata.Name_to_DS\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidata.Name_to_DsAIData\n",
    "split_name = [i for i in  aidata.Name_to_Data][0]\n",
    "Name_to_Data = aidata.Name_to_Data# [split_name]\n",
    "Data = Name_to_Data[split_name]\n",
    "df_case = Data['df_case']\n",
    "df_case.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfm = Data['ds_tfm']\n",
    "ds_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['df_case'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch = ds_tfm[:batch_size]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate  # New Hugging Face library for evaluation metrics\n",
    "\n",
    "# Load accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)  # Get highest probability token\n",
    "\n",
    "    # Flatten for sequence-based accuracy\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    return metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_subset_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(ds_tfm[:8000])\n",
    "dataset_test = Dataset.from_dict(ds_tfm[8000:10000])\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    example_dataset = {\n",
    "        \"input_ids\": torch.tensor(examples[\"input_ids\"], dtype=torch.long),\n",
    "        \"labels\": torch.tensor(examples[\"labels\"], dtype=torch.long),\n",
    "    }\n",
    "    example_dataset[\"attention_mask\"]=torch.ones_like(example_dataset['input_ids'])\n",
    "    return example_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Baseline Model - Simple Embedding and FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a Model Definitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Device setup (GPU if available)\n",
    "# device = torch.device( \"cpu\")\n",
    "\n",
    "# # Define vocab size and embedding dimensions\n",
    "# vocab_size = 1000   # Adjust based on vocab\n",
    "# embedding_dim = 128  # Size of the embedding vectors\n",
    "# seq_length = 313  # Length of each sequence in the batch\n",
    "\n",
    "# # Create an Embedding layer (trainable)\n",
    "# embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# # datast change\n",
    "# #[TODO]: build this into transformatino\n",
    "# batch_size = 4\n",
    "# token_ids = torch.tensor(batch['input_ids'], dtype=torch.long, device=device)  # (batch_size, seq_length)\n",
    "# target_labels = torch.tensor(batch['labels'], dtype=torch.long, device=device)  # (batch_size, seq_length)\n",
    "\n",
    "# # model\n",
    "# class SimpleModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "#         super(SimpleModel, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.fc = nn.Linear(embedding_dim, vocab_size)  # Predict token classes for each position\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)  # Convert token IDs to embeddings (batch_size, seq_len, embed_dim)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc(x)  # Output shape: (batch_size, seq_len, vocab_size) -> token classification\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)  # Predict token classes for each position\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        x = self.embedding(input_ids)  # (batch_size, seq_len, embed_dim)\n",
    "        x = self.relu(x)\n",
    "        logits = self.fc(x).float()  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Compute loss if labels are provided (Trainer requires this!)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(torch.long)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))  # Reshape for CE Loss\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Forward and Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Model\n",
    "model = SimpleModel(vocab_size=1000, embedding_dim=128, hidden_dim=256)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=2,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_test,  # Using same data for evaluation (change as needed)\n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Roberta MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maksed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "dataset_train = Dataset.from_dict(ds_tfm[:8000])\n",
    "dataset_test = Dataset.from_dict(ds_tfm[12000:14000])\n",
    "def tokenize_function_formaksed(examples):\n",
    "    example_dataset = {\n",
    "        \"input_ids\": torch.tensor(examples[\"input_ids\"], dtype=torch.long),\n",
    "        \"labels\": torch.tensor(examples[\"input_ids\"], dtype=torch.long),\n",
    "    }\n",
    "    example_dataset[\"attention_mask\"]=torch.ones_like(example_dataset['input_ids'])\n",
    "    return example_dataset\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset_train_masked = dataset_train.map(tokenize_function_formaksed)\n",
    "dataset_test_masked = dataset_test.map(tokenize_function_formaksed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "\n",
    "# Load the existing configuration\n",
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "\n",
    "# Update the vocabulary size\n",
    "config.vocab_size = 500\n",
    "\n",
    "# Save the updated configuration\n",
    "config.save_pretrained('./new_model_config')\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = RobertaForMaskedLM(config)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b Forward and Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Data Collator for Masked Language Modeling (MLM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Enable masked language modeling\n",
    "    mlm_probability=0.15  # Default 15% masking probability\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainer**\n",
    "```\n",
    "#[TODO]: define data_collator (DataCollator, optional) to specify how to get a batch\n",
    "#[TODO]: fill in compute_metrics (Callable[[EvalPrediction], Dict], optional) , other metrixs other than accuracy?\n",
    "#[TODO]: optimizers  (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional and related arguments\n",
    "```\n",
    "\n",
    "**TrainingArguments**\n",
    "- possible parameter:https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",  \n",
    "    overwrite_output_dir=True,  # Overwrite previous model output\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",  \n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints to save space\n",
    "    push_to_hub=False,  # Set True if you want to upload to Hugging Face Hub\n",
    "    # report_to=None,  # Force not to report to wandb\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_test,\n",
    "    # train_dataset=dataset_train_masked,\n",
    "    # eval_dataset=dataset_test_masked,\n",
    "    # tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics, \n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c  Masked LM Embedding + Downstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load pretrain mask LM, train for down stream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Load tokenizer and model from local directory\n",
    "model_path = \"roberta-base\" #\"results/rng_state.pth\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load RoBERTa model (num_labels=2 for binary classification per token)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "\n",
    "class RobertaForTokenClassification(nn.Module):\n",
    "    def __init__(self, model_path, num_labels=2):\n",
    "        super(RobertaForTokenClassification, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_path)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)  # Output shape (batch_size, seq_len, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state)  # Token-level classification\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))  # Flatten for loss function\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaForTokenClassification(model_path, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",  \n",
    "    overwrite_output_dir=True,  # Overwrite previous model output\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",  \n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints to save space\n",
    "    push_to_hub=False,  # Set True if you want to upload to Hugging Face Hub\n",
    "    report_to=None,  # Force not to report to wandb\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_test,\n",
    "    # tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics, \n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alina_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}