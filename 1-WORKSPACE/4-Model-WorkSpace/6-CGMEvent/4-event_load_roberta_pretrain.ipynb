{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881d73c8-f12f-4a9b-a485-996a76289767",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b19d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "KEY = 'WorkSpace'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY\n",
    "# print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "import sys\n",
    "from proj_space import SPACE\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "from datasets import disable_caching\n",
    "# disable_caching()\n",
    "\n",
    "SPACE['MODEL_ENDPOINT'] = 'vTest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1eb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp_name():\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57492949",
   "metadata": {},
   "source": [
    "# Part 1: AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "# 24 / 288\n",
    "\n",
    "# AIDataName = 'CGM_32h_24pd_WellDoc_v2_v0323' # CGM, 32h, 24 data per day. \n",
    "# AIDataName = 'CGM_32h_24pd_WellDoc_v2_sample' # CGM, 32h, 24 data per day. \n",
    "AIDataName = 'CGM2EventFood_bf6h_WellDoc_v2_v0323'\n",
    "\n",
    "\n",
    "path = os.path.join(SPACE['DATA_AIDATA'], AIDataName)\n",
    "print(path)\n",
    "dataset = load_from_disk(path)\n",
    "# dataset\n",
    "\n",
    "config = dataset.info.__dict__['config_name']# .features['cf'].feature.vocab\n",
    "print([i for i in config])\n",
    "CF_to_CFvocab = config['CF_to_CFvocab']\n",
    "print([i for i in CF_to_CFvocab])\n",
    "\n",
    "CF_to_CFArgs = config['CaseSettingInfo']['Case_Args_Settings']['CF_to_CFArgs']\n",
    "print([i for i in CF_to_CFArgs])\n",
    "\n",
    "\n",
    "TriggerCaseBaseName = config['TriggerCaseBaseName']\n",
    "TriggerCaseBaseArgs = config['TriggerCaseBaseName_to_TriggerCaseBaseArgs'][TriggerCaseBaseName]\n",
    "TriggerName = TriggerCaseBaseArgs['Trigger']['TriggerName']\n",
    "# TriggerName\n",
    "# print(TriggerCaseBaseArgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tag.columns\n",
    "\n",
    "from recfldtkn.base import assign_caseSplitTag_to_dsCase\n",
    "from recfldtkn.base import apply_multiple_conditions\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "columns = dataset.column_names\n",
    "columns_tag = [i for i in columns if '--' not in i]\n",
    "df_tag = dataset.select_columns(columns_tag).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a274280",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_to_Selection = {\n",
    "    'Train': {\n",
    "        'Rules': [\n",
    "            ['Age', '>=', 40],\n",
    "            ['Cohort', 'in', ['1', '2', '3']], # <--- add Cohort column\n",
    "            ['Year', 'in', [2020, 2021, 2022, 2023]], # <--- add Year column\n",
    "            ['GenderGroup', 'in', ['Gender.1', 'Gender.2']], \n",
    "            ['ObsDT', '<', '2022-07-01'], \n",
    "            ['ObsDT', '>=', '2021-01-01'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "    'Val': {\n",
    "        'Rules': [\n",
    "            ['Age', '>=', 40],\n",
    "            ['Cohort', 'in', ['1', '2', '3']], # <--- add Cohort column\n",
    "            ['Year', 'in', [2020, 2021, 2022, 2023]], # <--- add Year column\n",
    "            ['ObsDT', '<', '2023-01-01'], \n",
    "            ['ObsDT', '>=', '2022-07-01'],\n",
    "            ['GenderGroup', 'in', ['Gender.1', 'Gender.2']], \n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "    'Test': {\n",
    "        'Rules': [\n",
    "            ['Age', '>=', 40],\n",
    "            ['Cohort', 'in', ['1', '2', '3']], # <--- add Cohort column\n",
    "            ['Year', 'in', [2020, 2021, 2022, 2023]], # <--- add Year column\n",
    "            ['ObsDT', '>=', '2023-01-01'], \n",
    "            ['ObsDT', '<', '2024-01-01'],\n",
    "            ['GenderGroup', 'in', ['Gender.1', 'Gender.2']], \n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b755061",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_dataset = {}\n",
    "for split_name, Selection in Split_to_Selection.items():\n",
    "    # split_to_dataset[split_name] = dataset.filter(lambda x: apply_multiple_conditions(x, split_config['Rules'], split_config['Op']))\n",
    "    Rules = Selection['Rules']\n",
    "    Op = Selection['Op']\n",
    "\n",
    "    index = apply_multiple_conditions(df_tag, Rules, Op)\n",
    "    indices = np.where(index == 1)[0]\n",
    "    # len(indices)\n",
    "    dataset_selected = dataset.select(indices)\n",
    "    split_to_dataset[split_name] = dataset_selected\n",
    "\n",
    "\n",
    "split_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_Data = {}\n",
    "for split, dataset in split_to_dataset.items():\n",
    "    Name_to_Data[split] = {'ds_case': dataset}\n",
    "Name_to_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc298692",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneEntryArgs = {\n",
    "     # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'Mto1Period_MultiTknInStepNoWgt',\n",
    "        'CF_list': [\n",
    "            'cf.TargetCGM_Bf24H',\n",
    "            # 'cf.TargetCGM_Af2H',\n",
    "\n",
    "            'cf.TimeSparse_Bf24H', \n",
    "            # 'cf.TimeSparse_Af2H',\n",
    "\n",
    "            # 'cf.Diet5MinBaseLMH_Bf24H',\n",
    "            # 'cf.Diet5MinBaseLMH_Af2H',\n",
    "        ],\n",
    "        'TargetField': 'TargetCGM',\n",
    "        'TimeField':   'Time',\n",
    "        # 'EventFields': [\n",
    "        #     # 'Activity',\n",
    "        #     'Diet5MinBaseLMH',\n",
    "        # ],\n",
    "        'BeforePeriods': ['Bf24H'],\n",
    "        # 'AfterPeriods': ['Af2H'],\n",
    "        'InferenceMode': False, # 'WithFutureEvent' #  # 'NoFutureEvent', 'WithFutureEvent', \n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'EventPred',\n",
    "        \n",
    "        # ------------ one head for time to now ------------\n",
    "        'EventTimeToNow': 'co.Bf24H_Diet5MinInfo:MinToNow',\n",
    "        'label_to_id_head1': {'0h': 0, \n",
    "                              '1h': 1, '2h': 2, \n",
    "                              '3h': 3, '4h': 4, '5h': 5},\n",
    "        'dimensions_head1': ['food_event_time'],\n",
    "        # ------------ one head for food content ------------\n",
    "        'EventCF_Name': 'cf.Diet5MinBaseLMH_Bf24H',\n",
    "        'label_to_id_head2': {'low': 0, 'medium': 1, 'high': 2},\n",
    "        'dimensions_head2': ['carbs', 'fiber','fat', 'protein', 'sugar'],\n",
    "\n",
    "        'set_transform': False,\n",
    "        'num_proc': 4, \n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "from recfldtkn.aidata_base.entry import EntryAIData_Builder\n",
    "\n",
    "entry = EntryAIData_Builder(TriggerName = TriggerName, \n",
    "                            OneEntryArgs = OneEntryArgs, \n",
    "                            SPACE = SPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03140aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_Data = entry.setup_EntryFn_to_NameToData(Name_to_Data, CF_to_CFvocab, OneEntryArgs)\n",
    "# Name_to_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Name_to_Data['Train']['ds_tfm']\n",
    "ds_train.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "\n",
    "eval_dataset = Name_to_Data['Val']['ds_tfm']\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Name_to_Data['Test']['ds_tfm']\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eval_dataset_dict = {\n",
    "    # 'train': ds_train,\n",
    "    'val': eval_dataset,\n",
    "    'test': test_dataset\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a326ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extra process\n",
    "# for split in Name_to_Data:\n",
    "#     dataset_subsize = int(len(Name_to_Data[split]['ds_tfm'])*0.25)\n",
    "#     Name_to_Data[split]['ds_tfm'] = Name_to_Data[split]['ds_tfm'].shuffle(seed=42).select(range(dataset_subsize)).filter(lambda x: x['labels'] >0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831021f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../_Model/mlm_c123_backup/checkpoint-5000'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ec646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig\n",
    "# from transformers import RobertaForMaskedLM\n",
    "# from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# [CF for CF in CF_to_CFvocab]\n",
    "# vocab = CF_to_CFvocab['cf.TargetCGM_Af2H']['input_ids']['tid2tkn']\n",
    "# vocab\n",
    "\n",
    "\n",
    "# config = RobertaConfig(\n",
    "#     vocab_size=len(vocab),  # Must match the tokenizer\n",
    "#     hidden_size=768,\n",
    "#     num_attention_heads=12,\n",
    "#     num_hidden_layers=6,\n",
    "#     intermediate_size=3072,\n",
    "#     type_vocab_size=1,\n",
    "#     max_position_embeddings=514,\n",
    "#     num_labels=6\n",
    "# )\n",
    "\n",
    "# model = RobertaForSequenceClassification(config=config)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea20a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "# Load your vocab\n",
    "vocab = CF_to_CFvocab['cf.TargetCGM_Af2H']['input_ids']['tid2tkn']\n",
    "\n",
    "# Define config\n",
    "config = RobertaConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    intermediate_size=3072,\n",
    "    type_vocab_size=1,\n",
    "    max_position_embeddings=514,\n",
    "    num_labels=6  # For classification task\n",
    ")\n",
    "\n",
    "# Load the model from pretrained MLM checkpoint\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True  # if classification head differs from MLM head\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc0bf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all RoBERTa encoder parameters\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = ds_train[:4]\n",
    "\n",
    "# model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fcd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='_Model/roberta_for_meal_hour_with_pretrain',\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-4,\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_val_loss\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=eval_dataset_dict,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "val_results = trainer.evaluate()\n",
    "print(f\"Validation results: {val_results}\")\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_results = trainer.evaluate(eval_dataset_dict['test'])\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76499d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9a26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26257011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4db78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f1be4a",
   "metadata": {},
   "source": [
    "# Part 2: Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e151e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00ba7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneEntryArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c75762",
   "metadata": {},
   "source": [
    "# Food Hour Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac011e37",
   "metadata": {},
   "source": [
    "## Step 1: model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a285a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nn.cgmlhm.configuration_cgmlhm import CgmLhmConfig \n",
    "from nn.cgmevent.configuration_fieldencoder import FieldEncoderConfig\n",
    "\n",
    "ModelArgs = {\n",
    "    'model_type': 'cgm_encoder',\n",
    "    'num_classes': 6,\n",
    "    'num_hidden_layers': 3,\n",
    "}\n",
    "config = FieldEncoderConfig(**ModelArgs)\n",
    "# print(config)\n",
    "# config.field_to_fieldinfo\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e8813",
   "metadata": {},
   "source": [
    "## Step 2: model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f81ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmevent.modeling_fieldencoder import FieldEncoderForClassification\n",
    "\n",
    "eventmodel = FieldEncoderForClassification(config)\n",
    "eventmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ef1f6",
   "metadata": {},
   "source": [
    "## Step 3: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df471ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventmodel_input = batch\n",
    "# {\n",
    "#     'input_ids': batch['input_ids'],\n",
    "#     'labels': batch['food_event_time_labels'],\n",
    "#     # 'timestep_ids': batch['Time--timestep_orig_ids'],\n",
    "#     # 'attention_mask': batch['attention_mask'],\n",
    "# }\n",
    "\n",
    "\n",
    "event_outputs = eventmodel(**eventmodel_input)\n",
    "event_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f426cd",
   "metadata": {},
   "source": [
    "## Step4: train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidata.Name_to_DsAIData\n",
    "###############################\n",
    "TrainSetName = 'train'\n",
    "EvalSetNames = ['val', 'test']\n",
    "max_train_samples = None\n",
    "max_eval_samples = None\n",
    "###############################\n",
    "\n",
    "\n",
    "# ------------ train datasets ------------\n",
    "TrainData = Name_to_Data[TrainSetName]\n",
    "ds_tfm_train = TrainData['ds_tfm']\n",
    "if max_train_samples is not None:\n",
    "    max_train_samples = min(len(ds_tfm_train), max_train_samples)\n",
    "    ds_tfm_train = ds_tfm_train.shuffle(seed=42).select(range(max_train_samples))\n",
    "logger.info(ds_tfm_train)\n",
    "\n",
    "\n",
    "# ------------ eval datasets ------------\n",
    "eval_dataset_dict = {}\n",
    "for evalname in EvalSetNames:\n",
    "    if evalname not in Name_to_Data: \n",
    "        logger.info(f'{evalname} not in aidata.Name_to_Data')\n",
    "        continue\n",
    "    eval_dataset = Name_to_Data[evalname]['ds_tfm']\n",
    "    if max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), max_eval_samples)\n",
    "        eval_dataset = eval_dataset.shuffle(seed=42).select(range(max_eval_samples))\n",
    "    eval_dataset_dict[evalname] = eval_dataset\n",
    "logger.info(f'---- eval_datasets ----')\n",
    "logger.info(eval_dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ece8b",
   "metadata": {},
   "source": [
    "### Step4a Train with customize event category (food hour) by CGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a740a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "#Test Running Config (don't save anything, just print the evaluation)\n",
    "HuggingFaceTrainingArgs = {\n",
    "    'output_dir': '_noop',                # required field but won\u2019t be used\n",
    "    'overwrite_output_dir': True,\n",
    "\n",
    "    'do_train': True, \n",
    "    'num_train_epochs': 10,\n",
    "    'per_device_train_batch_size': 64,\n",
    "    'per_device_eval_batch_size': 64,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "\n",
    "    'do_eval': True, \n",
    "    'evaluation_strategy': 'steps',      # <-- evaluate once per epoch\n",
    "    'eval_steps': 50,                    # <-- evaluate once per epoch\n",
    "    \n",
    "    'logging_steps': 1,\n",
    "    'logging_strategy': 'steps',         # <-- print logs once per epoch\n",
    "    # 'logging_first_step': True,\n",
    "    \n",
    "    \n",
    "\n",
    "    'save_strategy': 'no',               # <-- disables checkpoint saving\n",
    "    'report_to': 'wandb',                 # <-- disables wandb/logging\n",
    "\n",
    "    'remove_unused_columns': True,\n",
    "    'dataloader_drop_last': True,\n",
    "    \n",
    "    'learning_rate': 5e-4,\n",
    "    'warmup_steps': 100,\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "}\n",
    "\n",
    "#################################\n",
    "\n",
    "training_args = TrainingArguments(**HuggingFaceTrainingArgs)\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "print(training_args.seed)\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcd26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datasets.fingerprint import Hasher \n",
    "\n",
    "###################\n",
    "AfTknNum = 24\n",
    "###################\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H\")\n",
    "experiment_id = timestamp + \"-\" + Hasher().hash([config])\n",
    "\n",
    "print(experiment_id)\n",
    "\n",
    "class TimestampCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Add the current timestamp to the logs\n",
    "        logs[\"step\"] = state.global_step\n",
    "        logs[\"timestamp\"] = str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def classification_softmax_mse(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    one_hot = F.one_hot(labels, num_classes=probs.size(-1)).float()\n",
    "    return F.mse_loss(probs, one_hot).item()\n",
    "\n",
    "def compute_metrics(eval_preds, experiment_id):\n",
    "    metric_acc = evaluate.load(\"accuracy\", experiment_id=experiment_id)\n",
    "    metric_f1 = evaluate.load(\"f1\", experiment_id=experiment_id)\n",
    "\n",
    "    logits, labels = eval_preds\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "    # Accuracy\n",
    "    d_acc = metric_acc.compute(predictions=preds.tolist(), references=labels.tolist())\n",
    "\n",
    "    # F1 Macro\n",
    "    d_f1 = metric_f1.compute(predictions=preds.tolist(), references=labels.tolist(), average=\"macro\")\n",
    "\n",
    "    # Softmax MSE\n",
    "    mse_soft = classification_softmax_mse(logits, labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": d_acc[\"accuracy\"],\n",
    "        \"f1_macro\": d_f1[\"f1\"],\n",
    "        \"softmax_mse\": mse_soft,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    ########## you have your model \n",
    "    model = eventmodel,\n",
    "    ########## you have your training_args\n",
    "    args = training_args,\n",
    "    ########## get train_dataset\n",
    "    train_dataset = ds_tfm_train, # if training_args.do_train else None,\n",
    "    ########## get eval_dataset\n",
    "    eval_dataset = eval_dataset_dict, # <--- for in-training evaluation\n",
    "    ########## huge question here: is it ok to ignore the tokenizer?\n",
    "    # tokenizer = tokenizer, # Apr 2024: don't add tokenizer, hard to save.\n",
    "    ########## huge question here: data_collator\n",
    "    data_collator = default_data_collator,\n",
    "    compute_metrics = lambda x: compute_metrics(x, experiment_id),\n",
    "    # preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    # callbacks = [CorrectProfilerCallback(wait=1, warmup=1, active=3)],\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6220c",
   "metadata": {},
   "source": [
    "### Step4b Train with Pretrained Roberta on event category (food hour) by CGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # \u2b05\ufe0f switched to pretrained\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tfm_train,\n",
    "    eval_dataset=eval_dataset_dict,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=lambda x: compute_metrics(x, experiment_id),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10864c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.run_name = 'exp_' + get_timestamp_name()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab06423",
   "metadata": {},
   "source": [
    "### Step4b Train with Pretrained Roberta on Hour Regression by CGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=6)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # \u2b05\ufe0f switched to pretrained\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tfm_train,\n",
    "    eval_dataset=eval_dataset_dict,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=lambda x: compute_metrics(x, experiment_id),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9668fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.run_name = 'exp_' + get_timestamp_name()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4258e1",
   "metadata": {},
   "source": [
    "### Step4c Fake Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Define label-to-boosted-region mapping\n",
    "def generate_structured_sample(label, length=289):\n",
    "    x = np.random.randint(0,10, size=length)  # base noise\n",
    "    \n",
    "    block_ranges = {\n",
    "        0: (0, 50),\n",
    "        1: (50, 100),\n",
    "        2: (100, 150),\n",
    "        3: (150, 200),\n",
    "        4: (200, 250),\n",
    "        5: (250, 289),\n",
    "    }\n",
    "    \n",
    "    start, end = block_ranges[label]\n",
    "    x[start:end] += 5  # inject signal\n",
    "    return x.tolist()\n",
    "\n",
    "# Dataset generator\n",
    "def generate_dataset(num_samples):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        label = np.random.randint(0, 6)\n",
    "        sample = generate_structured_sample(label)\n",
    "        input_ids.append(sample)\n",
    "        attention_mask.append([1] * 289)\n",
    "        labels.append(label)\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "\n",
    "# Generate train/val/test datasets\n",
    "train_dataset = generate_dataset(800)\n",
    "val_dataset = generate_dataset(100)\n",
    "test_dataset = generate_dataset(100)\n",
    "\n",
    "# Optional: combine into DatasetDict\n",
    "dataset_dict_train = train_dataset\n",
    "dataset_eval = DatasetDict({\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained Roberta\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# 1. Load pretrained classification model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=6)\n",
    "\n",
    "# 2. Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./simulation_signal_classification',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"simulation_signal_classification\",\n",
    ")\n",
    "\n",
    "# 3. Optional: compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred, experiment_id=None):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# 4. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict_train,\n",
    "    eval_dataset=dataset_eval,\n",
    "    data_collator=default_data_collator,\n",
    "     compute_metrics = lambda x: compute_metrics(x, experiment_id),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sum_column(example):\n",
    "    example['food_event_time_labels'] = example[\"labels\"]\n",
    "    return example\n",
    "\n",
    "dataset_dict_train = dataset_dict_train.map(add_sum_column)\n",
    "for i in dataset_eval:\n",
    "    dataset_eval[i] = dataset_eval[i].map(add_sum_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a391d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nn.cgmlhm.configuration_cgmlhm import CgmLhmConfig \n",
    "from nn.cgmevent.configuration_fieldencoder import FieldEncoderConfig\n",
    "\n",
    "ModelArgs = {\n",
    "    'model_type': 'cgm_encoder',\n",
    "    'num_classes': 6,\n",
    "    'num_hidden_layers': 3,\n",
    "}\n",
    "config = FieldEncoderConfig(**ModelArgs)\n",
    "\n",
    "from nn.cgmevent.modeling_fieldencoder import FieldEncoderForClassification\n",
    "\n",
    "eventmodel = FieldEncoderForClassification(config)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "#Test Running Config (don't save anything, just print the evaluation)\n",
    "HuggingFaceTrainingArgs = {\n",
    "    'output_dir': '_noop',                # required field but won\u2019t be used\n",
    "    'overwrite_output_dir': True,\n",
    "\n",
    "    'do_train': True, \n",
    "    'num_train_epochs': 130,\n",
    "    'per_device_train_batch_size': 64,\n",
    "    'per_device_eval_batch_size': 64,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "\n",
    "    'do_eval': True, \n",
    "    'evaluation_strategy': 'steps',      # <-- evaluate once per epoch\n",
    "    'eval_steps': 50,                    # <-- evaluate once per epoch\n",
    "    \n",
    "    'logging_steps': 1,\n",
    "    'logging_strategy': 'steps',         # <-- print logs once per epoch\n",
    "    # 'logging_first_step': True,\n",
    "    \n",
    "    \n",
    "\n",
    "    'save_strategy': 'no',               # <-- disables checkpoint saving\n",
    "    'report_to': 'wandb',                 # <-- disables wandb/logging\n",
    "\n",
    "    'remove_unused_columns': True,\n",
    "    'dataloader_drop_last': True,\n",
    "    \n",
    "    'learning_rate': 5e-4,\n",
    "    'warmup_steps': 100,\n",
    "    'lr_scheduler_type': 'cosine',\n",
    "}\n",
    "\n",
    "#################################\n",
    "\n",
    "training_args = TrainingArguments(**HuggingFaceTrainingArgs)\n",
    "training_args\n",
    "\n",
    "trainer = Trainer(\n",
    "    ########## you have your model \n",
    "    model = eventmodel,\n",
    "    ########## you have your training_args\n",
    "    args = training_args,\n",
    "    train_dataset=dataset_dict_train,\n",
    "    eval_dataset=dataset_eval,\n",
    "\n",
    "    data_collator = default_data_collator,\n",
    "    compute_metrics = lambda x: compute_metrics(x, experiment_id),\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "logger.info(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc9aea",
   "metadata": {},
   "source": [
    "\n",
    "# Food and Carb Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3c02",
   "metadata": {},
   "source": [
    "## Step 1:model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9598ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nn.cgmlhm.configuration_cgmlhm import CgmLhmConfig \n",
    "from nn.cgmevent.configuration_fieldencoder import FieldEncoderConfig\n",
    "\n",
    "ModelArgs = {\n",
    "    'model_type': 'cgm_encoder',\n",
    "    'num_classes': 6,\n",
    "    'num_hidden_layers': 6,\n",
    "    'quantity_regression':False,\n",
    "    'num_quantity_classes':3\n",
    "}\n",
    "config = FieldEncoderConfig(**ModelArgs)\n",
    "# print(config)\n",
    "# config.field_to_fieldinfo\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a5067",
   "metadata": {},
   "source": [
    "## Step 2: model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07656ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmevent.modeling_fieldencoder import FieldEncoderForClassificationAndRegression\n",
    "\n",
    "eventmodel = FieldEncoderForClassificationAndRegression(config)\n",
    "eventmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf99b2",
   "metadata": {},
   "source": [
    "## step 3: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cffe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eventmodel_input = {\n",
    "    'input_ids': batch['input_ids'],\n",
    "    'labels': batch['food_event_time_labels'],\n",
    "    # 'timestep_ids': batch['Time--timestep_orig_ids'],\n",
    "    'labels_quantity': batch['carbs_labels'],\n",
    "}\n",
    "\n",
    "\n",
    "event_outputs = eventmodel(**eventmodel_input)\n",
    "event_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabf335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alina_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}