{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881d73c8-f12f-4a9b-a485-996a76289767",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b19d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "KEY = 'WorkSpace'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY\n",
    "# print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "import sys\n",
    "from proj_space import SPACE\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "SPACE['MODEL_ENDPOINT'] = 'vTest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57492949",
   "metadata": {},
   "source": [
    "# Part 1: AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oneday: 288, 24pd. 1/12\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# 24 / 288\n",
    "AIDataName = 'CGM_32h_24pd_WellDoc_v2_v0323' # CGM, 32h, 24 data per day. \n",
    "\n",
    "path = os.path.join(SPACE['DATA_AIDATA'], AIDataName)\n",
    "print(path)\n",
    "dataset = load_from_disk(path)\n",
    "# dataset\n",
    "\n",
    "config = dataset.info.__dict__['config_name']# .features['cf'].feature.vocab\n",
    "print([i for i in config])\n",
    "CF_to_CFvocab = config['CF_to_CFvocab']\n",
    "print([i for i in CF_to_CFvocab])\n",
    "\n",
    "CF_to_CFArgs = config['CaseSettingInfo']['Case_Args_Settings']['CF_to_CFArgs']\n",
    "print([i for i in CF_to_CFArgs])\n",
    "\n",
    "\n",
    "TriggerCaseBaseName = config['TriggerCaseBaseName']\n",
    "TriggerCaseBaseArgs = config['TriggerCaseBaseName_to_TriggerCaseBaseArgs'][TriggerCaseBaseName]\n",
    "TriggerName = TriggerCaseBaseArgs['Trigger']['TriggerName']\n",
    "TriggerName\n",
    "# print(TriggerCaseBaseArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0376a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tag.columns\n",
    "\n",
    "from recfldtkn.base import assign_caseSplitTag_to_dsCase\n",
    "from recfldtkn.base import apply_multiple_conditions\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "columns = dataset.column_names\n",
    "columns_tag = [i for i in columns if '--' not in i]\n",
    "df_tag = dataset.select_columns(columns_tag).to_pandas()\n",
    "\n",
    "df_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_age_to_agegroup(age):\n",
    "    if age < 18:\n",
    "        return '0-17'\n",
    "    elif 18<= age < 40:\n",
    "        return '18-39'\n",
    "    elif 40<= age < 65:\n",
    "        return '40-64'\n",
    "    else:\n",
    "        return '65+'\n",
    "    \n",
    "###### additional tagging columns \n",
    "df_tag['Year'] = df_tag['ObsDT'].dt.year\n",
    "df_tag['Cohort'] = df_tag['PID'].astype(str).str[0]\n",
    "df_tag['Age'] = df_tag['Year'] - df_tag['YearOfBirth']  # .dt.year\n",
    "df_tag['AgeGroup'] = df_tag['Age'].apply(map_age_to_agegroup)\n",
    "##########################\n",
    "\n",
    "\n",
    "dataset = dataset.add_column('Age', df_tag['Age'].values)\n",
    "dataset = dataset.add_column('Cohort', df_tag['Cohort'].values)\n",
    "dataset = dataset.add_column('Year', df_tag['Year'].values)\n",
    "dataset = dataset.add_column('AgeGroup', df_tag['AgeGroup'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_to_Selection = {\n",
    "    'Train': {\n",
    "        'Rules': [\n",
    "            ['Age', '>=', 40],\n",
    "            ['Cohort', 'in', ['1', '2', '3']], # <--- add Cohort column\n",
    "            ['Year', 'in', [2020, 2021, 2022, 2023]], # <--- add Year column\n",
    "            ['GenderGroup', 'in', ['Gender.1', 'Gender.2']], \n",
    "            ['ObsDT', '<', '2022-07-01'], \n",
    "            ['ObsDT', '>=', '2021-01-01'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "    'Val': {\n",
    "        'Rules': [\n",
    "            ['Age', '>=', 40],\n",
    "            ['Cohort', 'in', ['1', '2', '3']], # <--- add Cohort column\n",
    "            ['Year', 'in', [2020, 2021, 2022, 2023]], # <--- add Year column\n",
    "            ['ObsDT', '<', '2023-01-01'], \n",
    "            ['ObsDT', '>=', '2022-07-01'],\n",
    "            ['GenderGroup', 'in', ['Gender.1', 'Gender.2']], \n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "    'Test': {\n",
    "        'Rules': [\n",
    "            ['Age', '>=', 40],\n",
    "            ['Cohort', 'in', ['1', '2', '3']], # <--- add Cohort column\n",
    "            ['Year', 'in', [2020, 2021, 2022, 2023]], # <--- add Year column\n",
    "            ['ObsDT', '>=', '2023-01-01'], \n",
    "            ['ObsDT', '<', '2024-01-01'],\n",
    "            ['GenderGroup', 'in', ['Gender.1', 'Gender.2']], \n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_dataset = {}\n",
    "for split_name, Selection in Split_to_Selection.items():\n",
    "    # split_to_dataset[split_name] = dataset.filter(lambda x: apply_multiple_conditions(x, split_config['Rules'], split_config['Op']))\n",
    "    Rules = Selection['Rules']\n",
    "    Op = Selection['Op']\n",
    "\n",
    "    index = apply_multiple_conditions(df_tag, Rules, Op)\n",
    "    indices = np.where(index == 1)[0]\n",
    "    # len(indices)\n",
    "    dataset_selected = dataset.select(indices)\n",
    "    split_to_dataset[split_name] = dataset_selected\n",
    "\n",
    "split_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneEntryArgs = {\n",
    "     # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'Mto1Period_MultiTknInStepNoWgt',\n",
    "        'CF_list': [\n",
    "            'cf.TargetCGM_Bf24H',\n",
    "            'cf.TargetCGM_Af2H',\n",
    "            # 'cf.TargetCGM_Af2Hto8H',\n",
    "        ],\n",
    "        'TargetField': 'TargetCGM',\n",
    "        'BeforePeriods': ['Bf24H'],\n",
    "        'AfterPeriods': ['Af2H'],\n",
    "        'InferenceMode': False, # 'WithFutureEvent' #  # 'NoFutureEvent', 'WithFutureEvent', \n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'CausalLM',\n",
    "        'set_transform': True,\n",
    "        'num_proc': 4, \n",
    "    },\n",
    "\n",
    "    # 'Output_Part': {\n",
    "    #     'EntryOutputMethod': 'MaskedLM',\n",
    "    #     'MaskingRate': 0.15,\n",
    "    #     'set_transform': True,\n",
    "    #     'num_proc': 4, \n",
    "    # },\n",
    "\n",
    "    # 'Output_Part': {\n",
    "    #     'EntryOutputMethod': 'SupervisedFT',\n",
    "    #     'AfStepNum': 24, # 12, # assert AfterPeriods Af2H,so 12 * 2 = 24\n",
    "    #     'set_transform': True,\n",
    "    #     'num_proc': 4, \n",
    "    # },\n",
    "}\n",
    "\n",
    "from recfldtkn.aidata_base.entry import EntryAIData_Builder\n",
    "\n",
    "entry = EntryAIData_Builder(TriggerName = TriggerName, \n",
    "                            OneEntryArgs = OneEntryArgs, \n",
    "                            SPACE = SPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_Data = {}\n",
    "for split, dataset in split_to_dataset.items():\n",
    "    Name_to_Data[split] = {'ds_case': dataset}\n",
    "# Name_to_Data\n",
    "\n",
    "Name_to_Data = entry.setup_EntryFn_to_NameToData(Name_to_Data, CF_to_CFvocab, OneEntryArgs)\n",
    "# Name_to_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidata.Name_to_DsAIData\n",
    "###############################\n",
    "TrainSetName = 'Train'\n",
    "EvalSetNames = [Name for Name in Name_to_Data if Name != TrainSetName]\n",
    "max_train_samples = 1000\n",
    "max_eval_samples = 64\n",
    "###############################\n",
    "\n",
    "\n",
    "# ------------ train datasets ------------\n",
    "TrainData = Name_to_Data[TrainSetName]\n",
    "ds_tfm_train = TrainData['ds_tfm']\n",
    "if max_train_samples is not None:\n",
    "    max_train_samples = min(len(ds_tfm_train), max_train_samples)\n",
    "    ds_tfm_train = ds_tfm_train.shuffle(seed=42).select(range(max_train_samples))\n",
    "logger.info(ds_tfm_train)\n",
    "\n",
    "\n",
    "# ------------ eval datasets ------------\n",
    "eval_dataset_dict = {}\n",
    "for evalname in EvalSetNames:\n",
    "    if evalname not in Name_to_Data: \n",
    "        logger.info(f'{evalname} not in aidata.Name_to_Data')\n",
    "        continue\n",
    "    eval_dataset = Name_to_Data[evalname]['ds_tfm']\n",
    "    if max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), max_eval_samples)\n",
    "        eval_dataset = eval_dataset.shuffle(seed=42).select(range(max_eval_samples))\n",
    "    eval_dataset_dict[evalname] = eval_dataset\n",
    "logger.info(f'---- eval_datasets ----')\n",
    "logger.info(eval_dataset_dict)\n",
    "\n",
    "\n",
    "print(len(ds_tfm_train))\n",
    "for k, v in eval_dataset_dict.items():\n",
    "    print(k, len(v))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1be4a",
   "metadata": {},
   "source": [
    "# Part 2: Model Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4677e",
   "metadata": {},
   "source": [
    "## Step 1: init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmlhm.configuration_cgmlhm import CgmLhmConfig \n",
    "\n",
    "ModelArgs = {\n",
    "    'model_type': 'cgmlhm',\n",
    "    'OneEntryArgs': OneEntryArgs,\n",
    "    'CF_to_CFvocab': CF_to_CFvocab,\n",
    "    'sc_num_hidden_layers': 0, \n",
    "    'tf_n_layer': 0, \n",
    "}\n",
    "\n",
    "config = CgmLhmConfig(**ModelArgs)\n",
    "# print(config)\n",
    "config.field_to_fieldinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9028ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmlhm.modeling_cgmlhm import GgmLhmLMHeadModel\n",
    "\n",
    "model = GgmLhmLMHeadModel(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd78d45",
   "metadata": {},
   "source": [
    "# Part 3: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import torch \n",
    "\n",
    "# batch2dp = 8\n",
    "# batch = ds_tfm.select(range(batch2dp))[:batch2dp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(**batch)\n",
    "# output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f021bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_key_values_lsm, past_key_values_fusor = output.past_key_values# [0][0].shape\n",
    "# print(past_key_values_lsm[0][0].shape)\n",
    "# print(len(past_key_values_lsm), len(past_key_values_lsm[0]))\n",
    "\n",
    "# # past_key_values_fusor could be None\n",
    "# if past_key_values_fusor is not None:   \n",
    "#     print(past_key_values_fusor[0][0].shape)\n",
    "#     print(len(past_key_values_fusor), len(past_key_values_fusor[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ec92f",
   "metadata": {},
   "source": [
    "# Part 4: Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nn.cgmlhm.inference_cgmlhm import inference_model_with_ds\n",
    "\n",
    "\n",
    "InferenceArgs = {\n",
    "    # 'NTP_Args': {\n",
    "    #     'num_old_tokens': 289, \n",
    "    #     'items_list': [\n",
    "    #         'losses_each_seq', \n",
    "    #         # 'losses_each_token', \n",
    "    #         # 'predicted_ntp_labels'\n",
    "    #     ]\n",
    "    # }, \n",
    "    'GEN_Args': {\n",
    "        'num_old_tokens': 289,\n",
    "        'max_new_tokens': 24,\n",
    "        'do_sample': False,\n",
    "        'items_list': [\n",
    "            'hist', \n",
    "            'real', \n",
    "            # 'pred_wfe', \n",
    "            # 'logits_wfe', \n",
    "            'pred_nfe', \n",
    "            # 'logits_nfe'\n",
    "            ],\n",
    "    },\n",
    "}\n",
    "\n",
    "ds_tfm = eval_dataset_dict['Test']\n",
    "print(ds_tfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a380c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inference_model_with_ds(model, ds_tfm, InferenceArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cda29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_eval = results['df_case_eval']\n",
    "df_case_eval.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1de5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.eval.seqeval import SeqPredEval\n",
    "\n",
    "EvaluationArgs = {\n",
    "    'subgroup_config_list': ['DiseaseTypeGroup'],\n",
    "    'x_hist_seq_name': 'hist',\n",
    "    'y_real_seq_name': 'real',\n",
    "    'y_pred_seq_name': 'pred_nfe',\n",
    "    # 'losses_each_seq': 'losses_each_seq',\n",
    "    'metric_list': ['rMSE'],\n",
    "    'horizon_to_se': {\n",
    "        'a_0.5h': [ 0,  6],   # 6\u00d75 = 30min\n",
    "        'b_1h':   [ 0, 12],   # 12\u00d75 = 60min\n",
    "        'c_2h':   [ 0, 24],   # 24\u00d75 = 120min\n",
    "        'd_3h':   [ 0, 36],   # 36\u00d75 = 180min\n",
    "        'e_1t2h': [12, 24],   # from 60min (12*5) to 120min (24*5)\n",
    "    },\n",
    "}\n",
    "\n",
    "subgroup_config_list = EvaluationArgs['subgroup_config_list']\n",
    "x_hist_seq_name      = EvaluationArgs['x_hist_seq_name']\n",
    "y_real_seq_name      = EvaluationArgs['y_real_seq_name']\n",
    "y_pred_seq_name      = EvaluationArgs['y_pred_seq_name']\n",
    "metric_list          = EvaluationArgs['metric_list']\n",
    "horizon_to_se        = EvaluationArgs['horizon_to_se']\n",
    "# losses_each_seq      = EvaluationArgs['losses_each_seq']\n",
    "\n",
    "eval_instance = SeqPredEval(\n",
    "    df_case_eval = df_case_eval, \n",
    "    subgroup_config_list = subgroup_config_list,\n",
    "    x_hist_seq_name = x_hist_seq_name,\n",
    "    y_real_seq_name = y_real_seq_name,\n",
    "    y_pred_seq_name = y_pred_seq_name,\n",
    "    # losses_each_seq = losses_each_seq,\n",
    "    metric_list = metric_list,\n",
    "    horizon_to_se = horizon_to_se\n",
    ")\n",
    "\n",
    "eval_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inference_model_with_ds(model, ds_tfm, InferenceArgs)\n",
    "df_case_eval = results['df_case_eval']\n",
    "subgroup_config_list = EvaluationArgs['subgroup_config_list']\n",
    "x_hist_seq_name      = EvaluationArgs['x_hist_seq_name']\n",
    "y_real_seq_name      = EvaluationArgs['y_real_seq_name']\n",
    "y_pred_seq_name      = EvaluationArgs['y_pred_seq_name']\n",
    "metric_list          = EvaluationArgs['metric_list']\n",
    "horizon_to_se        = EvaluationArgs['horizon_to_se']\n",
    "# losses_each_seq      = EvaluationArgs['losses_each_seq']\n",
    "\n",
    "eval_instance = SeqPredEval(\n",
    "    df_case_eval = df_case_eval, \n",
    "    subgroup_config_list = subgroup_config_list,\n",
    "    x_hist_seq_name = x_hist_seq_name,\n",
    "    y_real_seq_name = y_real_seq_name,\n",
    "    y_pred_seq_name = y_pred_seq_name,\n",
    "    # losses_each_seq = losses_each_seq,\n",
    "    metric_list = metric_list,\n",
    "    horizon_to_se = horizon_to_se\n",
    ")\n",
    "\n",
    "df_report = eval_instance.df_report_neat\n",
    "\n",
    "d = {}\n",
    "for idx, row in df_report.iterrows():\n",
    "    # print(row)\n",
    "    row_d = row.to_dict()\n",
    "    setname = row_d.pop('setname')\n",
    "    for k, v in row_d.items():\n",
    "        d[f'{k}_{setname}'] = v\n",
    "    # d[setname] = row_d\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7762756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_instance.df_report_neat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f08bb",
   "metadata": {},
   "source": [
    "# Part 5: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c77a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0face",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "\n",
    "#################################\n",
    "HuggingFaceTrainingArgs = {\n",
    "    'output_dir': '_test',  # will be updated to model_instance.model_checkpoint_path\n",
    "    'overwrite_output_dir': False,\n",
    "\n",
    "    'do_train': True, \n",
    "    'num_train_epochs': 10,\n",
    "    'per_device_train_batch_size': 4, # 64, # 4, # 64\n",
    "    'per_device_eval_batch_size': 4, # 64, # 4, # 64\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'save_strategy': 'epoch',\n",
    "    'save_total_limit': 5, \n",
    "\n",
    "    'logging_steps': 1,\n",
    "\n",
    "    'do_eval': True, \n",
    "    'eval_steps': 100, \n",
    "    'eval_strategy': 'steps',\n",
    "    'report_to': 'wandb',\n",
    "    \n",
    "    # ------- do not change these -------\n",
    "    'remove_unused_columns': False, # <--- must be False.\n",
    "    'dataloader_drop_last': True,\n",
    "    'logging_first_step': True,\n",
    "}\n",
    "#################################\n",
    "\n",
    "training_args = TrainingArguments(**HuggingFaceTrainingArgs)\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c57d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "print(training_args.seed)\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datasets.fingerprint import Hasher \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H\")\n",
    "experiment_id = timestamp + \"-\" + Hasher().hash([config])\n",
    "\n",
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66397508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestampCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Add the current timestamp to the logs\n",
    "        logs[\"step\"] = state.global_step\n",
    "        logs[\"timestamp\"] = str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e26f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # print(logits.shape, type(logits), '<----- logits')\n",
    "    return logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b77e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "from nn.cgmlhm.inference_cgmlhm import inference_model_with_ds\n",
    "from nn.eval.seqeval import SeqPredEval  # assuming this is the evaluation class you're using\n",
    "\n",
    "import wandb \n",
    "\n",
    "\n",
    "class GenerationEvalCallback(TrainerCallback):\n",
    "    def __init__(self, ds_tfm, InferenceArgs, EvaluationArgs):\n",
    "        \"\"\"\n",
    "        ds_tfm: processed dataset used for inference\n",
    "        InferenceArgs: dictionary of inference-related arguments\n",
    "        EvaluationArgs: dictionary of evaluation-related configs, including:\n",
    "            - subgroup_config_list\n",
    "            - x_hist_seq_name\n",
    "            - y_real_seq_name\n",
    "            - y_pred_seq_name\n",
    "            - metric_list\n",
    "            - horizon_to_se\n",
    "        \"\"\"\n",
    "        self.ds_tfm = ds_tfm\n",
    "        self.InferenceArgs = InferenceArgs\n",
    "        self.EvaluationArgs = EvaluationArgs\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        model = model or kwargs.get(\"model\", None)\n",
    "        if model is None:\n",
    "            print(\"\u26a0\ufe0f Model not provided during evaluation.\")\n",
    "            return {}\n",
    "\n",
    "        # Run inference using the dataset and model\n",
    "        inference_results = inference_model_with_ds(model, self.ds_tfm, self.InferenceArgs)\n",
    "        df_case_eval = inference_results['df_case_eval']\n",
    "\n",
    "        # Extract evaluation config\n",
    "        subgroup_config_list = self.EvaluationArgs['subgroup_config_list']\n",
    "        x_hist_seq_name      = self.EvaluationArgs['x_hist_seq_name']\n",
    "        y_real_seq_name      = self.EvaluationArgs['y_real_seq_name']\n",
    "        y_pred_seq_name      = self.EvaluationArgs['y_pred_seq_name']\n",
    "        metric_list          = self.EvaluationArgs['metric_list']\n",
    "        horizon_to_se        = self.EvaluationArgs['horizon_to_se']\n",
    "\n",
    "        # Initialize evaluator\n",
    "        eval_instance = SeqPredEval(\n",
    "            df_case_eval=df_case_eval,\n",
    "            subgroup_config_list=subgroup_config_list,\n",
    "            x_hist_seq_name=x_hist_seq_name,\n",
    "            y_real_seq_name=y_real_seq_name,\n",
    "            y_pred_seq_name=y_pred_seq_name,\n",
    "            metric_list=metric_list,\n",
    "            horizon_to_se=horizon_to_se\n",
    "        )\n",
    "\n",
    "        # Create a flat dictionary of evaluation metrics\n",
    "        df_report = eval_instance.df_report_neat\n",
    "        flat_metrics = {}\n",
    "\n",
    "        for _, row in df_report.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            setname = row_dict.pop('setname')\n",
    "            for metric_name, value in row_dict.items():\n",
    "                flat_metrics[f\"{metric_name}_{setname}\"] = value\n",
    "\n",
    "        # \u2705 Log to Weights & Biases\n",
    "        try:\n",
    "            # \ud83d\udee0 Ensure step is increasing\n",
    "            current_wandb_step = wandb.run.step if wandb.run else 0\n",
    "            log_step = max(current_wandb_step, state.global_step)\n",
    "            wandb.log(flat_metrics, step=log_step)\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f W&B logging failed: {e}\")\n",
    "\n",
    "            \n",
    "        # return flat_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd23c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67353cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_dict\n",
    "\n",
    "ds_tfm_eval = eval_dataset_dict['Test']\n",
    "ds_tfm_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    ########## you have your model \n",
    "    model = model,\n",
    "    ########## you have your training_args\n",
    "    args = training_args,\n",
    "    ########## get train_dataset\n",
    "    train_dataset = ds_tfm_train, # if training_args.do_train else None,\n",
    "    ########## get eval_dataset\n",
    "    eval_dataset = ds_tfm_eval, # <--- for in-training evaluation\n",
    "    ########## huge question here: is it ok to ignore the tokenizer?\n",
    "    # tokenizer = tokenizer, # Apr 2024: don't add tokenizer, hard to save.\n",
    "    ########## huge question here: data_collator\n",
    "    data_collator = default_data_collator,\n",
    "    # compute_metrics = lambda x: compute_metrics_for_ntp(x, experiment_id, AfTknNum),\n",
    "    # preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    callbacks = [TimestampCallback, GenerationEvalCallback(ds_tfm_eval, InferenceArgs, EvaluationArgs)],\n",
    ")\n",
    "\n",
    "logger.info(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_tfm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93483762",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ccaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "def prepare_last_checkpoint(training_args):\n",
    "    # ------------------------------- part 3: last checkpoint -------------------------------\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "\n",
    "    dont_overwrite_output_dir = bool(not training_args.overwrite_output_dir)\n",
    "\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and dont_overwrite_output_dir:\n",
    "\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "               f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "               f\"Checkpoint detected, resuming training at {last_checkpoint}.\"\n",
    "                \"To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    return last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = prepare_last_checkpoint(training_args)\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e364e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(f\"Batch shape: {batch['input_ids'].shape}\")\n",
    "    break  # Just check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cdaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}