{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881d73c8-f12f-4a9b-a485-996a76289767",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b19d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "KEY = 'WorkSpace'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY\n",
    "# print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "import sys\n",
    "from proj_space import SPACE\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "SPACE['MODEL_ENDPOINT'] = 'vTest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57492949",
   "metadata": {},
   "source": [
    "# Part 1: AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.aidata import AIData\n",
    "\n",
    "DATA_AIDATA = SPACE['DATA_AIDATA']\n",
    "OneAIDataName = 'CgmLhm_Bf24Af2Af2t8H_5Min_3Cohort_EventFlt_Sample'\n",
    "\n",
    "\n",
    "OneEntryArgs = {\n",
    "    # ----------------- Task Part -----------------\n",
    "    'Task_Part': {\n",
    "\n",
    "        'Tagging': {\n",
    "            # 'TagName_to_TaggingMethod': {\n",
    "            #     # TagName: TaggingMethod {Rules: [(x,x,x)], Op: and or}\n",
    "            # },\n",
    "            # 'ColumnsAddToDsCase': [],\n",
    "            'TagFilter': True, # <--- still need to add Fitlter Tag, as we need to do the RandomDownSample.\n",
    "            'TagSplit': False, # <--- do not need to add Split Tag anymore, as we already have. \n",
    "        },\n",
    "\n",
    "        'Filtering': {\n",
    "            # 'FilterTagging': None,\n",
    "            'FilterTagging': {\n",
    "                \"Rules\": [\n",
    "                    ['RandDownSample', '<=', 0.5],\n",
    "                    ['co.Bf24H_Food_recnum:recnum', '>=', 1], \n",
    "                    ], \n",
    "                'Op': 'and',\n",
    "            }\n",
    "        }, \n",
    "        \n",
    "        'Splitting': {\n",
    "            # 'SplitTagging': { # <----- for the Tagging part.\n",
    "            #     'RANDOM_SEED': 32,\n",
    "            #     'out_ratio': 0.1,\n",
    "            #     'test_ratio': 'tail0.1',\n",
    "            #     'valid_ratio': 0.1\n",
    "            # },\n",
    "            'TrainEvals': {\n",
    "                'TrainSetName': 'In-Train', \n",
    "                'EvalSetNames': ['In-Test', 'In-Valid', 'Out']\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'Mto1Period_MultiTknInStep',\n",
    "        'CF_list': [\n",
    "            'cf.TargetCGM_Bf24H',\n",
    "            'cf.TargetCGM_Af2H',\n",
    "            'cf.TimeSparse_Bf24H', \n",
    "            'cf.TimeSparse_Af2H',\n",
    "            'cf.DietSparse_Bf24H',\n",
    "            'cf.DietSparse_Af2H',\n",
    "        ],\n",
    "        'TargetField': 'TargetCGM',\n",
    "        'TimeField':   'Time',\n",
    "        'EventFields': [\n",
    "            'Diet',\n",
    "        ],\n",
    "        'BeforePeriods': ['Bf24H'],\n",
    "        'AfterPeriods': ['Af2H'],\n",
    "        'InferenceMode': False, # 'WithFutureEvent' #  # 'NoFutureEvent', 'WithFutureEvent', \n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'NTP',\n",
    "    },\n",
    "}\n",
    "\n",
    "aidata = AIData.load_aidata(DATA_AIDATA, OneAIDataName, SPACE)\n",
    "aidata.update_NameToData_with_OneEntryArgs(OneEntryArgs)\n",
    "dataset = aidata.Name_to_DS\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidata.Name_to_DsAIData\n",
    "split_name = [i for i in  aidata.Name_to_Data][0]\n",
    "Name_to_Data = aidata.Name_to_Data# [split_name]\n",
    "Data = Name_to_Data[split_name]\n",
    "df_case = Data['df_case']\n",
    "df_case.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfm = Data['ds_tfm']\n",
    "# ds_tfm\n",
    "\n",
    "batch_size = 4\n",
    "batch = ds_tfm[:batch_size]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1be4a",
   "metadata": {},
   "source": [
    "# Part 2: Model Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4677e",
   "metadata": {},
   "source": [
    "## Step 1: init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmlhm.configuration_cgmlhm import CgmLhmConfig \n",
    "\n",
    "ModelArgs = {\n",
    "    'model_type': 'cgmlhm',\n",
    "    'OneEntryArgs': aidata.OneEntryArgs,\n",
    "    'CF_to_CFvocab': aidata.CF_to_CFvocab,\n",
    "    \n",
    "}\n",
    "\n",
    "config = CgmLhmConfig(**ModelArgs)\n",
    "# print(config)\n",
    "config.field_to_fieldinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9028ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmlhm.modeling_cgmlhm import GgmLhmLMHeadModel\n",
    "\n",
    "model = GgmLhmLMHeadModel(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd78d45",
   "metadata": {},
   "source": [
    "# Part 3: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "batch2dp = 8\n",
    "batch = ds_tfm.select(range(batch2dp))[:batch2dp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1520ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4026eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71457703",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values_lsm, past_key_values_fusor = output.past_key_values# [0][0].shape\n",
    "print(past_key_values_lsm[0][0].shape)\n",
    "print(len(past_key_values_lsm), len(past_key_values_lsm[0]))\n",
    "\n",
    "# past_key_values_fusor could be None\n",
    "print(past_key_values_fusor[0][0].shape)\n",
    "print(len(past_key_values_fusor), len(past_key_values_fusor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in output.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185d925",
   "metadata": {},
   "source": [
    "# Part 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in batch.items():\n",
    "    batch[k] = v.to(model.device)\n",
    "    print(k, v.device, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d34fc",
   "metadata": {},
   "source": [
    "## 1. NTP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08034c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "num_old_tokens = 289\n",
    "\n",
    "items_list = ['losses_each_seq', \n",
    "              'losses_each_token', \n",
    "              'predicted_ntp_labels', \n",
    "              ]\n",
    "###############################\n",
    "\n",
    "batch_ntp = {k: v[:, :num_old_tokens] for k, v in batch.items()}\n",
    "\n",
    "for k, v in batch_ntp.items(): print(k, v.shape)\n",
    "\n",
    "\n",
    "output = model(**batch_ntp)\n",
    "\n",
    "# get predicted_labels\n",
    "logits = output.logits\n",
    "\n",
    "\n",
    "# get the loss each token\n",
    "labels = batch['labels'][:, :num_old_tokens]\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "logits_permuted = shift_logits.permute(0, 2, 1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "losses = loss_fn(logits_permuted, shift_labels)\n",
    "\n",
    "batch_ntp_output = {}\n",
    "\n",
    "if 'losses_each_seq' in items_list:\n",
    "    losses_each_seq = losses.mean(dim=1).detach().cpu().numpy().tolist()\n",
    "    batch_ntp_output['losses_each_seq'] = losses_each_seq\n",
    "\n",
    "if 'losses_each_token' in items_list:\n",
    "    losses_each_token = losses.detach().cpu().numpy()\n",
    "    losses_each_token = [losses_each_token[i] for i in range(len(losses_each_token))]\n",
    "    batch_ntp_output['losses_each_token'] = losses_each_token\n",
    "\n",
    "if 'predicted_ntp_labels' in items_list:\n",
    "    # from logits to next token prediction.\n",
    "    predicted_ntp_labels = torch.argmax(logits, dim=-1)\n",
    "    predicted_ntp_labels = predicted_ntp_labels.detach().cpu().numpy()# .tolist()\n",
    "    predicted_ntp_labels = [predicted_ntp_labels[i] for i in range(len(predicted_ntp_labels))]\n",
    "    batch_ntp_output['predicted_ntp_labels'] = predicted_ntp_labels\n",
    "\n",
    "df_ouput = pd.DataFrame(batch_ntp_output)\n",
    "df_ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89916558",
   "metadata": {},
   "source": [
    "## 2. Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe38545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "###############################\n",
    "items_list = ['hist', 'real', 'pred', 'logits']\n",
    "num_old_tokens = 289\n",
    "max_new_tokens = 24 \n",
    "do_sample = False \n",
    "with_future_events = False # True\n",
    "###############################\n",
    "\n",
    "\n",
    "HF_GenerationConfig = {}\n",
    "HF_GenerationConfig['max_new_tokens'] = max_new_tokens\n",
    "HF_GenerationConfig['do_sample'] = do_sample\n",
    "HF_GenerationConfig['return_dict_in_generate'] = True\n",
    "if 'logits' in items_list:\n",
    "    HF_GenerationConfig['output_scores'] = True\n",
    "\n",
    "batch_gen = {k: v[:, :num_old_tokens] for k, v in batch.items() if '--' not in k}\n",
    "\n",
    "batch_gen_field = {k: v for k, v in batch.items() if '--' in k}\n",
    "if with_future_events == False:\n",
    "    for k, v in batch_gen_field.items():\n",
    "        if 'event_indicators' in k:\n",
    "            v[:, num_old_tokens:] = 0\n",
    "            batch_gen_field[k] = v\n",
    "batch_gen.update(batch_gen_field)\n",
    "\n",
    "for k, v in batch_gen.items(): \n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e1b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_old_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c634274",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen['Diet--event_indicators'][:, num_old_tokens:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45edd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(**HF_GenerationConfig)\n",
    "gen_outputs = model.generate(generation_config = generation_config, \n",
    "                              **batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296aa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen_output = {}\n",
    "# if 'hist' in \n",
    "if 'hist' in items_list:\n",
    "    hist = batch_gen['input_ids']\n",
    "    hist = hist.cpu().numpy()\n",
    "    batch_gen_output['hist'] = hist\n",
    "\n",
    "if 'real' in items_list:\n",
    "    real = batch['labels'][:, num_old_tokens: num_old_tokens+max_new_tokens]\n",
    "    real = real.cpu().numpy()\n",
    "    batch_gen_output['real'] = real\n",
    "\n",
    "if 'pred' in items_list:\n",
    "    sequences = gen_outputs['sequences']\n",
    "    pred = sequences[:, -max_new_tokens:]\n",
    "    pred = pred.cpu().numpy()\n",
    "    batch_gen_output['pred'] = pred\n",
    "\n",
    "if 'logits' in items_list:\n",
    "    logits = gen_outputs['scores']\n",
    "    logit_scores = np.array([logit.cpu().numpy() \n",
    "                            for logit in logits]\n",
    "                            ).transpose(1, 0, 2) \n",
    "    batch_gen_output['logit_scores'] = logit_scores\n",
    "\n",
    "\n",
    "batch_gen_output = {\n",
    "    k: [v[i] for i in range(v.shape[0])] for k, v in batch_gen_output.items()\n",
    "}\n",
    "\n",
    "\n",
    "df_output_gen = pd.DataFrame(batch_gen_output)\n",
    "df_output_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65ea14",
   "metadata": {},
   "source": [
    "## Step 1: Process_A_Single_Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ccabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_a_single_batch(model, batch, InferenceArgs = None):\n",
    "\n",
    "    if InferenceArgs is None: InferenceArgs = {}\n",
    "\n",
    "    # ------------ next-token-generation part ----------------\n",
    "    NTP_Args = InferenceArgs.get('NTP_Args', None)\n",
    "    if NTP_Args is not None:\n",
    "        ###############################\n",
    "        num_old_tokens = NTP_Args['num_old_tokens']\n",
    "        items_list = NTP_Args['items_list']\n",
    "        ###############################\n",
    "        batch_ntp = {k: v[:, :num_old_tokens] for k, v in batch.items()}\n",
    "        output = model(**batch_ntp)\n",
    "\n",
    "        # get predicted_labels\n",
    "        lm_logits = output.logits\n",
    "\n",
    "        # get the loss each token\n",
    "        labels = batch['labels'][:, :num_old_tokens]\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        logits_permuted = shift_logits.permute(0, 2, 1)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        losses = loss_fn(logits_permuted, shift_labels)\n",
    "\n",
    "        batch_ntp_output = {}\n",
    "\n",
    "        if 'losses_each_seq' in items_list:\n",
    "            losses_each_seq = losses.mean(dim=1).detach().cpu().numpy().tolist()\n",
    "            batch_ntp_output['losses_each_seq'] = losses_each_seq\n",
    "\n",
    "        if 'losses_each_token' in items_list:\n",
    "            losses_each_token = losses.detach().cpu().numpy()\n",
    "            losses_each_token = [losses_each_token[i] for i in range(len(losses_each_token))]\n",
    "            batch_ntp_output['losses_each_token'] = losses_each_token\n",
    "\n",
    "        if 'predicted_ntp_labels' in items_list:\n",
    "            predicted_ntp_labels = torch.argmax(lm_logits, dim=-1)\n",
    "            predicted_ntp_labels = predicted_ntp_labels.detach().cpu().numpy()# .tolist()\n",
    "            predicted_ntp_labels = [predicted_ntp_labels[i] for i in range(len(predicted_ntp_labels))]\n",
    "            batch_ntp_output['predicted_ntp_labels'] = predicted_ntp_labels\n",
    "    else:\n",
    "        batch_ntp_output = {}\n",
    "        \n",
    "\n",
    "    # ------------ generation part ----------------\n",
    "    GEN_Args = InferenceArgs.get('GEN_Args', None)\n",
    "    if GEN_Args is not None:\n",
    "        ###############################\n",
    "        items_list = GEN_Args['items_list']\n",
    "        num_old_tokens = GEN_Args['num_old_tokens']\n",
    "        max_new_tokens = GEN_Args['max_new_tokens']\n",
    "        do_sample = GEN_Args['do_sample']\n",
    "        ###############################\n",
    "\n",
    "\n",
    "        HF_GenerationConfig = {}\n",
    "        HF_GenerationConfig['max_new_tokens'] = max_new_tokens\n",
    "        HF_GenerationConfig['do_sample'] = do_sample\n",
    "        HF_GenerationConfig['return_dict_in_generate'] = True\n",
    "        if any(['logits' in i for i in items_list]):\n",
    "            HF_GenerationConfig['output_scores'] = True\n",
    "        generation_config = GenerationConfig(**HF_GenerationConfig)\n",
    "\n",
    "        batch_gen = {k: v[:, :num_old_tokens] for k, v in batch.items() if '--' not in k}\n",
    "\n",
    "\n",
    "        # gen_outputs with future events\n",
    "        if 'pred_wfe' in items_list:\n",
    "            batch_gen_field_wte = {k: v for k, v in batch.items() if '--' in k}\n",
    "            batch_gen_wte = {**batch_gen, **batch_gen_field_wte}\n",
    "            gen_outputs_wte = model.generate(generation_config = generation_config, **batch_gen_wte)\n",
    "        else:\n",
    "            gen_outputs_wte = None\n",
    "\n",
    "        # gen_outputs without future events\n",
    "        if 'pred_nfe' in items_list:\n",
    "            batch_gen_field_nfe = {k: v for k, v in batch.items() if '--' in k}\n",
    "            for k, v in batch_gen_field_nfe.items():\n",
    "                if 'event_indicators' in k:\n",
    "                    v[:, num_old_tokens:] = 0   # set future events to 0    \n",
    "                    batch_gen_field_nfe[k] = v\n",
    "            batch_gen_nfe = {**batch_gen, **batch_gen_field_nfe}\n",
    "            gen_outputs_nfe = model.generate(generation_config = generation_config, **batch_gen_nfe)\n",
    "        else:\n",
    "            gen_outputs_nfe = None\n",
    "\n",
    "\n",
    "        \n",
    "        batch_gen_output = {}\n",
    "        # if 'hist' in \n",
    "        if 'hist' in items_list:\n",
    "            hist = batch_gen['input_ids']\n",
    "            hist = hist.cpu().numpy()\n",
    "            batch_gen_output['hist'] = hist\n",
    "\n",
    "        if 'real' in items_list:\n",
    "            real = batch['labels'][:, num_old_tokens: num_old_tokens+max_new_tokens]\n",
    "            real = real.cpu().numpy()\n",
    "            batch_gen_output['real'] = real\n",
    "\n",
    "        if 'pred_wfe' in items_list:\n",
    "            sequences = gen_outputs_wte['sequences']\n",
    "            pred_wfe = sequences[:, -max_new_tokens:]\n",
    "            pred_wfe = pred_wfe.cpu().numpy()\n",
    "            batch_gen_output['pred_wfe'] = pred_wfe\n",
    "\n",
    "        if 'logits_wfe' in items_list:\n",
    "            logits_wfe = gen_outputs_wte['scores']\n",
    "            logits_wfe = np.array([logit.cpu().numpy() \n",
    "                                    for logit in logits_wfe]\n",
    "                                    ).transpose(1, 0, 2) \n",
    "            batch_gen_output['logits_wfe'] = logits_wfe\n",
    "\n",
    "        if 'pred_nfe' in items_list:\n",
    "            sequences = gen_outputs_nfe['sequences']\n",
    "            pred_nfe = sequences[:, -max_new_tokens:]\n",
    "            pred_nfe = pred_nfe.cpu().numpy()\n",
    "            batch_gen_output['pred_nfe'] = pred_nfe\n",
    "\n",
    "        if 'logits_nfe' in items_list:\n",
    "            logits_nfe = gen_outputs_nfe['scores']\n",
    "            logits_nfe = np.array([logit.cpu().numpy() \n",
    "                                    for logit in logits_nfe]\n",
    "                                    ).transpose(1, 0, 2) \n",
    "            batch_gen_output['logits_nfe'] = logits_nfe\n",
    "\n",
    "\n",
    "        batch_gen_output = {\n",
    "            k: [v[i] for i in range(v.shape[0])] for k, v in batch_gen_output.items()\n",
    "        }\n",
    "    else:\n",
    "        batch_gen_output = {}\n",
    "\n",
    "    batch_output = {**batch_ntp_output, **batch_gen_output}\n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gen['input_ids'].shape\n",
    "InferenceArgs = {\n",
    "    'NTP_Args': {\n",
    "        'num_old_tokens': 289, \n",
    "        'items_list': ['losses_each_seq', 'losses_each_token', 'predicted_ntp_labels']\n",
    "    }, \n",
    "    'GEN_Args': {\n",
    "        'num_old_tokens': 289,\n",
    "        'max_new_tokens': 24,\n",
    "        'do_sample': False,\n",
    "        'items_list': ['hist', 'real', 'pred_wfe', 'logits_wfe', 'pred_nfe', 'logits_nfe'], # wfe: with future events, nfe: without future events\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = process_a_single_batch(model, batch, InferenceArgs)\n",
    "\n",
    "df_batch = pd.DataFrame(batch_output)\n",
    "df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = df_batch.iloc[0]\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7975b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec['logits_wfe'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f838e",
   "metadata": {},
   "source": [
    "## Step 2: df_case_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "Split_Name = [i for i in aidata.Name_to_Data][0]\n",
    "Data = aidata.Name_to_Data[Split_Name]\n",
    "########################\n",
    "\n",
    "ds_tfm  = Data['ds_tfm']\n",
    "df_case = Data['df_case']\n",
    "print(ds_tfm)\n",
    "display(df_case.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf080ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "max_inference_num = 1000\n",
    "save_df = False \n",
    "load_df = False \n",
    "chunk_size = 12800\n",
    "batch_size = 16\n",
    "#################################\n",
    "\n",
    "# case_id_columns = aidata.case_id_columns\n",
    "model = model\n",
    "\n",
    "Split_Name = [i for i in aidata.Name_to_Data][0]\n",
    "Data = aidata.Name_to_Data[Split_Name]\n",
    "\n",
    "ds_tfm = Data['ds_tfm']\n",
    "df_case = Data['df_case']\n",
    "\n",
    "if max_inference_num is not None: \n",
    "    ds_tfm = ds_tfm.select(range(max_inference_num))\n",
    "    df_case = df_case.iloc[:max_inference_num]\n",
    "\n",
    "print(ds_tfm)\n",
    "print(df_case.shape)\n",
    "display(df_case.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6247f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "###################\n",
    "# df_case\n",
    "# ds_tfm\n",
    "###################\n",
    "\n",
    "print(model.device)\n",
    "chunk_numbers = len(df_case) // chunk_size\n",
    "print(chunk_numbers)\n",
    "\n",
    "for chunk_id in range(chunk_numbers+1):\n",
    "    # chunk_id = 0\n",
    "    start = chunk_id * chunk_size\n",
    "    end = min((chunk_id+1) * chunk_size, len(df_case))\n",
    "    print(start, end)\n",
    "\n",
    "\n",
    "    df_case_chunk = df_case.iloc[start:end].reset_index(drop = True)\n",
    "    ds_tfm_chunk = ds_tfm.select(range(start, end))\n",
    "    print(ds_tfm_chunk)\n",
    "    print(df_case_chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_chunk = ds_tfm.select(range(start, end)) # ds: chunk_size, 1024.\n",
    "\n",
    "# TODO: update the folder path and file path\n",
    "# folder = os.path.join(SPACE['MODEL_ROOT'], model_checkpoint_name, task)\n",
    "# if not os.path.exists(folder): os.makedirs(folder)\n",
    "\n",
    "\n",
    "# file = os.path.join(folder, f'chunk_{chunk_id:05}_s{start}_e{end}.p')\n",
    "\n",
    "# if load_df == True and os.path.exists(file):\n",
    "#     logger.info(f'Loading chunk {chunk_id} from {file}')\n",
    "#     inference_results_list.append(file)\n",
    "#     continue\n",
    "\n",
    "df_eval_chunk = pd.DataFrame()\n",
    "for batch_s in tqdm(range(0, len(ds_tfm_chunk), batch_size)):\n",
    "    batch_e = min(batch_s + batch_size, len(ds_tfm_chunk))\n",
    "    batch = ds_tfm_chunk[batch_s: batch_e]\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = process_a_single_batch(model, batch, InferenceArgs)\n",
    "        \n",
    "    df_batch = pd.DataFrame(output)\n",
    "    df_eval_chunk = pd.concat([df_eval_chunk, df_batch], axis = 0)\n",
    "\n",
    "df_eval_chunk = df_eval_chunk.reset_index(drop=True)  \n",
    "\n",
    "df_chunk = pd.concat([df_case_chunk, df_eval_chunk], axis = 1)\n",
    "\n",
    "df_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c591d4",
   "metadata": {},
   "source": [
    "# Part 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case_eval = df_chunk\n",
    "df_case_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = df_case_eval.iloc[0]\n",
    "case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.eval.seqeval import SeqEvalForOneDataPoint\n",
    "\n",
    "x_obs_seq = case['hist']    \n",
    "y_real_seq = case['real']\n",
    "y_pred_seq = case['pred_wfe']\n",
    "etric_list = ['rMSE', 'MAE']\n",
    "\n",
    "print(len(x_obs_seq), len(y_real_seq), len(y_pred_seq))\n",
    "\n",
    "eval_dp = SeqEvalForOneDataPoint(x_obs_seq, y_real_seq, y_pred_seq, etric_list)\n",
    "print(eval_dp)\n",
    "print(eval_dp.get_metric_scores())\n",
    "eval_dp.plot_cgm_sensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.eval.seqeval import SeqEvalForOneDataPointWithHorizons\n",
    "\n",
    "\n",
    "x_obs_seq_total = case['hist']    \n",
    "y_real_seq_total = case['real']\n",
    "y_pred_seq_total = case['pred_wfe']\n",
    "metric_list = ['rMSE', 'MAE']\n",
    "\n",
    "horizon_to_se = {\n",
    "    '000-030min': [0, 6],\n",
    "    '000-060min': [0, 12],\n",
    "    '000-120min': [0, 18],\n",
    "    '000-180min': [0, 24],\n",
    "    '060-120min': [6, 18],\n",
    "}\n",
    "\n",
    "eval_dp = SeqEvalForOneDataPointWithHorizons(x_obs_seq_total, \n",
    "                                             y_real_seq_total, \n",
    "                                             y_pred_seq_total, \n",
    "                                             metric_list,\n",
    "                                             horizon_to_se)\n",
    "eval_dp.get_complete_metrics_with_horizon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.eval.seqeval import SeqEvalForOneEvalSet\n",
    "\n",
    "setname = 'test'\n",
    "x_hist_seq_name = 'hist'\n",
    "y_real_seq_name = 'real'\n",
    "y_pred_seq_name = 'pred_wfe'\n",
    "\n",
    "df_case_eval = df_case_eval\n",
    "\n",
    "eval_instance = SeqEvalForOneEvalSet(\n",
    "    setname = setname,\n",
    "    df_case_eval = df_case_eval, \n",
    "    x_hist_seq_name = x_hist_seq_name,\n",
    "    y_real_seq_name = y_real_seq_name, \n",
    "    y_pred_seq_name = y_pred_seq_name,\n",
    "    metric_list = metric_list,\n",
    "    horizon_to_se = horizon_to_se, \n",
    ")\n",
    "\n",
    "eval_results = eval_instance.get_evaluation_report()\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04577b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}