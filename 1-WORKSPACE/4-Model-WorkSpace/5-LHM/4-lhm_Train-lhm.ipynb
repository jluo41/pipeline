{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881d73c8-f12f-4a9b-a485-996a76289767",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b19d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "KEY = 'WorkSpace'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY\n",
    "# print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "import sys\n",
    "from proj_space import SPACE\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "SPACE['MODEL_ENDPOINT'] = 'vTest'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57492949",
   "metadata": {},
   "source": [
    "# Part 1: AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from recfldtkn.base import apply_multiple_conditions\n",
    "import datasets \n",
    "import numpy as np\n",
    "\n",
    "#### ----------- part 0: load dataset -----------\n",
    "##########################################################################################################################\n",
    "\n",
    "\n",
    "#############################################################\n",
    "AIDataName = 'EventFood2CGM_bf5min_WellDoc_v2_v0323'\n",
    "#############################################################\n",
    "path = os.path.join(SPACE['DATA_AIDATA'], AIDataName)\n",
    "dataset = datasets.load_from_disk(path)\n",
    "PID_with_food_full = list(set(dataset['PID']))\n",
    "print(len(PID_with_food_full)) # 654\n",
    "\n",
    "columns = dataset.column_names\n",
    "columns_tag = [i for i in columns if '--' not in i]\n",
    "df_tag = dataset.select_columns(columns_tag).to_pandas()\n",
    "\n",
    "Split_to_Selection_Food = {\n",
    "    'eval_food_t1d_train': {\n",
    "        'Rules': [\n",
    "            # ['PID', 'in', pid_selected],\n",
    "            # ['GenderGroup', '==', 'Gender.1'],\n",
    "            ['DiseaseTypeGroup', '==', 'DiseaseType.1.0'],\n",
    "            # ['AgeGroup', '==', '40-64'],\n",
    "            ['Split', '==', 'Train'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "\n",
    "    'eval_food_t2d_train': {\n",
    "        'Rules': [\n",
    "            # ['PID', 'in', pid_selected],\n",
    "            # ['GenderGroup', '==', 'Gender.1'],\n",
    "            ['DiseaseTypeGroup', '==', 'DiseaseType.2.0'],\n",
    "            # ['AgeGroup', '==', '40-64'],\n",
    "            ['Split', '==', 'Train'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "\n",
    "\n",
    "    'eval_food_t1d_valid': {\n",
    "        'Rules': [\n",
    "            # ['PID', 'in', pid_selected],\n",
    "            # ['GenderGroup', '==', 'Gender.2'],\n",
    "            ['DiseaseTypeGroup', '==', 'DiseaseType.1.0'],\n",
    "            # ['AgeGroup', '==', '40-64'],\n",
    "            ['Split', '==', 'Valid'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "\n",
    "    'eval_food_t2d_valid': {\n",
    "        'Rules': [\n",
    "            # ['PID', 'in', pid_selected],\n",
    "            # ['GenderGroup', '==', 'Gender.2'],\n",
    "            ['DiseaseTypeGroup', '==', 'DiseaseType.2.0'],\n",
    "            # ['AgeGroup', '==', '40-64'],\n",
    "            ['Split', '==', 'Valid'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "\n",
    "    'eval_food_t1d_test': {\n",
    "        'Rules': [\n",
    "            # ['PID', 'in', pid_selected],\n",
    "            # ['GenderGroup', '==', 'Gender.2'],\n",
    "            ['DiseaseTypeGroup', '==', 'DiseaseType.1.0'],\n",
    "            # ['AgeGroup', '==', '40-64'],\n",
    "            ['Split', '==', 'Test'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "\n",
    "    'eval_food_t2d_test': {\n",
    "        'Rules': [\n",
    "            # ['PID', 'in', pid_selected],\n",
    "            # ['GenderGroup', '==', 'Gender.2'],\n",
    "            ['DiseaseTypeGroup', '==', 'DiseaseType.2.0'],\n",
    "            # ['AgeGroup', '==', '40-64'],\n",
    "            ['Split', '==', 'Test'],\n",
    "        ], \n",
    "        'Op': 'and',\n",
    "    },\n",
    "}\n",
    "\n",
    "split_to_dataset = {}\n",
    "for split_name, Selection in Split_to_Selection_Food.items():\n",
    "    # split_to_dataset[split_name] = dataset.filter(lambda x: apply_multiple_conditions(x, split_config['Rules'], split_config['Op']))\n",
    "    Rules = Selection['Rules']\n",
    "    Op = Selection['Op']\n",
    "\n",
    "    index = apply_multiple_conditions(df_tag, Rules, Op)\n",
    "    indices = np.where(index == 1)[0]\n",
    "    # len(indices)\n",
    "    dataset_selected = dataset.select(indices)\n",
    "    split_to_dataset[split_name] = dataset_selected\n",
    "\n",
    "split_to_dataset_food = split_to_dataset\n",
    "# print(split_to_dataset_food)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed321a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ----------- part: define entry arguments -----------\n",
    "OneEntryArgs = {\n",
    "     # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'Mto1Period_MultiTknInStepNoWgt',\n",
    "        'CF_list': [\n",
    "            'cf.TargetCGM_Bf24H',\n",
    "            'cf.TargetCGM_Af2H',\n",
    "            # 'cf.TargetCGM_Af2Hto8H',\n",
    "        ],\n",
    "        'TargetField': 'TargetCGM',\n",
    "        'BeforePeriods': ['Bf24H'],\n",
    "        'AfterPeriods': ['Af2H'],\n",
    "        'InferenceMode': False, # 'WithFutureEvent' #  # 'NoFutureEvent', 'WithFutureEvent', \n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'CausalLM',\n",
    "        'set_transform': True,\n",
    "        'num_proc': 4, \n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_tfm = Data['ds_tfm']\n",
    "# # ds_tfm\n",
    "\n",
    "# batch_size = 4\n",
    "# batch = ds_tfm[:batch_size]\n",
    "# for k, v in batch.items(): print(k, v.shape)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1be4a",
   "metadata": {},
   "source": [
    "# Part 2: Model Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4677e",
   "metadata": {},
   "source": [
    "## Step 1: init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574125f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmlhm.configuration_cgmlhm import CgmLhmConfig \n",
    "\n",
    "config = dataset.info.__dict__['config_name']# .features['cf'].feature.vocab\n",
    "CF_to_CFvocab = config['CF_to_CFvocab']\n",
    "CF_to_CFArgs = config['CaseSettingInfo']['Case_Args_Settings']['CF_to_CFArgs']\n",
    "TriggerCaseBaseName = config['TriggerCaseBaseName']\n",
    "TriggerCaseBaseArgs = config['TriggerCaseBaseName_to_TriggerCaseBaseArgs'][TriggerCaseBaseName]\n",
    "TriggerName = TriggerCaseBaseArgs['Trigger']['TriggerName']\n",
    "\n",
    "ModelArgs = {\n",
    "    'model_type': 'cgmlhm',\n",
    "    'OneEntryArgs': OneEntryArgs,\n",
    "    'CF_to_CFvocab': CF_to_CFvocab,\n",
    "    'fe_num_hidden_layers': 6, \n",
    "    'sc_num_hidden_layers': 0, \n",
    "    'tf_n_layer': 0, \n",
    "}\n",
    "\n",
    "config = CgmLhmConfig(**ModelArgs)\n",
    "# print(config)\n",
    "config.field_to_fieldinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9028ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.cgmlhm.modeling_cgmlhm import GgmLhmLMHeadModel\n",
    "\n",
    "model = GgmLhmLMHeadModel(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name, params in model.named_parameters():\n",
    "    print(layer_name, params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd78d45",
   "metadata": {},
   "source": [
    "# Part 3: Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "batch2dp = 8\n",
    "batch = ds_tfm.select(range(batch2dp))[:batch2dp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f021bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values_lsm, past_key_values_fusor = output.past_key_values# [0][0].shape\n",
    "print(past_key_values_lsm[0][0].shape)\n",
    "print(len(past_key_values_lsm), len(past_key_values_lsm[0]))\n",
    "\n",
    "# past_key_values_fusor could be None\n",
    "if past_key_values_fusor is not None:   \n",
    "    print(past_key_values_fusor[0][0].shape)\n",
    "    print(len(past_key_values_fusor), len(past_key_values_fusor[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f08bb",
   "metadata": {},
   "source": [
    "# Part 4: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidata.TrainEvalsInTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb897ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidata.Name_to_DsAIData\n",
    "###############################\n",
    "TrainSetName = aidata.TrainEvals['TrainSetName_InTrain']\n",
    "EvalSetNames = aidata.TrainEvals['EvalSetNames_InTrain']\n",
    "max_train_samples = 1000\n",
    "max_eval_samples = 64\n",
    "###############################\n",
    "\n",
    "\n",
    "# ------------ train datasets ------------\n",
    "TrainData = aidata.Name_to_Data[TrainSetName]\n",
    "ds_tfm_train = TrainData['ds_tfm']\n",
    "if max_train_samples is not None:\n",
    "    max_train_samples = min(len(ds_tfm_train), max_train_samples)\n",
    "    ds_tfm_train = ds_tfm_train.shuffle(seed=42).select(range(max_train_samples))\n",
    "logger.info(ds_tfm_train)\n",
    "\n",
    "\n",
    "# ------------ eval datasets ------------\n",
    "eval_dataset_dict = {}\n",
    "for evalname in EvalSetNames:\n",
    "    if evalname not in aidata.Name_to_Data: \n",
    "        logger.info(f'{evalname} not in aidata.Name_to_Data')\n",
    "        continue\n",
    "    eval_dataset = aidata.Name_to_Data[evalname]['ds_tfm']\n",
    "    if max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), max_eval_samples)\n",
    "        eval_dataset = eval_dataset.shuffle(seed=42).select(range(max_eval_samples))\n",
    "    eval_dataset_dict[evalname] = eval_dataset\n",
    "logger.info(f'---- eval_datasets ----')\n",
    "logger.info(eval_dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_tfm_train))\n",
    "for k, v in eval_dataset_dict.items():\n",
    "    print(k, len(v))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c77a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0face",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "\n",
    "#################################\n",
    "HuggingFaceTrainingArgs = {\n",
    "    'output_dir': '_test',  # will be updated to model_instance.model_checkpoint_path\n",
    "    'overwrite_output_dir': False,\n",
    "\n",
    "    'do_train': True, \n",
    "    'num_train_epochs': 10,\n",
    "    'per_device_train_batch_size': 4, # 64, # 4, # 64\n",
    "    'per_device_eval_batch_size': 4, # 64, # 4, # 64\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'save_strategy': 'epoch',\n",
    "    'save_total_limit': 10, \n",
    "\n",
    "    'logging_steps': 1,\n",
    "\n",
    "    'do_eval': True, \n",
    "    'eval_steps': 100, \n",
    "    'eval_strategy': 'steps',\n",
    "    'report_to': 'wandb',\n",
    "\n",
    "\n",
    "    'save_strategy': 'steps',\n",
    "    'save_steps': 1000,\n",
    "    'save_total_limit': 3,\n",
    "\n",
    "    \n",
    "    \n",
    "    # ------- do not change these -------\n",
    "    'remove_unused_columns': False, # <--- must be False.\n",
    "    'dataloader_drop_last': True,\n",
    "}\n",
    "#################################\n",
    "\n",
    "training_args = TrainingArguments(**HuggingFaceTrainingArgs)\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c57d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "print(training_args.seed)\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datasets.fingerprint import Hasher \n",
    "\n",
    "###################\n",
    "AfTknNum = 24\n",
    "###################\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H\")\n",
    "experiment_id = timestamp + \"-\" + Hasher().hash([aidata.OneAIDataArgs, config])\n",
    "\n",
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66397508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestampCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Add the current timestamp to the logs\n",
    "        logs[\"step\"] = state.global_step\n",
    "        logs[\"timestamp\"] = str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_metrics_for_ntp(eval_preds, experiment_id, AfTknNum = 24):\n",
    "\n",
    "    metric_acc = evaluate.load(\"accuracy\", experiment_id = experiment_id)\n",
    "    metric_mse = evaluate.load('mse',      experiment_id = experiment_id)\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "    # print(preds.shape, labels.shape)\n",
    "    # print(preds.shape, labels.shape)\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:]\n",
    "    preds  = preds[:, :-1] \n",
    "    # print(preds.shape, labels.shape)\n",
    "\n",
    "    all_labels = labels.reshape(-1)\n",
    "    all_preds = preds.reshape(-1)\n",
    "    # print(all_labels.shape, all_preds.shape)\n",
    "\n",
    "\n",
    "    af_labels = labels[:, -AfTknNum:].reshape(-1)\n",
    "    af_preds  = preds[:, -AfTknNum:].reshape(-1)\n",
    "    # print(af_labels.shape, af_preds.shape)\n",
    "    \n",
    "    d_accu = metric_acc.compute(predictions=all_preds, references=all_labels)\n",
    "    d_mse = metric_mse.compute(predictions=all_preds, references=all_labels)\n",
    "    d_accu_af = metric_acc.compute(predictions=af_preds, references=af_labels)\n",
    "    d_mse_af = metric_mse.compute(predictions=af_preds, references=af_labels)\n",
    "    \n",
    "    d = {}\n",
    "    for k, v in d_accu.items(): d[k] = v\n",
    "    for k, v in d_accu_af.items(): d[k + '_af'] = v\n",
    "\n",
    "    for k, v in d_mse.items(): d[k] = v\n",
    "    for k, v in d_mse_af.items(): d[k + '_af'] = v\n",
    "\n",
    "    d['rMSE'] = np.sqrt(d['mse'])\n",
    "    d['rMSEaf'] = np.sqrt(d['mse_af'])\n",
    "    \n",
    "    d['ACUU']   = d['accuracy'] # np.sqrt()\n",
    "    d['ACUUaf'] = d['accuracy_af'] # np.sqrt()\n",
    "    \n",
    "    del d['mse'], d['mse_af'], d['accuracy'], d['accuracy_af']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e26f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    # print(logits.shape, type(logits), '<----- logits')\n",
    "    return logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca635626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule as profiler_schedule\n",
    "\n",
    "class CorrectProfilerCallback(TrainerCallback):\n",
    "    def __init__(self, wait=1, warmup=1, active=3):\n",
    "        self.wait_steps = wait\n",
    "        self.warmup_steps = warmup\n",
    "        self.active_steps = active\n",
    "        self.profiler = None\n",
    "        self.step_count = 0\n",
    "        self.profiling_active = False  # Track state manually\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.profiler = profile(\n",
    "            activities=[ProfilerActivity.CUDA],\n",
    "            schedule=profiler_schedule(\n",
    "                wait=self.wait_steps,\n",
    "                warmup=self.warmup_steps,\n",
    "                active=self.active_steps,\n",
    "                repeat=1\n",
    "            ),\n",
    "            on_trace_ready=self._on_trace_ready,\n",
    "            record_shapes=True\n",
    "        )\n",
    "\n",
    "    def _on_trace_ready(self, prof):\n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        if not self.profiling_active and state.global_step >= self.wait_steps:\n",
    "            self.profiler.start()\n",
    "            self.profiling_active = True\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if self.profiling_active:\n",
    "            self.profiler.step()\n",
    "            self.step_count += 1\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.profiling_active:\n",
    "            self.profiler.stop()\n",
    "            self.profiling_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    ########## you have your model \n",
    "    model = model,\n",
    "    ########## you have your training_args\n",
    "    args = training_args,\n",
    "    ########## get train_dataset\n",
    "    train_dataset = ds_tfm_train, # if training_args.do_train else None,\n",
    "    ########## get eval_dataset\n",
    "    eval_dataset = eval_dataset_dict, # <--- for in-training evaluation\n",
    "    ########## huge question here: is it ok to ignore the tokenizer?\n",
    "    # tokenizer = tokenizer, # Apr 2024: don't add tokenizer, hard to save.\n",
    "    ########## huge question here: data_collator\n",
    "    data_collator = default_data_collator,\n",
    "    compute_metrics = lambda x: compute_metrics_for_ntp(x, experiment_id, AfTknNum),\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    callbacks = [CorrectProfilerCallback(wait=1, warmup=1, active=3)],\n",
    ")\n",
    "\n",
    "logger.info(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (for exactly 5 steps: wait=1 + warmup=1 + active=3)\n",
    "training_args.max_steps = 5\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe22bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()  # Before starting profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e0369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c90283a2",
   "metadata": {},
   "source": [
    "# Final Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_tfm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93483762",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ccaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "def prepare_last_checkpoint(training_args):\n",
    "    # ------------------------------- part 3: last checkpoint -------------------------------\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "\n",
    "    dont_overwrite_output_dir = bool(not training_args.overwrite_output_dir)\n",
    "\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and dont_overwrite_output_dir:\n",
    "\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "               f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "               f\"Checkpoint detected, resuming training at {last_checkpoint}.\"\n",
    "                \"To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    return last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = prepare_last_checkpoint(training_args)\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e364e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(f\"Batch shape: {batch['input_ids'].shape}\")\n",
    "    break  # Just check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61380794",
   "metadata": {},
   "outputs": [],
   "source": [
    "5466579 / 64 / 4 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cdaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}