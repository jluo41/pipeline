{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import logging\n",
    "from pprint import pprint \n",
    " \n",
    "# WorkSpace\n",
    "KEY = 'WorkSpace'; WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY; print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "sys.path.append(WORKSPACE_PATH)\n",
    "\n",
    "# Pipeline Space\n",
    "from proj_space import SPACE\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "\n",
    "from recfldtkn.configfn import load_cohort_args\n",
    "from config_observer.CF import cf_to_CaseFeatConfig\n",
    "from config_observer.QCF import cf_to_QueryCaseFeatConfig\n",
    "from config_observer.CKPD import ckpd_to_CkpdObsConfig\n",
    "from recfldtkn.pipeline_dataset import pipeline_to_generate_dfcase_and_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: CaseSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "recfldtkn_config_path = os.path.join(SPACE['CODE_FN'], 'config_recfldtkn/')\n",
    "cohort_args = load_cohort_args(recfldtkn_config_path, SPACE)\n",
    "cohort_args['ckpd_to_CkpdObsConfig'] = ckpd_to_CkpdObsConfig\n",
    "cohort_args['ObsDTName'] = 'ObsDT'\n",
    "cohort_args['PID_ObsDT_columns'] = [cohort_args['RootID'], cohort_args['ObsDTName']]\n",
    "\n",
    "CASE_TAGGING_PROC_CONFIG = {\n",
    "    'use_CF_from_disk': False,\n",
    "    'use_CO_from_disk': False,\n",
    "    'start_chunk_id': 0,\n",
    "    'end_chunk_id': None,\n",
    "    'chunk_size': 10000, # 100000,\n",
    "    'save_to_pickle': True,\n",
    "    'num_processors': 8, #1, # 12,\n",
    "}\n",
    "\n",
    "CASE_FIEDLING_PROC_CONFIG = {\n",
    "    'use_CF_from_disk': False,\n",
    "    'use_CO_from_disk': False,\n",
    "    'start_chunk_id': 0,\n",
    "    'end_chunk_id': None,\n",
    "    'chunk_size': 10000, # 100000,\n",
    "    'num_processors': 8,\n",
    "}\n",
    "\n",
    "PROC_SETTINGS = {\n",
    "    'LOAD_DF_CASE': True,\n",
    "    'LOAD_DS_DATA': True,\n",
    "    'SAVE_DF_CASE': False,\n",
    "    'SAVE_DS_DATA': False,\n",
    "    'RANDOM_SAMPLE': False,\n",
    "    'SAVE_TRIGGER_DF': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.pipeline_dataset import pipeline_to_generate_dfcase_and_dataset\n",
    "from DsConst import DsConst_Config\n",
    "from recfldtkn.aidata_base.caseset import CaseSet\n",
    "\n",
    "##################\n",
    "dataset_name = 'SurveyTagCrntFeat'  # 'InvTagCrntFeat' \n",
    "##################\n",
    "\n",
    "caseset_args = {\n",
    "    'dataset_name': dataset_name,\n",
    "    'DsConst_Config': DsConst_Config, \n",
    "    'cohort_args': cohort_args,\n",
    "    'cf_to_QueryCaseFeatConfig': cf_to_QueryCaseFeatConfig,\n",
    "    'cf_to_CaseFeatConfig': cf_to_CaseFeatConfig,\n",
    "    'SPACE': SPACE,\n",
    "    'CASE_TAGGING_PROC_CONFIG': CASE_TAGGING_PROC_CONFIG,\n",
    "    'CASE_FIEDLING_PROC_CONFIG': CASE_FIEDLING_PROC_CONFIG,\n",
    "    'PROC_SETTINGS': PROC_SETTINGS,\n",
    "    'pipeline_to_generate_dfcase_and_dataset': pipeline_to_generate_dfcase_and_dataset,\n",
    "}\n",
    "\n",
    "caseset = CaseSet(caseset_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: AIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConfigInput import InputConfig_Settings\n",
    "from ConfigTask import TaskSeries_Settings\n",
    "from ConfigAIDev import AIDevConfig_Settings\n",
    "\n",
    "from recfldtkn.aidata_base.entry import EntryAIData_Builder\n",
    "\n",
    "## %%%%%%%%%%%%%%%%%%%%%%%% user generation \n",
    "InputData_Args = {\n",
    "    'INPUT_CFs_Args': 'InvCrntFeat',\n",
    "    'EntryInputMethod': 'SparseMatrixFromOneCF',\n",
    "    # we might add the Filtering for Input Data as well. \n",
    "}\n",
    "\n",
    "OneTask_Args = {\n",
    "    'OneTaskSeries': 'SurveyPred',\n",
    "    'OneTaskName': 'Mars.MedAdhere',\n",
    "    'EntryOutputMethod': 'UniLabel'\n",
    "}\n",
    "\n",
    "AIDevData_Args = {\n",
    "    'NewName_to_OldNames': 'BaseC2',  # 'BaseC1', \n",
    "    'TrainEvals': 'BaseTrTe', \n",
    "    'SplitTagging': 'Rs42t2',\n",
    "    'Filtering': 'FltNone', # 'FltBaseSMS',\n",
    "}\n",
    "## %%%%%%%%%%%%%%%%%%%%%%%% user generation \n",
    "\n",
    "\n",
    "entry_builder = EntryAIData_Builder(InputData_Args, \n",
    "                                    InputConfig_Settings, \n",
    "                                    OneTask_Args, \n",
    "                                    TaskSeries_Settings,\n",
    "                                    AIDevData_Args, \n",
    "                                    AIDevConfig_Settings, \n",
    "                                    SPACE = SPACE)\n",
    "\n",
    "entry_builder.load_pypath()\n",
    "EntryArgs = entry_builder.EntryArgs \n",
    "EntryArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.aidata import AIData\n",
    "aidata = AIData(caseset, entry_builder)\n",
    "aidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_DsAIData = aidata.Name_to_DsAIData\n",
    "Name_to_DsAIData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_DsCaseFields = aidata.Name_to_DsCaseFields\n",
    "Name_to_DsCaseFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = aidata.df_case\n",
    "df_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: model inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect \n",
    "from recfldtkn.loadtools import convert_variables_to_pystirng\n",
    "from recfldtkn.loadtools import load_module_variables\n",
    "\n",
    "\n",
    "LoadFnMethod = 'XGBClassifier'\n",
    "pypath = os.path.join(SPACE['CODE_FN'], 'fn_io', f'{LoadFnMethod}.py')\n",
    "module = load_module_variables(pypath)\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%% user generation\n",
    "ModelArgs = {\n",
    "    'algorithm': 'XGBClassifier',\n",
    "    'random_state': 42, \n",
    "    'max_depth': 10,\n",
    "}\n",
    "\n",
    "TrainingArgs = {\n",
    "    'n_estimators': 2000, # num_boost_round\n",
    "    'learning_rate': 0.1, # eta\n",
    "    'objective': 'binary:logistic', \n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_metric': 'logloss',  \n",
    "}\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%% user generation\n",
    "\n",
    "fn_model_structure = module.fn_model_structure\n",
    "fn_model_training = module.fn_model_training\n",
    "model = fn_model_structure(ModelArgs, TrainingArgs)\n",
    "\n",
    "TrainEvals = entry_builder.TrainEvals\n",
    "model = fn_model_training(model, \n",
    "                          Name_to_DsAIData, TrainEvals,\n",
    "                          ModelArgs, TrainingArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainEvals = entry_builder.TrainEvals\n",
    "EvalSetNames = TrainEvals['EvalSetNames']\n",
    "\n",
    "Name_to_Inference = {}\n",
    "\n",
    "for Name in EvalSetNames: \n",
    "    fn_model_inference = module.fn_model_inference\n",
    "    dataset = Name_to_DsAIData[Name]\n",
    "    results = fn_model_inference(model, dataset)\n",
    "    Name_to_Inference[Name] = results\n",
    "\n",
    "Name_to_Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aidata.Name_to_DsCaseFields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "case_id_columns = aidata.case_id_columns\n",
    "df_case = aidata.df_case\n",
    "Name_to_dfInference = {}\n",
    "for Name in Name_to_Inference:\n",
    "    dsCF = Name_to_DsCaseFields[Name]\n",
    "    df = dsCF.select_columns(case_id_columns).to_pandas()\n",
    "    df_case_eval = pd.merge(df_case, df, on=case_id_columns, how='right') \n",
    "    Inference = Name_to_Inference[Name]\n",
    "    for key in Inference:\n",
    "        df_case_eval[key] = Inference[key]\n",
    "    Name_to_dfInference[Name] = df_case_eval\n",
    "    \n",
    "df_case_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}