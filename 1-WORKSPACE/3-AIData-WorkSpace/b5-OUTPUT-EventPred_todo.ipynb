{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "KEY = '1-WORKSPACE'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0]\n",
    "print(WORKSPACE_PATH); os.chdir(WORKSPACE_PATH)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "SPACE = {\n",
    "    'DATA_RAW': f'_Data/0-Data_Raw',\n",
    "    'DATA_RFT': f'_Data/1-Data_RFT',\n",
    "    'DATA_CASE': f'_Data/2-Data_CASE',\n",
    "    'DATA_AIDATA': f'_Data/3-Data_AIDATA',\n",
    "    'DATA_EXTERNAL': f'code/external',\n",
    "    'CODE_FN': f'code/pipeline', \n",
    "}\n",
    "assert os.path.exists(SPACE['CODE_FN']), f'{SPACE[\"CODE_FN\"]} not found'\n",
    "\n",
    "print(SPACE['CODE_FN'])\n",
    "sys.path.append(SPACE['CODE_FN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "from recfldtkn.case_base.casefnutils.casefn import Case_Fn #  import AIDATA_ENTRYINPUT_PATH\n",
    "\n",
    "######################## get the CF_DataName list\n",
    "CF_DataName = 'CGMwithDietBf8h-CaseBase-CGM5MinEntry-31ec84c0520b37c1'\n",
    "CohortName_list = [\n",
    "    # 'WellDoc2022CGM',\n",
    "    'WellDoc2025ALS',\n",
    "    # 'WellDoc2025CVS', \n",
    "    # 'WellDoc2025LLY',\n",
    "]\n",
    "######################## \n",
    "\n",
    "######################## get the CF_DataName list\n",
    "CF_DataName_list = [\n",
    "    f'{CF_DataName}/{i}' for i in CohortName_list\n",
    "]\n",
    "########################\n",
    "\n",
    "ds_list = []\n",
    "ref_config = None\n",
    "ref_column_names = None\n",
    "for i, CF_DataName in enumerate(CF_DataName_list):\n",
    "    path = os.path.join(SPACE['DATA_AIDATA'], CF_DataName)\n",
    "    ds = datasets.load_from_disk(path)\n",
    "    print(CF_DataName, ds )\n",
    "    # config = copy.deepcopy(ds.info.config.__dict__) if hasattr(ds.info, 'config') else {}\n",
    "    config = ds.config_name\n",
    "    column_names = ds.column_names\n",
    "    ds_list.append(ds)\n",
    "\n",
    "# pprint(config)\n",
    "dataset = datasets.concatenate_datasets(ds_list)\n",
    "\n",
    "CF_list = list(set([i.split('--')[0] for i in dataset.column_names if '--tid' in i]))\n",
    "CF_fn_list = [Case_Fn(CF, SPACE) for CF in CF_list]\n",
    "CF_to_CFvocab = {CF: CF_fn.COVocab for CF, CF_fn in zip(CF_list, CF_fn_list)}\n",
    "\n",
    "CF_DataName = config['TriggerCaseBaseName']\n",
    "TriggerCaseBaseArgs = config['TriggerCaseBaseName_to_TriggerCaseBaseArgs'][CF_DataName]\n",
    "TriggerName = TriggerCaseBaseArgs['Trigger']['TriggerName']\n",
    "\n",
    "logger.info(f'set up TriggerName: {TriggerName}')\n",
    "logger.info(f'set up CF_Config: {[i for i in config]}')\n",
    "config['CF_to_CFvocab'] = CF_to_CFvocab\n",
    "\n",
    "print('total', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = {'ds_case': dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT 1: UniLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneEntryArgs = {\n",
    "    # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': '1TknInStep',\n",
    "        'CF_list': [\n",
    "            'CGMValueBf24h',\n",
    "            # 'CGMValueAf2h',\n",
    "        ],\n",
    "        'BeforePeriods': ['Bf24h'],\n",
    "        # 'AfterPeriods': ['Af2h'],\n",
    "        'InferenceMode': False, # True, # True, # False, # True, \n",
    "        'TargetField': 'CGMValue', \n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'EventPred',\n",
    "        \n",
    "        # ------------ one head for time to now ------------\n",
    "        'EventTimeToNow': 'co.Bf24H_Diet5MinInfo:MinToNow',\n",
    "        'label_to_id_head1': {'0h': 0, '1h': 1, '2h': 2, \n",
    "                              '3h': 3, '4h': 4, '5h': 5},\n",
    "        'dimensions_head1': ['food_event_time'],\n",
    "        # ------------ one head for food content ------------\n",
    "        # 'EventCF_Name': 'cf.Diet5MinBaseLMH_Bf24H',\n",
    "        'label_to_id_head2': {'low': 0, 'medium': 1, 'high': 2},\n",
    "        'dimensions_head2': ['carbs', 'fiber','fat', 'protein', 'sugar'],\n",
    "\n",
    "\n",
    "        'set_transform': True,\n",
    "        'num_proc': 4, \n",
    "    },\n",
    "}\n",
    "\n",
    "# Data = {'df_case': caseset.df_case, 'ds_case': caseset.ds_case\n",
    "EntryOutputMethod = OneEntryArgs['Output_Part']['EntryOutputMethod']\n",
    "EntryInputMethod = OneEntryArgs['Input_Part']['EntryInputMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.entry import EntryAIData_Builder\n",
    "\n",
    "\n",
    "entry = EntryAIData_Builder(OneEntryArgs = OneEntryArgs, \n",
    "                            SPACE = SPACE)\n",
    "\n",
    "tfm_fn_AIInputData = entry.tfm_fn_AIInputData\n",
    "entry_fn_AIInputData = entry.entry_fn_AIInputData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_OUTPUT_CFs(OneEntryArgs):\n",
    "    if 'Output_Part' not in OneEntryArgs:\n",
    "        return []\n",
    "    else:\n",
    "        return OneEntryArgs['Output_Part'].get('CF_list', [])\n",
    "get_OUTPUT_CFs.fn_string = inspect.getsource(get_OUTPUT_CFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fn_output(examples, tfm_fn_AIInputData, OneEntryArgs, CF_to_CFvocab):\n",
    "    # print([i for i in examples])\n",
    "    examples_tfm = tfm_fn_AIInputData(examples, OneEntryArgs, CF_to_CFvocab)\n",
    "    # print([i for i in examples_tfm]) \n",
    "\n",
    "\n",
    "    def convert_token(event_input_ids, tid2tkn, dimensions, label_to_id):\n",
    "        labels = []\n",
    "        # event_tokens = []\n",
    "        for one_datapoint_event_input_ids in event_input_ids:\n",
    "            token_list = [tid2tkn[str(t)].split(':')[-1] for t in one_datapoint_event_input_ids]\n",
    "            token_list = [i for i in token_list if ' ' in i]\n",
    "            try:\n",
    "                d = {k.split(' ')[1]: k.split(' ')[0] for k in token_list}\n",
    "            except:\n",
    "                print(token_list)\n",
    "                raise\n",
    "            one_labels = []\n",
    "            for dim in dimensions:\n",
    "                if dim in d:\n",
    "                    label = label_to_id.get(d[dim])\n",
    "                    one_labels.append(label)\n",
    "                else:\n",
    "                    # print(f'{dim} not in {d}')\n",
    "                    one_labels.append(-100)\n",
    "            labels.append(one_labels)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        dim_to_labels = {}\n",
    "        for idx, dim in enumerate(dimensions):\n",
    "            dim_to_labels[dim] = torch.LongTensor(labels[:, idx])\n",
    "        return dim_to_labels\n",
    "\n",
    "\n",
    "    EventTimeToNow = OneEntryArgs['Output_Part']['EventTimeToNow']\n",
    "    # print(EventTimeToNow)\n",
    "    min_to_now = examples[EventTimeToNow]\n",
    "    # print(min_to_now)\n",
    "    hour_labels = [min(int(minutes // 60), 5) for minutes in min_to_now]\n",
    "    # print(hour_labels)\n",
    "\n",
    "    # print([i for i in examples if 'Diet5Min' in i ])\n",
    "    EventCF_Name = OneEntryArgs['Output_Part']['EventCF_Name']\n",
    "    event_input_ids = [i[-1] for i in examples[EventCF_Name + '--input_ids']]\n",
    "    # event_input_ids\n",
    "\n",
    "    CFvocab = CF_to_CFvocab[EventCF_Name]\n",
    "    input_vocab = CFvocab['input_ids']# ['tkn2tid']\n",
    "    tkn2tid = input_vocab['tkn2tid']\n",
    "    tid2tkn = input_vocab['tid2tkn']\n",
    "\n",
    "    # tkn2tid\n",
    "    label_to_id = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    dimensions = ['carbs', 'fiber','fat', 'protein', 'sugar']\n",
    "    dim_to_labels_food = convert_token(event_input_ids, tid2tkn, dimensions, label_to_id)\n",
    "    # pd.DataFrame(event_tokens)\n",
    "    # labels\n",
    "\n",
    "    dim_to_labels = {}\n",
    "    dim_to_labels['food_event_time'] = torch.LongTensor(hour_labels)\n",
    "    dim_to_labels.update(dim_to_labels_food) # ['food_event_time']\n",
    "    \n",
    "    dim_to_labels = {k+\"_labels\": v for k, v in dim_to_labels.items()}\n",
    "\n",
    "    examples_tfm.update(dim_to_labels)\n",
    "    examples_tfm['labels'] = examples_tfm['food_event_time_labels'].clone()\n",
    "    return examples_tfm\n",
    "\n",
    "transform_fn_output.fn_string = inspect.getsource(transform_fn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset[:64]\n",
    "examples_tfm = transform_fn_output(examples, tfm_fn_AIInputData, OneEntryArgs, CF_to_CFvocab)\n",
    "print([i for i in examples_tfm])\n",
    "\n",
    "pprint(OneEntryArgs, sort_dicts=True)\n",
    "pprint(examples_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entry_fn_AITaskData(Data, \n",
    "                        CF_to_CFvocab, \n",
    "                        OneEntryArgs,\n",
    "                        tfm_fn_AIInputData = None,\n",
    "                        entry_fn_AIInputData = None,\n",
    "                        ):\n",
    "\n",
    "    # InputCFs = OneEntryArgs['Input_FullArgs']['INPUT_CFs_Args']['InputCFs']\n",
    "    transform_fn = lambda examples: transform_fn_output(examples, tfm_fn_AIInputData, OneEntryArgs, CF_to_CFvocab)\n",
    "    ds_case = Data['ds_case']\n",
    "\n",
    "    if type(ds_case) == pd.DataFrame:\n",
    "        ds_case = datasets.Dataset.from_pandas(ds_case)\n",
    "        \n",
    "    # ds_case.set_transform(transform_fn)\n",
    "    # use_map = OneEntryArgs.get('use_map', False)\n",
    "    Output_Part = OneEntryArgs['Output_Part']\n",
    "    num_proc = Output_Part.get('num_proc', 4)\n",
    "    set_transform = Output_Part.get('set_transform', True)\n",
    "    if set_transform == True:\n",
    "        ds_case.set_transform(transform_fn)\n",
    "        ds_tfm = ds_case\n",
    "    else:\n",
    "        old_cols = ds_case.column_names\n",
    "        ds_tfm = ds_case.map(transform_fn, batched = True, num_proc = num_proc)\n",
    "        ds_tfm = ds_tfm.remove_columns(old_cols)\n",
    "        \n",
    "    Data['ds_tfm'] = ds_tfm\n",
    "    \n",
    "    return Data\n",
    "\n",
    "entry_fn_AITaskData.fn_string = inspect.getsource(entry_fn_AITaskData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = entry_fn_AITaskData(Data, \n",
    "                           CF_to_CFvocab, \n",
    "                           OneEntryArgs,\n",
    "                           tfm_fn_AIInputData,\n",
    "                           entry_fn_AIInputData)\n",
    "\n",
    "ds_tfm = Data['ds_tfm']\n",
    "ds_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ds_tfm[:4]\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import Base\n",
    "from recfldtkn.aidata_base.entry import AIDATA_ENTRYOUTPUT_PATH\n",
    "\n",
    "prefix = [\n",
    "    'import torch',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np', \n",
    "    'import datasets',\n",
    "    ]\n",
    "fn_variables = [\n",
    "    get_OUTPUT_CFs,\n",
    "    entry_fn_AITaskData,\n",
    "]\n",
    "pycode = Base.convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], AIDATA_ENTRYOUTPUT_PATH, f'{EntryOutputMethod}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create DataLoader with your actual training parameters\n",
    "loader = DataLoader(\n",
    "    dataset=ds_tfm,  # Your dataset with set_transform\n",
    "    batch_size=32,            # Use your real batch size\n",
    "    num_workers=1,            # Match your training setup\n",
    "    pin_memory=True,          # Same as training config\n",
    "    shuffle=False             # Disable for consistent measurement\n",
    ")\n",
    "\n",
    "# 2. Warm-up run (initial batches are slower due to setup)\n",
    "print(\"Warming up...\")\n",
    "for _ in loader: pass\n",
    "\n",
    "# 3. Timed measurement\n",
    "num_batches = len(loader)\n",
    "print(f\"Testing with {num_batches} batches...\")\n",
    "\n",
    "start_time = time.perf_counter()  # More precise timer\n",
    "for _ in loader:\n",
    "    pass\n",
    "total_time = time.perf_counter() - start_time\n",
    "\n",
    "# 4. Calculate metrics\n",
    "throughput = num_batches / total_time\n",
    "samples_per_sec = len(ds_tfm) / total_time\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"- Batches/s: {throughput:.1f}\")\n",
    "print(f\"- Samples/s: {samples_per_sec:.1f}\")\n",
    "print(f\"- Batch time: {1000*total_time/num_batches:.1f}ms\")\n",
    "print(f\"- Total time: {total_time:.2f}s\")\n",
    "\n",
    "\n",
    "# Warming up...\n",
    "# Testing with 1657 batches...\n",
    "\n",
    "# Results:\n",
    "# - Batches/s: 47.8\n",
    "# - Samples/s: 1527.5\n",
    "# - Batch time: 20.9ms\n",
    "# - Total time: 34.69s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create DataLoader with your actual training parameters\n",
    "loader = DataLoader(\n",
    "    dataset=ds_tfm,  # Your dataset with set_transform\n",
    "    batch_size=64,            # Use your real batch size\n",
    "    num_workers=1,            # Match your training setup\n",
    "    pin_memory=True,          # Same as training config\n",
    "    shuffle=False             # Disable for consistent measurement\n",
    ")\n",
    "\n",
    "# 2. Warm-up run (initial batches are slower due to setup)\n",
    "print(\"Warming up...\")\n",
    "for _ in loader: pass\n",
    "\n",
    "# 3. Timed measurement\n",
    "num_batches = len(loader)\n",
    "print(f\"Testing with {num_batches} batches...\")\n",
    "\n",
    "start_time = time.perf_counter()  # More precise timer\n",
    "for _ in loader:\n",
    "    pass\n",
    "total_time = time.perf_counter() - start_time\n",
    "\n",
    "# 4. Calculate metrics\n",
    "throughput = num_batches / total_time\n",
    "samples_per_sec = len(ds_tfm) / total_time\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"- Batches/s: {throughput:.1f}\")\n",
    "print(f\"- Samples/s: {samples_per_sec:.1f}\")\n",
    "print(f\"- Batch time: {1000*total_time/num_batches:.1f}ms\")\n",
    "print(f\"- Total time: {total_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}