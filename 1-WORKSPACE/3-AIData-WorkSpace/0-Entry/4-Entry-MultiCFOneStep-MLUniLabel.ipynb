{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd \n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "KEY = 'WorkSpace'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0] + KEY\n",
    "# print(WORKSPACE_PATH)\n",
    "os.chdir(WORKSPACE_PATH)\n",
    "import sys\n",
    "from proj_space import SPACE\n",
    "sys.path.append(SPACE['CODE_FN'])\n",
    "SPACE['WORKSPACE_PATH'] = WORKSPACE_PATH\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Record and Case Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_case.GROUP import GROUP_TO_GROUPMethodArgs\n",
    "from config.config_case.CF import CF_to_CFArgs\n",
    "from config.config_case.CKPD import Ckpd_to_CkpdObsConfig\n",
    "from config.config_case.TagRec import TagRec_to_TagRecArgs\n",
    "from config.config_case.TagCF import TagCF_to_TagCFArgs \n",
    "from config.config_case.Flt import FltName_to_FltArgs\n",
    "from config.config_case.CASE import TriggerCaseBaseName_to_TriggerCaseBaseArgs\n",
    "\n",
    "from config.config_record.Cohort import CohortName_to_OneCohortArgs\n",
    "from config.config_case.CKPD import Ckpd_to_CkpdObsConfig\n",
    "\n",
    "from recfldtkn.record_base import Record_Base\n",
    "from recfldtkn.case_base.case_base import Case_Base\n",
    "\n",
    "CohortNames = [i for i in CohortName_to_OneCohortArgs.keys()]\n",
    "print(CohortNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "Inference_Entry = None # this is not inference mode\n",
    "Case_Args_Settings = {\n",
    "    'Ckpd_to_CkpdObsConfig': Ckpd_to_CkpdObsConfig,\n",
    "    'CF_to_CFArgs': CF_to_CFArgs,\n",
    "    'TagCF_to_TagCFArgs': TagCF_to_TagCFArgs,\n",
    "    'TagRec_to_TagRecArgs': TagRec_to_TagRecArgs,\n",
    "    'FltName_to_FltArgs': FltName_to_FltArgs,\n",
    "    'GROUP_TO_GROUPMethodArgs': GROUP_TO_GROUPMethodArgs,\n",
    "}\n",
    "\n",
    "Record_Proc_Config = {\n",
    "    'save_data': True, \n",
    "    'load_data':True, \n",
    "    'via_method': 'ds',\n",
    "}\n",
    "\n",
    "Case_Proc_Config = {\n",
    "    'max_trigger_case_num': None, \n",
    "    'use_task_cache': False, \n",
    "    'caseset_chunk_size': 10000, # 200k for CGM, 50k for others.\n",
    "    'save_data': True, \n",
    "    'load_data': True, \n",
    "    'load_casecollection': True,\n",
    "    'via_method': 'ds',\n",
    "    'n_cpus': 4, \n",
    "    'batch_size': 1000,  \n",
    "}\n",
    "###################################  \n",
    "\n",
    "CohortName_list = [ \n",
    "    'WellDoc2023CVSDeRx',\n",
    "]\n",
    "\n",
    "TriggerCaseBaseName = 'WeightEntry-FutureWeightAndMultiHistoricalEgm'\n",
    "TriggerCaseBaseArgs = {\n",
    "    'Trigger': {\n",
    "        'TriggerName': 'WeightEntry', \n",
    "        'TagRec': [\n",
    "            'TagRec.PDemoFromP',\n",
    "        ],\n",
    "        'Filter': 'FltBasicDemo',\n",
    "        'Group': 'GrpGenderDisease', # <--- get CaseSetName_to_CaseSet \n",
    "        'ObsTask': {\n",
    "            'TagCF_list': [\n",
    "                'TagCF.FutureWeightInfo', \n",
    "            ],\n",
    "            'CF_list':  [\n",
    "                'cf.PDemo',\n",
    "                'cf.Bf1mRecNum',\n",
    "                'cf.Bf24hCGMFeat',\n",
    "                'cf.Bf24hMedalFeat',\n",
    "                'cf.Bf1mMedalFeat',\n",
    "                'cf.Bf2mMedalFeat',\n",
    "\n",
    "                'cf.FutureWeightInfo',\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "}\n",
    "TriggerCaseBaseName_to_TriggerCaseBaseArgs[TriggerCaseBaseName] = TriggerCaseBaseArgs\n",
    "pprint(TriggerCaseBaseArgs, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.check import update_and_assert_CaseInfo\n",
    "from recfldtkn.check import retrive_pipeline_info\n",
    "PIPELINE_INFO = retrive_pipeline_info(SPACE)\n",
    "\n",
    "\n",
    "CaseSettingInfo = update_and_assert_CaseInfo(\n",
    "                                TriggerCaseBaseName,\n",
    "                                TriggerCaseBaseArgs,\n",
    "                                Case_Args_Settings,\n",
    "                                Case_Proc_Config, \n",
    "                                PIPELINE_INFO, \n",
    "                                )\n",
    "\n",
    "HumanRecordRecfeat_Args = CaseSettingInfo['HumanRecordRecfeat_Args']\n",
    "record_base = Record_Base(CohortName_list, \n",
    "                            HumanRecordRecfeat_Args,\n",
    "                            CohortName_to_OneCohortArgs,\n",
    "                            SPACE = SPACE, \n",
    "                            Inference_Entry = Inference_Entry,\n",
    "                            Record_Proc_Config = Record_Proc_Config,\n",
    "                            )\n",
    "\n",
    "\n",
    "TriggerCaseBaseName_to_CohortNameList = {\n",
    "    TriggerCaseBaseName: CohortName_list,\n",
    "}\n",
    "\n",
    "TriggerCaseBaseName_to_CohortNameList\n",
    "\n",
    "case_base = Case_Base(\n",
    "    record_base = record_base, \n",
    "    TriggerCaseBaseName_to_CohortNameList = TriggerCaseBaseName_to_CohortNameList, \n",
    "    TriggerCaseBaseName_to_TriggerCaseBaseArgs = TriggerCaseBaseName_to_TriggerCaseBaseArgs,\n",
    "    Case_Proc_Config = Case_Proc_Config,\n",
    "    Case_Args_Settings = Case_Args_Settings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CaseSetNameToCaseset = case_base.TriggerCaseBaseName_to_CaseSetNameToCaseset[TriggerCaseBaseName]\n",
    "CaseSetNameToCaseset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, caseset in CaseSetNameToCaseset.items(): break \n",
    "\n",
    "caseset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseset.ds_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CF for CF in case_base.TriggerCaseBaseName_to_CFtoCFvocab[TriggerCaseBaseName]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: EntryFn - Input_Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneEntryArgs = {\n",
    "    # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'SparseMatrixFromMultiCF',\n",
    "        'CF_list': [\n",
    "            'cf.Bf2mMedalFeat',\n",
    "            'cf.Bf1mRecNum',\n",
    "            'cf.Bf1mMedalFeat',\n",
    "\n",
    "            'cf.PDemo',\n",
    "            'cf.Bf24hCGMFeat',\n",
    "            'cf.Bf24hMedalFeat',\n",
    "\n",
    "            'cf.FutureWeightInfo',\n",
    "        ],\n",
    "        # 'BeforePeriods': ['Bf24H'],\n",
    "        # 'AfterPeriods': ['Af2H'],\n",
    "        # 'InferenceMode': False, \n",
    "        'CFs_current': ['cf.PDemo',\n",
    "                        'cf.Bf24hCGMFeat',\n",
    "                        'cf.Bf24hMedalFeat',],\n",
    "        'CFs_Before': ['cf.Bf1mRecNum',\n",
    "                       'cf.Bf1mMedalFeat',\n",
    "                       'cf.Bf2mMedalFeat',],\n",
    "        'CFs_After': ['cf.FutureWeightInfo'], \n",
    "        'InferenceMode': 'FullCFs', # 'CurrentCFs' # \n",
    "    }, \n",
    "}\n",
    "\n",
    "EntryInputMethod = OneEntryArgs['Input_Part']['EntryInputMethod']\n",
    "\n",
    "# caseset\n",
    "Data = {'df_case': caseset.df_case, 'ds_case': caseset.ds_case}\n",
    "\n",
    "CF_to_CFvocab = case_base.TriggerCaseBaseName_to_CFtoCFvocab[TriggerCaseBaseName]\n",
    "print([i for i in CF_to_CFvocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InputCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import datasets\n",
    "import inspect\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import itertools\n",
    "\n",
    "## %%%%%%%%%%%%%%%%%%%%% user functions\n",
    "def get_INPUT_CFs(OneEntryArgs):\n",
    "    Input_Part = OneEntryArgs['Input_Part']\n",
    "    CF_list = Input_Part['CF_list']\n",
    "    ############################ # INPUT_CFs\n",
    "    assert type(CF_list) == list, f'InputCFs must be a list, but got {type(CF_list)}'\n",
    "    # INPUT_CFs = sorted(InputCFs_Args)\n",
    "    INPUT_CFs = CF_list\n",
    "\n",
    "    InferenceMode = Input_Part['InferenceMode'] \n",
    "    # BeforePeriods = Input_Part['BeforePeriods']\n",
    "    # TargetField = Input_Part['TargetField']\n",
    "    if InferenceMode == 'FullCFs':\n",
    "        INPUT_CFs = INPUT_CFs\n",
    "    elif InferenceMode == 'CurrentCFs':\n",
    "        CFs_current = Input_Part['CFs_current']\n",
    "        INPUT_CFs = CFs_current\n",
    "    else:\n",
    "        raise ValueError(f'Invalid InferenceMode: {InferenceMode}')\n",
    "\n",
    "    ############################\n",
    "    return INPUT_CFs\n",
    "\n",
    "get_INPUT_CFs.fn_string = inspect.getsource(get_INPUT_CFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputCFs = get_INPUT_CFs(OneEntryArgs)\n",
    "InputCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_case = Data['ds_case']\n",
    "ds_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ds_case.shuffle(seed=42)[:5] # .select(range(5))  \n",
    "# examples = ds_case[:4] \n",
    "pprint(examples, sort_dicts=False, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfm_fn_AIInputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Part = OneEntryArgs['Input_Part']\n",
    "CF_list = Input_Part['CF_list']\n",
    "CF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF = CF_list[-1]\n",
    "CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_to_CF_type_id = {cf: idx+1 for idx, cf in enumerate(CF_list)}\n",
    "CF_to_CF_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CF_to_CFvocab \n",
    "CF_to_CFvocabsize = {cf: len(CF_to_CFvocab[cf]['input_ids']['tid2tkn']) for cf in CF_to_CFvocab}\n",
    "CF_to_CFvocabsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(examples_cf, sort_dicts=False, compact=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "import numpy as np \n",
    "\n",
    "def pad_with_numpy(lst, fill_value=0):\n",
    "    # Handle empty list\n",
    "    if not lst:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Convert single-level list to numpy array\n",
    "    if not isinstance(lst[0], list):\n",
    "        return np.array(lst)\n",
    "    \n",
    "    # Find maximum lengths at each nesting level\n",
    "    def get_max_lens(curr_lst, level=0, max_lens=None):\n",
    "        if max_lens is None:\n",
    "            max_lens = []\n",
    "            \n",
    "        # Update max length at current level\n",
    "        if level == len(max_lens):\n",
    "            max_lens.append(0)\n",
    "        max_lens[level] = max(max_lens[level], len(curr_lst))\n",
    "        \n",
    "        # Recursively check all sublists\n",
    "        for item in curr_lst:\n",
    "            if isinstance(item, list):\n",
    "                get_max_lens(item, level + 1, max_lens)\n",
    "                \n",
    "        return max_lens\n",
    "    \n",
    "    max_lens = get_max_lens(lst)\n",
    "    \n",
    "    # Pad each level\n",
    "    def pad_recursive(curr_lst, level=0):\n",
    "        if not isinstance(curr_lst, list):\n",
    "            return curr_lst\n",
    "            \n",
    "        # Pad current level\n",
    "        curr_max_len = max_lens[level]\n",
    "        padded = curr_lst + [fill_value] * (curr_max_len - len(curr_lst))\n",
    "        \n",
    "        # Recursively pad sublists\n",
    "        if level + 1 < len(max_lens):\n",
    "            padded = [pad_recursive(item, level + 1) if isinstance(item, list) \n",
    "                     else [fill_value] * max_lens[level + 1] \n",
    "                     for item in padded]\n",
    "            \n",
    "        return padded\n",
    "    \n",
    "    padded_lst = pad_recursive(lst)\n",
    "    \n",
    "    # Convert to numpy array and apply max_subtoken_len if needed\n",
    "    arr = np.array(padded_lst)\n",
    "    # if len(arr.shape) == 3: arr = arr[:, :, :max_subtoken_len]\n",
    "    return arr\n",
    "\n",
    "pad_with_numpy.fn_string = inspect.getsource(pad_with_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_to_CF_inputs = {}\n",
    "\n",
    "\n",
    "for CF in CF_list:\n",
    "\n",
    "    examples_cf = {k.split('--')[-1]:v for k,v in examples.items() if CF in k}\n",
    "\n",
    "    # current CF is not avaialable in examples\n",
    "    if len(examples_cf) == 0: continue\n",
    "\n",
    "\n",
    "    # pprint(examples_cf, sort_dicts=False, compact=True)\n",
    "    # pad the input_ids\n",
    "    for k, v in examples_cf.items():\n",
    "        examples_cf[k] = pad_with_numpy(v)\n",
    "\n",
    "    # pprint(examples_cf, sort_dicts=False, compact=True) \n",
    "\n",
    "    cf_type_id = CF_to_CF_type_id[CF]\n",
    "    cf_type_vocabsize = CF_to_CFvocabsize[CF]\n",
    "    PAD_ID = 0\n",
    "    mask = examples_cf['input_ids'] != PAD_ID\n",
    "    attention_mask = mask.astype(int)\n",
    "    field_type_id = attention_mask * cf_type_id\n",
    "    field_type_vocabsize = attention_mask * cf_type_vocabsize\n",
    "\n",
    "    examples_cf['attention_mask'] = attention_mask\n",
    "    examples_cf['field_type_id'] = field_type_id\n",
    "    examples_cf['field_type_vocabsize'] = field_type_vocabsize\n",
    "    # pprint(examples_cf, sort_dicts=False, compact=True) \n",
    "    CF_to_CF_inputs[CF] = examples_cf\n",
    "\n",
    "pprint(CF_to_CF_inputs, sort_dicts=False, compact=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: masked_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_cf = CF_to_CF_inputs[CF]\n",
    "input_ids = examples_cf['input_ids']\n",
    "input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "\n",
    "def masking_examples_input_ids(input_ids, \n",
    "                               MASK_ID, \n",
    "                               PAD_ID=0,\n",
    "                               UNK_ID=1,\n",
    "                               mask_probability=0.15):\n",
    "    \n",
    "    # Flatten input_ids into a tensor\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "    # Create a mask to ignore padding tokens\n",
    "    padding_mask = input_ids != PAD_ID\n",
    "\n",
    "    # Create a placeholder with UNK_ID for non-pad tokens\n",
    "    input_placeholder_ids = padding_mask * UNK_ID\n",
    "\n",
    "    # Create a copy of input_ids for labels\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Generate a mask for masking tokens\n",
    "    probability_matrix = torch.full(labels.shape, mask_probability)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "    # Avoid masking padding tokens\n",
    "    masked_indices[~padding_mask] = False\n",
    "\n",
    "    # Replace masked positions with `[MASK]` token ID\n",
    "    input_ids[masked_indices] = MASK_ID\n",
    "\n",
    "    # Set labels to -100 for padding tokens and unmasked positions\n",
    "    labels[~masked_indices] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_masked_ids\": input_ids,\n",
    "        \"input_placeholder_ids\": input_placeholder_ids,\n",
    "        \"mlm_labels\": labels\n",
    "    }\n",
    "\n",
    "masking_examples_input_ids.fn_string = inspect.getsource(masking_examples_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "MASK_ID = 1002\n",
    "mask_probability = 0.15\n",
    "\n",
    "masked_inputs = masking_examples_input_ids(input_ids, \n",
    "                                           MASK_ID, \n",
    "                                           PAD_ID,\n",
    "                                           UNK_ID,\n",
    "                                           mask_probability, )\n",
    "\n",
    "masked_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: sequence prediction label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_list\n",
    "\n",
    "CF = CF_list[-1]\n",
    "CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_inputs = CF_to_CF_inputs[CF]\n",
    "CF_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = CF_inputs['input_ids']\n",
    "input_ids\n",
    "\n",
    "\n",
    "# check whether certain token is their. \n",
    "\n",
    "\n",
    "tokens_as_labels = [\n",
    "    'co.Weight_Af2Minfo:future_bmi_above25',\n",
    "    'co.Weight_Af2Minfo:no_future_weight',\n",
    "]\n",
    "\n",
    "\n",
    "CF_vocab = CF_to_CFvocab[CF]\n",
    "# CF_vocab['input_ids']['tid2tkn']\n",
    "tokenids_as_labels = [\n",
    "    CF_vocab['input_ids']['tkn2tid'][token] for token in tokens_as_labels\n",
    "]\n",
    "print(tokenids_as_labels)\n",
    "\n",
    "\n",
    "labelid_to_tokenid = {k+1:v for k,v in enumerate(tokenids_as_labels)}\n",
    "print(labelid_to_tokenid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.zeros(input_ids.shape[0], dtype=torch.long)\n",
    "for labelid, tokenid in labelid_to_tokenid.items():\n",
    "    print(labelid, tokenid)\n",
    "    labels_single_type = (input_ids == tokenid).sum(axis=1) == 1\n",
    "    print(labels_single_type)\n",
    "    labels[labels_single_type] = labelid\n",
    "    print(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_CF = CFs_current[0]\n",
    "input_CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = examples[f'{CF}--input_ids']\n",
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_tfm = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_tfm['input_ids'] = input_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import itertools\n",
    "\n",
    "## %%%%%%%%%%%%%%%%%%%%% user functions\n",
    "def tfm_fn_AIInputData(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def get_INPUT_CFs(OneEntryArgs):\n",
    "    Input_Part = OneEntryArgs['Input_Part']\n",
    "    CF_list = Input_Part['CF_list']\n",
    "    ############################ # INPUT_CFs\n",
    "    # assert type(InputCFs_Args) == list, f'InputCFs_Args must be a list, but got {type(InputCFs_Args)}'\n",
    "    INPUT_CFs = sorted(CF_list) # why sorted here?\n",
    "    ############################\n",
    "    return INPUT_CFs\n",
    "    \n",
    "\n",
    "def entry_fn_AIInputData(Data, \n",
    "                         CF_to_CFvocab, \n",
    "                         OneEntryArgs,\n",
    "                         tfm_fn_AIInputData = None):\n",
    "\n",
    "    ds_case = Data['ds_case']\n",
    "    # Input feaures. \n",
    "    \n",
    "    INPUT_CFs = get_INPUT_CFs(OneEntryArgs)\n",
    "    # print('\\n\\n\\n\\n ---------- INPUT_CFs\" {} --------- \\n\\n\\n\\n'.format(INPUT_CFs))\n",
    "    \n",
    "    \n",
    "    accumulated_matrices = []  # Initialize a list to accumulate the sparse matrices\n",
    "    for INPUT_CF in INPUT_CFs:\n",
    "        CF_vocab = CF_to_CFvocab[INPUT_CF]\n",
    "        \n",
    "        tid2tkn = CF_vocab['input_ids']['tid2tkn']\n",
    "        num_features = len(tid2tkn)\n",
    "\n",
    "        # tid2tkn_filter = EntryArgs.get('tid2tkn_filter', None)\n",
    "        input_ids_name  = f'{INPUT_CF}--input_ids'\n",
    "        input_wgts_name = f'{INPUT_CF}--input_wgts'\n",
    "\n",
    "        col_indices = list(itertools.chain(*[          tid  for i,   tid in enumerate(ds_case[input_ids_name])]))\n",
    "        row_indices = list(itertools.chain(*[[i] * len(tid) for i,   tid in enumerate(ds_case[input_ids_name])]))\n",
    "        data        = list(itertools.chain(*[          wgt  for tid, wgt in zip(ds_case[input_ids_name], ds_case[input_wgts_name])]))\n",
    "        \n",
    "        sparse_matrix_value = (data, (row_indices, col_indices))\n",
    "        shape = (len(ds_case), num_features)\n",
    "        X = csr_matrix(sparse_matrix_value, shape=shape)\n",
    "        \n",
    "        # Inside your loop, after creating each X, append it to the list:\n",
    "        accumulated_matrices.append(X)\n",
    "\n",
    "    # After the loop, concatenate all sparse matrices horizontally\n",
    "    X = hstack(accumulated_matrices, format='csr')\n",
    "\n",
    "    ds_tfm = {'X': X}\n",
    "    Data['ds_tfm'] = ds_tfm\n",
    "    return Data\n",
    "\n",
    "\n",
    "get_INPUT_CFs.fn_string = inspect.getsource(get_INPUT_CFs)\n",
    "tfm_fn_AIInputData.fn_string = inspect.getsource(tfm_fn_AIInputData)\n",
    "entry_fn_AIInputData.fn_string = inspect.getsource(entry_fn_AIInputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = entry_fn_AIInputData(Data, \n",
    "                            CF_to_CFvocab, \n",
    "                            OneEntryArgs,\n",
    "                            tfm_fn_AIInputData)\n",
    "\n",
    "\n",
    "ds_tfm = Data['ds_tfm']\n",
    "ds_tfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save Entry Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.entry import AIDATA_ENTRYINPUT_PATH\n",
    "from recfldtkn.base import Base\n",
    "\n",
    "pypath = os.path.join(SPACE['CODE_FN'],  AIDATA_ENTRYINPUT_PATH, f'{EntryInputMethod}.py')\n",
    "print(pypath) \n",
    "\n",
    "prefix = [\n",
    "    'import itertools',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np', \n",
    "    'import datasets',\n",
    "    'from scipy.sparse import csr_matrix, hstack',\n",
    "    ]\n",
    "\n",
    "fn_variables = [\n",
    "    get_INPUT_CFs,\n",
    "    tfm_fn_AIInputData,\n",
    "    entry_fn_AIInputData,\n",
    "]\n",
    "\n",
    "pycode = Base.convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "\n",
    "# print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EntryFn - Output_Part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaskType = 'MLUniLabel'\n",
    "SeriesName  = 'weightpred.Af1M'\n",
    "\n",
    "OneTaskName = 'WeightPred.Af1M.WeightLossPctLarge2'\n",
    "OneEntryArgs = {\n",
    "    # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': 'SparseMatrixFromMultiCF',\n",
    "        'CF_list': [\n",
    "            'cf.PDemo',\n",
    "            'cf.Bf1mRecNum',\n",
    "            'cf.Bf24hCGMFeat',\n",
    "            'cf.Bf24hMedalFeat',\n",
    "            'cf.Bf1mMedalFeat',\n",
    "            'cf.Bf2mMedalFeat',\n",
    "        ],\n",
    "    }, \n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'MLUniLabel',\n",
    "        'TagCF_list': [\n",
    "            'TagCF.FutureWeightInfo', \n",
    "        ], \n",
    "        'Labeling': ('co.Weight_Af1Minfo:weight_loss_pct', '>', 0.02), \n",
    "    },\n",
    "\n",
    "\n",
    "    # ----------------- Task Part -----------------\n",
    "    'Task_Part': {\n",
    "        'Tagging': [],\n",
    "        'Filtering': [\n",
    "            ('co.Weight_Af1Minfo:no_future_weight', '!=', 1),\n",
    "        ], \n",
    "    },\n",
    "}\n",
    "\n",
    "# EntryInputMethod = OneEntryArgs['EntryInputMethod']\n",
    "EntryOutputMethod = OneEntryArgs['Output_Part']['EntryOutputMethod']\n",
    "# caseset\n",
    "Data = {'df_case': caseset.df_case, 'ds_case': caseset.ds_case}\n",
    "\n",
    "CF_to_CFvocab = case_base.TriggerCaseBaseName_to_CFtoCFvocab[TriggerCaseBaseName]\n",
    "print([i for i in CF_to_CFvocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%%%%%%%%%%%%%%%%\n",
    "# UniLabel\n",
    "import inspect \n",
    "import numpy as np \n",
    "# from recfldtkn.loadtools import convert_variables_to_pystirng\n",
    "\n",
    "\n",
    "def get_OUTPUT_CFs(OneEntryArgs):\n",
    "    if 'Output_Part' not in OneEntryArgs:\n",
    "        return []\n",
    "    else:\n",
    "        return OneEntryArgs['Output_Part'].get('CF_list', [])\n",
    "get_OUTPUT_CFs.fn_string = inspect.getsource(get_OUTPUT_CFs)\n",
    "\n",
    "\n",
    "def entry_fn_AITaskData(Data, \n",
    "                        CF_to_CFvocab, \n",
    "                        OneEntryArgs,\n",
    "                        tfm_fn_AIInputData = None,\n",
    "                        entry_fn_AIInputData = None,\n",
    "                        ):\n",
    "\n",
    "    Data = entry_fn_AIInputData(Data, CF_to_CFvocab, OneEntryArgs, tfm_fn_AIInputData) \n",
    "    \n",
    "    \n",
    "    Output_Part = OneEntryArgs['Output_Part']\n",
    "    Labeling = Output_Part['Labeling']\n",
    "    # assert type(Labeling) == tuple, f'Labeling must be a tuple, but got {type(Labeling)}'\n",
    "    assert len(Labeling) == 3, f'Labeling must have 3 elements, but got {len(Labeling)}'\n",
    "    label_name, label_op, label_value = Labeling\n",
    "    df_case = Data['df_case']\n",
    "    \n",
    "    if label_op == '>':\n",
    "        Y = df_case[label_name] > label_value\n",
    "    elif label_op == '<':\n",
    "        Y = df_case[label_name] < label_value\n",
    "    elif label_op == '==':\n",
    "        Y = df_case[label_name] == label_value\n",
    "    elif label_op == 'in':\n",
    "        Y = df_case[label_name].isin(label_value)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid label_op: {label_op}')\n",
    "    \n",
    "    Y = Y.astype(int).values\n",
    "    ds_tfm = Data['ds_tfm']\n",
    "    ds_tfm['Y'] = Y\n",
    "    Data['ds_tfm'] = ds_tfm\n",
    "    return Data\n",
    "\n",
    "entry_fn_AITaskData.fn_string = inspect.getsource(entry_fn_AITaskData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = entry_fn_AITaskData(Data, \n",
    "                           CF_to_CFvocab, \n",
    "                           OneEntryArgs,\n",
    "                           tfm_fn_AIInputData,\n",
    "                           entry_fn_AIInputData)\n",
    "\n",
    "ds_tfm = Data['ds_tfm']\n",
    "ds_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfm['Y'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import Base\n",
    "from recfldtkn.aidata_base.entry import AIDATA_ENTRYOUTPUT_PATH\n",
    "\n",
    "prefix = [\n",
    "    'import torch',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np', \n",
    "    'import datasets'\n",
    "    ]\n",
    "fn_variables = [\n",
    "    get_OUTPUT_CFs,\n",
    "    entry_fn_AITaskData,\n",
    "]\n",
    "pycode = Base.convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], AIDATA_ENTRYOUTPUT_PATH, f'{EntryOutputMethod}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}