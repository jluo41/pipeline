{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "KEY = '1-WORKSPACE'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0]\n",
    "print(WORKSPACE_PATH); os.chdir(WORKSPACE_PATH)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "SPACE = {\n",
    "    'DATA_RAW': f'_Data/0-Data_Raw',\n",
    "    'DATA_RFT': f'_Data/1-Data_RFT',\n",
    "    'DATA_CASE': f'_Data/2-Data_CASE',\n",
    "    'DATA_AIDATA': f'_Data/3-Data_AIDATA',\n",
    "    'DATA_SPLIT': f'_Data/4-Data_Split', \n",
    "    'DATA_EXTERNAL': f'code/external',\n",
    "    'CODE_FN': f'code/pipeline', \n",
    "    'MODEL_ROOT': f'_Model',\n",
    "}\n",
    "assert os.path.exists(SPACE['CODE_FN']), f'{SPACE[\"CODE_FN\"]} not found'\n",
    "\n",
    "print(SPACE['CODE_FN'])\n",
    "sys.path.append(SPACE['CODE_FN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "from recfldtkn.case_base.casefnutils.casefn import Case_Fn #  import AIDATA_ENTRYINPUT_PATH\n",
    "\n",
    "######################## get the CF_DataName list\n",
    "CF_DataName = 'CGMwithDietBf8h-CaseBase-CGM5MinEntry-31ec84c0520b37c1'\n",
    "CohortName_list = [\n",
    "    'WellDoc2022CGM',\n",
    "    'WellDoc2025ALS',\n",
    "    'WellDoc2025CVS', \n",
    "    'WellDoc2025LLY',\n",
    "]\n",
    "######################## \n",
    "\n",
    "######################## get the CF_DataName list\n",
    "CF_DataName_list = [\n",
    "    f'{CF_DataName}/{i}' for i in CohortName_list\n",
    "]\n",
    "########################\n",
    "\n",
    "ds_list = []\n",
    "ref_config = None\n",
    "ref_column_names = None\n",
    "for i, CF_DataName in enumerate(CF_DataName_list):\n",
    "    path = os.path.join(SPACE['DATA_AIDATA'], CF_DataName)\n",
    "    ds = datasets.load_from_disk(path)\n",
    "    print(CF_DataName, ds )\n",
    "    # config = copy.deepcopy(ds.info.config.__dict__) if hasattr(ds.info, 'config') else {}\n",
    "    config = ds.config_name\n",
    "    column_names = ds.column_names\n",
    "    ds_list.append(ds)\n",
    "\n",
    "# pprint(config)\n",
    "dataset = datasets.concatenate_datasets(ds_list)\n",
    "\n",
    "CF_list = list(set([i.split('--')[0] for i in dataset.column_names if '--tid' in i]))\n",
    "CF_fn_list = [Case_Fn(CF, SPACE) for CF in CF_list]\n",
    "CF_to_CFvocab = {CF: CF_fn.COVocab for CF, CF_fn in zip(CF_list, CF_fn_list)}\n",
    "\n",
    "CF_DataName = config['TriggerCaseBaseName']\n",
    "TriggerCaseBaseArgs = config['TriggerCaseBaseName_to_TriggerCaseBaseArgs'][CF_DataName]\n",
    "TriggerName = TriggerCaseBaseArgs['Trigger']['TriggerName']\n",
    "\n",
    "logger.info(f'set up TriggerName: {TriggerName}')\n",
    "logger.info(f'set up CF_Config: {[i for i in config]}')\n",
    "config['CF_to_CFvocab'] = CF_to_CFvocab\n",
    "\n",
    "print('total', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### should be a split here #######\n",
    "\n",
    "Data = {'ds_case': dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT: RandomByPat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneEntryArgs = {\n",
    "    'Split_Part': {\n",
    "        'SplitMethod': 'SplitFromTable', # <--- you need to design this function.\n",
    "        'TablePath': f'{SPACE[\"DATA_SPLIT\"]}/Split_All_WellDoc.parquet',\n",
    "        # 'SplitRatio': {'train': 0.8, 'valid': 0.1, 'test': 0.1, 'random_state': 42},\n",
    "        'ObsDT_Minute': True,\n",
    "        'Split_to_Selection': {\n",
    "            'train': {'Rules': [\n",
    "                                ['split', '==', 'train-early'],\n",
    "                                ['MEDInfoBf24h-DietRecNum', '>', 0],\n",
    "                                ['MEDInfoBf24h-DietLastToNow', '>=', 120], \n",
    "                                ['MEDInfoBf24h-DietLastToNow', '<=', 420], \n",
    "                                ['ObsDT_Minute', '==', 0], \n",
    "\n",
    "                                ],\n",
    "                      'Op': 'and'},\n",
    "\n",
    "\n",
    "\n",
    "            'valid':  {'Rules': [\n",
    "                                ['split', '==', 'val-early'],\n",
    "                                # ['split', '==', 'val-late'],\n",
    "\n",
    "                                ['MEDInfoBf24h-DietRecNum', '>', 0],\n",
    "                                ['MEDInfoBf24h-DietLastToNow', '>=', 120], \n",
    "                                ['MEDInfoBf24h-DietLastToNow', '<=', 420], \n",
    "                                ['ObsDT_Minute', '==', 0], \n",
    "\n",
    "\n",
    "                                ],\n",
    "                      'Op': 'and'},\n",
    "            'test':  {'Rules': [\n",
    "                                ['split', '==', 'test-early'],\n",
    "\n",
    "                                ['MEDInfoBf24h-DietRecNum', '>', 0],\n",
    "                                ['MEDInfoBf24h-DietLastToNow', '>=', 120], \n",
    "                                ['MEDInfoBf24h-DietLastToNow', '<=', 420], \n",
    "                                ['ObsDT_Minute', '==', 0], \n",
    "\n",
    "\n",
    "                                ],\n",
    "                      'Op': 'and'},\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # ----------------- Input Part -----------------\n",
    "    'Input_Part': {\n",
    "        'EntryInputMethod': '1TknInStep',\n",
    "        'CF_list': [\n",
    "            'CGMValueBf24h',\n",
    "            # 'CGMValueAf2h',\n",
    "        ],\n",
    "        'BeforePeriods': ['Bf24h'],\n",
    "        # 'AfterPeriods': ['Af2h'],\n",
    "        'InferenceMode': False, # True, # True, # False, # True, \n",
    "        'TargetField': 'CGMValue', \n",
    "    }, \n",
    "\n",
    "\n",
    "    # ----------------- Output Part -----------------\n",
    "    'Output_Part': {\n",
    "        'EntryOutputMethod': 'UniLabelRules',\n",
    "        'CF_list': ['MEDInfoBf24h'],\n",
    "        'label_rule': {\n",
    "            1: ('MEDInfoBf24h-DietLastToNow', 'in', [120, 180]), # eat between before 2 to 3 hours\n",
    "            0: ('MEDInfoBf24h-DietLastToNow', 'in', [180, 420]), # eat between before 3 to 7 hours\n",
    "            -100: 'others'\n",
    "        },\n",
    "        'assertion': [\n",
    "            ('MEDInfoBf24h-DietLastToNow', 'in', [120, 420]),\n",
    "        ],\n",
    "        'set_transform': False,\n",
    "        'num_proc': 4, \n",
    "    },\n",
    "}\n",
    "\n",
    "SplitMethod = OneEntryArgs['Split_Part']['SplitMethod']\n",
    "SplitMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.entry import EntryAIData_Builder\n",
    "\n",
    "entry = EntryAIData_Builder(OneEntryArgs = OneEntryArgs, \n",
    "                            SPACE = SPACE)\n",
    "\n",
    "# tfm_fn_AIInputData = entry.tfm_fn_AIInputData\n",
    "# entry_fn_AIInputData = entry.entry_fn_AIInputData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_columns = [i for i in dataset.column_names if '--' not in i]\n",
    "df_tag = dataset.select_columns(tag_columns).to_pandas()\n",
    "# df_tag = dataset_split_tagging_fn(df_tag, OneEntryArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_Part = OneEntryArgs['Split_Part']\n",
    "TablePath = Split_Part['TablePath']\n",
    "df_pre_split = pd.read_parquet(TablePath).rename(columns = {'ObsDT': 'ObsDay'})\n",
    "\n",
    "\n",
    "\n",
    "df_tag['ObsDay'] = pd.to_datetime(df_tag['ObsDT']).dt.date\n",
    "\n",
    "columns = ['PID', 'ObsDay', 'has_event', 'has_diet',\n",
    "              'has_med', 'has_exercise', 'age', 'age_group', 'time_bin',\n",
    "              'days_to_split', 'date_idx', 'split']\n",
    "df_pre_split['ObsDay'] = pd.to_datetime(df_pre_split['ObsDay']).dt.date\n",
    "\n",
    "df_pre_split = df_pre_split[columns].reset_index(drop = True)# .columns\n",
    "\n",
    "print(df_pre_split.shape)\n",
    "df_pre_split = df_pre_split[df_pre_split['PID'].isin(df_tag['PID'])].reset_index(drop = True) # .drop_duplicates(subset = ['PID', 'ObsDay'])\n",
    "print(df_pre_split.shape)\n",
    "print(df_tag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag = pd.merge(df_tag, df_pre_split, how = 'left', on = ['PID', 'ObsDay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import inspect \n",
    "\n",
    "\n",
    "########################################################\n",
    "def dataset_split_tagging_fn(df_tag, OneEntryArgs):\n",
    "    df_tag['ObsDay'] = pd.to_datetime(df_tag['ObsDT']).dt.date\n",
    "    Split_Part = OneEntryArgs['Split_Part']\n",
    "    TablePath = Split_Part['TablePath']\n",
    "    df_pre_split = pd.read_parquet(TablePath).rename(columns = {'ObsDT': 'ObsDay'})\n",
    "    columns = ['PID', 'ObsDay', 'has_event', 'has_diet',\n",
    "                'has_med', 'has_exercise', 'age', 'age_group', 'time_bin',\n",
    "                'days_to_split', 'date_idx', 'split']\n",
    "    df_pre_split['ObsDay'] = pd.to_datetime(df_pre_split['ObsDay']).dt.date\n",
    "    df_pre_split = df_pre_split[columns].reset_index(drop = True)# .columns\n",
    "    df_pre_split = df_pre_split[df_pre_split['PID'].isin(df_tag['PID'])].reset_index(drop = True)\n",
    "\n",
    "    print(df_pre_split.shape)\n",
    "    print(df_tag.shape)\n",
    "    df_tag = pd.merge(df_tag, df_pre_split, how = 'left', on = ['PID', 'ObsDay'])\n",
    "    print(df_tag.shape)\n",
    "    if Split_Part['ObsDT_Minute']:\n",
    "        df_tag['ObsDT_Minute'] = df_tag['ObsDT'].dt.minute\n",
    "\n",
    "    return df_tag\n",
    "\n",
    "dataset_split_tagging_fn.fn_string = inspect.getsource(dataset_split_tagging_fn)\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_columns = [i for i in dataset.column_names if '--' not in i]\n",
    "df_tag = dataset.select_columns(tag_columns).to_pandas()\n",
    "\n",
    "df_tag = dataset_split_tagging_fn(df_tag, OneEntryArgs)\n",
    "df_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import apply_multiple_conditions\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "Split_to_Selection = OneEntryArgs['Split_Part']['Split_to_Selection']\n",
    "\n",
    "split_to_dataset = {}\n",
    "for split_name, Selection in Split_to_Selection.items():\n",
    "    # split_to_dataset[split_name] = dataset.filter(lambda x: apply_multiple_conditions(x, split_config['Rules'], split_config['Op']))\n",
    "    Rules = Selection['Rules']\n",
    "    Op = Selection['Op']\n",
    " \n",
    "    index = apply_multiple_conditions(df_tag, Rules, Op)\n",
    "    indices = np.where(index == 1)[0]\n",
    "    # len(indices)\n",
    "    dataset_selected = dataset.select(indices)\n",
    "    split_to_dataset[split_name] = dataset_selected\n",
    "\n",
    "split_to_dataset = datasets.DatasetDict(split_to_dataset)\n",
    "split_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import Base\n",
    "from recfldtkn.aidata_base.entry import AIDATA_SPLIT_PATH\n",
    "\n",
    "prefix = [\n",
    "    'import torch',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np', \n",
    "    'import datasets',\n",
    "    'from sklearn.model_selection import train_test_split'\n",
    "    ]\n",
    "fn_variables = [\n",
    "    dataset_split_tagging_fn,\n",
    "]\n",
    "pycode = Base.convert_variables_to_pystirng(fn_variables = fn_variables, prefix = prefix)\n",
    "pypath = os.path.join(SPACE['CODE_FN'], AIDATA_SPLIT_PATH, f'{SplitMethod}.py')\n",
    "print(pypath)\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(OneEntryArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.aidata_base.entry import EntryAIData_Builder\n",
    "\n",
    "entry = EntryAIData_Builder(OneEntryArgs = OneEntryArgs, \n",
    "                            SPACE = SPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_dataset = entry.split_cf_dataset(dataset)\n",
    "split_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_to_Data = entry.setup_EntryFn_to_NameToData(split_to_dataset, CF_to_CFvocab, OneEntryArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Name_to_Data['train']\n",
    "ds = Data['ds_tfm']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ds['labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}