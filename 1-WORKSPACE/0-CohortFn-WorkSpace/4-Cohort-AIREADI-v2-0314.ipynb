{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab3f242",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80023bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "KEY = '1-WORKSPACE'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0]\n",
    "print(WORKSPACE_PATH); os.chdir(WORKSPACE_PATH)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "SPACE = {\n",
    "    'DATA_RAW': f'./_Data/0-Data_Raw',\n",
    "    'DATA_RFT': f'./_Data/1-Data_RFT',\n",
    "    'DATA_CASE': f'./_Data/2-Data_CASE',\n",
    "    'DATA_AIDATA': f'./_Data/3-Data_AIDATA',\n",
    "    'DATA_EXTERNAL': f'./code/external',\n",
    "    'CODE_FN': f'./code/pipeline', \n",
    "}\n",
    "assert os.path.exists(SPACE['CODE_FN']), f'{SPACE[\"CODE_FN\"]} not found'\n",
    "\n",
    "print(SPACE['CODE_FN'])\n",
    "sys.path.append(SPACE['CODE_FN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704eb394",
   "metadata": {},
   "source": [
    "# Step 1: OneCohort_Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3ed49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This step is foundational for the rest of the notebook, as it ensures that the correct settings and parameters are in place for processing the cohort's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11053dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cohort configuration dictionary\n",
    "from config.config_record.Cohort import CohortName_to_OneCohortArgs\n",
    "\n",
    "# List available cohort names\n",
    "cohort_names = list(CohortName_to_OneCohortArgs.keys())\n",
    "print(\"Available Cohorts:\", cohort_names)\n",
    "\n",
    "# Select a specific cohort and retrieve its arguments\n",
    "# selected_cohort = 'WellDoc2023CVSDeRx'\n",
    "selected_cohort ='aireadi-noimage-v2'\n",
    "cohort_args = CohortName_to_OneCohortArgs[selected_cohort]\n",
    "print(\"Selected Cohort Arguments:\", cohort_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeaef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%%%%%%%%%%%%%%%%%%%% user\n",
    "# CohortName = 'WellDoc2022CGM'\n",
    "# CohortName = 'WellDoc2023CVSDeRx'\n",
    "# CohortName = 'WellDoc2023CVSTDC'\n",
    "\n",
    "# CohortName = 'WellDoc2025CVS'\n",
    "CohortName = 'aireadi-noimage-v2'\n",
    "# CohortName = 'WellDoc2025LLY'\n",
    "# # %%%%%%%%%%%%%%%%%%%%% \n",
    "OneCohort_Args = CohortName_to_OneCohortArgs[CohortName]\n",
    "OneCohort_Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e47628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import Cohort   # Cohort is a class\n",
    "\n",
    "# Define a placeholder for a cohort funciton \n",
    "cohort_fn = None\n",
    "# Initialize an object of the Cohort class with initial arguments\n",
    "cohort = Cohort(OneCohort_Args, SPACE, cohort_fn)  # cohort is an object of class Cohort, this object  will manage dataset parameters\n",
    "# Update the cohort arguments using the update_cohort_args method\n",
    "# Input is OneCohort_Args and SPACE, which are two dictionaries and the \n",
    "OneCohort_Args = cohort.update_cohort_args(OneCohort_Args, SPACE) # update cohort args\n",
    "# Pretty print the updated cohort argument\n",
    "pprint(OneCohort_Args, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfa716",
   "metadata": {},
   "source": [
    "# Step 2: Get Source Files\n",
    "The purpose of this code segment is to retrieve all files with a specific suffix (in this case, .csv) from a specified folder and list their paths. The folder path and file suffix list are specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "# Define file suffix list to look for .csv files\n",
    "SourceFile_SuffixList = ['json', 'tsv', 'csv', 'dat', 'hea'] \n",
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "\n",
    "# Get the folder path from OneCohort_Args dictionary (provided by the user)\n",
    "Folder = OneCohort_Args['SourcePath'] \n",
    "\n",
    "# Use the cohort object to get a list of source files with the specified suffix\n",
    "SourceFile_List = cohort.get_SourceFile_List(Folder, SourceFile_SuffixList)\n",
    "\n",
    "# Print the list of source files\n",
    "SourceFile_List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f4c0a",
   "metadata": {},
   "source": [
    "# Step 3: Get RawName from SourceFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "def get_RawName_from_SourceFile(file_path, OneCohort_Args):\n",
    "    \"\"\"\n",
    "    Extracts a 'raw name' from a given file path.\n",
    "\n",
    "    This function takes a file path and extracts what is assumed to be a 'raw name'\n",
    "    by splitting the path and selecting specific parts. The 'raw name' is considered\n",
    "    to be the last part of the file name before the file extension.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path of the file from which to extract the raw name.\n",
    "        OneCohort_Args: Currently unused. Reserved for future functionality.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted 'raw name' from the file path.\n",
    "\n",
    "    \"\"\"\n",
    "    RawName = file_path.split('_')[-1].split('.')[0]\n",
    "    return RawName\n",
    "\n",
    "get_RawName_from_SourceFile.fn_string = inspect.getsource(get_RawName_from_SourceFile)\n",
    "# %%%%%%%%%%%%%%%%%%%%% \n",
    "\n",
    "file_path = SourceFile_List[0]\n",
    "print(type(file_path))\n",
    "RawName = get_RawName_from_SourceFile(file_path, OneCohort_Args)\n",
    "print(file_path)\n",
    "print(RawName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SourceFile_List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c48a",
   "metadata": {},
   "source": [
    "# Step 4: Process Source to Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f7bc8",
   "metadata": {},
   "source": [
    "## participants & clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7599164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "import pytz\n",
    "participants_xml_file_list  = [i for i in SourceFile_List if 'participants' in i]\n",
    "participants_xml_file = participants_xml_file_list[0]\n",
    "print(participants_xml_file)\n",
    "\n",
    "participants_tsv_file = participants_xml_file.replace('.json', '.tsv')\n",
    "\n",
    "df = pd.read_csv(participants_tsv_file, sep='\\t')\n",
    "df['YearOfBirth'] = pd.to_datetime(df['study_visit_date']).dt.year - df['age']\n",
    "# df\n",
    "df['study_group'].value_counts()\n",
    "\n",
    "\n",
    "clinical_site_to_timezone = {\n",
    "    'UAB': 'America/Chicago',\n",
    "    'UW': 'America/Los_Angeles',\n",
    "    'UCSD': 'America/Los_Angeles',\n",
    "}\n",
    "\n",
    "df['UserTimeZone'] = df['clinical_site'].map(clinical_site_to_timezone)\n",
    "# df['UserTimeZoneOffset'] = df['UserTimeZone'].apply(lambda tz: int(datetime.now(pytz.timezone(tz)).utcoffset().total_seconds() / 3600))\n",
    "df['DiseaseType'] = df['study_group'].map({\n",
    "    'healthy': 0, \n",
    "    'pre_diabetes_lifestyle_controlled': 0.5, \n",
    "    'oral_medication_and_or_non_insulin_injectable_medication_controlled': 2, \n",
    "    'insulin_dependent': 2})\n",
    "\n",
    "df['MRSegmentID'] = df['study_group']\n",
    "\n",
    "RawName = 'Patient'\n",
    "raw_columns = ['PatientID', \n",
    "               'MRSegmentID', 'MRSegmentModifiedDateTime', 'DiseaseType',\n",
    "                             'Gender', 'ActivationDate', 'UserTimeZoneOffset', 'UserTimeZone',\n",
    "                             'Description', 'YearOfBirth']\n",
    "\n",
    "df = df.rename(columns = {'participant_id': 'PatientID'})\n",
    "df = df.reindex(columns = raw_columns)\n",
    "\n",
    "df_user = df# [['PatientID', 'UserTimeZone', 'UserTimeZoneOffset']]\n",
    "\n",
    "df_user['PatientID'] = 'AIREADI-' + df_user['PatientID'].astype(str)\n",
    "df_user.head()\n",
    "\n",
    "print(df_user.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UserTimeZone'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "measurement_file = [i for i in SourceFile_List if 'measurement' in i][0]\n",
    "# measurement_file\n",
    "\n",
    "df_measurement = pd.read_csv(measurement_file)\n",
    "# df_measurement\n",
    "\n",
    "measurement_type_list = list(df_measurement['measurement_source_value'].value_counts().index)\n",
    "# pprint(measurement_type_list)\n",
    "\n",
    "df_measurement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_file = [i for i in SourceFile_List if 'person' in i][0]\n",
    "# person_file\n",
    "\n",
    "df_person = pd.read_csv(person_file)\n",
    "df_person.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb988",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_file = [i for i in SourceFile_List if 'observation' in i][0]\n",
    "observation_file\n",
    "df_observation = pd.read_csv(observation_file)\n",
    "print(df_observation.shape)\n",
    "df_observation.head()\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f8370",
   "metadata": {},
   "source": [
    "## wearable_blood_glucose - cgm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56277af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_json_list = [i for i in SourceFile_List if 'wearable_blood_glucose' in i and  'json' in i]\n",
    "\n",
    "cgm_json_path = cgm_json_list[0]\n",
    "print(cgm_json_path)\n",
    "\n",
    "\n",
    "import json \n",
    "\n",
    "with open(cgm_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69656954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all CGM files from the cgm_json_list and combine into one DataFrame\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def convert_cgm_json_to_df(json_path):\n",
    "    \"\"\"\n",
    "    Convert CGM JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): Path to the CGM JSON file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing CGM data with all available columns\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract patient ID from header\n",
    "    patient_id = data['header']['patient_id']\n",
    "    if 'AIREADI-' not in patient_id:\n",
    "        patient_id = 'AIREADI-' + patient_id\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    records = []\n",
    "    \n",
    "    # Process CGM readings\n",
    "    for reading in data['body']['cgm']:\n",
    "        # Create a base record with patient ID\n",
    "        record = {'patient_id': patient_id}\n",
    "        \n",
    "        # Extract timestamp from effective_time_frame\n",
    "        if 'effective_time_frame' in reading and 'time_interval' in reading['effective_time_frame']:\n",
    "            time_interval = reading['effective_time_frame']['time_interval']\n",
    "            if 'start_date_time' in time_interval:\n",
    "                record['timestamp'] = time_interval['start_date_time']\n",
    "        \n",
    "        # Extract blood glucose value\n",
    "        if 'blood_glucose' in reading:\n",
    "            record['glucose_value'] = reading['blood_glucose']['value']\n",
    "            record['glucose_unit'] = reading['blood_glucose']['unit']\n",
    "        \n",
    "        # Add all other available fields\n",
    "        for key, value in reading.items():\n",
    "            if key not in ['effective_time_frame', 'blood_glucose']:\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        record[f\"{key}_{sub_key}\"] = sub_value\n",
    "                else:\n",
    "                    record[key] = value\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Convert timestamp to datetime if it exists\n",
    "    if 'timestamp' in df.columns:\n",
    "        # df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "convert_cgm_json_to_df.fn_string = inspect.getsource(convert_cgm_json_to_df)\n",
    "\n",
    "# Test the function with the first CGM JSON file\n",
    "cgm_df = convert_cgm_json_to_df(cgm_json_path)\n",
    "\n",
    "\n",
    "# print(f\"Found {len(cgm_json_list)} CGM JSON files\")\n",
    "\n",
    "# Process all CGM files and combine into one DataFrame\n",
    "all_cgm_data = []\n",
    "\n",
    "for file_path in tqdm(cgm_json_list, desc=\"Processing CGM files\"):\n",
    "    try:\n",
    "        # Extract patient ID from the file path\n",
    "        patient_id = os.path.basename(os.path.dirname(file_path))\n",
    "        \n",
    "        # Convert the JSON file to DataFrame\n",
    "        df = convert_cgm_json_to_df(file_path)\n",
    "        \n",
    "        # Add patient ID if not already in the DataFrame\n",
    "        if 'patient_id' not in df.columns:\n",
    "            df['patient_id'] = patient_id\n",
    "            \n",
    "        all_cgm_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "final_cgm_df = pd.concat(all_cgm_data, ignore_index=True)\n",
    "\n",
    "final_cgm_df.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cgm_df['patient_id'].value_counts().sort_index().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c79d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user['UserTimeZone'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "RawName = 'ElogBGEntry'\n",
    "raw_columns = ['BGEntryID', 'PatientID', 'ObservationDateTime', 'BGValue',\n",
    "                'IsNormalIndicator', 'ObservationEntryDateTime', 'TimezoneOffset',\n",
    "                'Timezone', 'EntryCreatedDateTime', 'ActualBGValue',\n",
    "                'ExternalSourceID', 'UserObservationDateTime']\n",
    "\n",
    "df = final_cgm_df\n",
    "df = df.rename(columns = {'patient_id': 'PatientID', 'glucose_value': 'BGValue', 'timestamp': 'ObservationDateTime'})\n",
    "df = df.reindex(columns = raw_columns)\n",
    "df = pd.merge(df, df_user[['PatientID', 'UserTimeZone']], on = 'PatientID', how = 'left')\n",
    "df['BGEntryID'] = df.index\n",
    "\n",
    "df['ObservationDateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89793649",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df['ObservationDateTime'].iloc[0]\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dbf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone_name = df_user['UserTimeZone'].iloc[0] # .value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timezone_offset_minutes_corrected(dt, timezone_name):\n",
    "    \"\"\"\n",
    "    Get timezone offset in minutes for a specific datetime\n",
    "    \n",
    "    Args:\n",
    "        dt: datetime object (in UTC, either timezone-aware or naive)\n",
    "        timezone_name: string like 'America/Los_Angeles'\n",
    "        \n",
    "    Returns:\n",
    "        offset in minutes (negative for timezones west of UTC)\n",
    "    \"\"\"\n",
    "    import pytz\n",
    "    \n",
    "    # Ensure dt is timezone-aware UTC\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=pytz.UTC)\n",
    "    elif dt.tzinfo != pytz.UTC:\n",
    "        dt = dt.astimezone(pytz.UTC)\n",
    "        \n",
    "    # Get the timezone object\n",
    "    tz = pytz.timezone(timezone_name)\n",
    "    \n",
    "    # Convert to target timezone\n",
    "    localized_dt = dt.astimezone(tz)\n",
    "    \n",
    "    # Get the offset in seconds and convert to minutes\n",
    "    offset_seconds = localized_dt.utcoffset().total_seconds()\n",
    "    offset_minutes = int(offset_seconds / 60)\n",
    "    \n",
    "    return offset_minutes\n",
    "\n",
    "get_timezone_offset_minutes_corrected.fn_string = inspect.getsource(get_timezone_offset_minutes_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_minutes = get_timezone_offset_minutes_corrected(dt, timezone_name)\n",
    "offset_minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TimezoneOffset'] = df.apply(lambda row: get_timezone_offset_minutes_corrected(\n",
    "                                                    row['ObservationDateTime'], \n",
    "                                                    row['UserTimeZone']), \n",
    "                                                    axis = 1)\n",
    "df['ExternalSourceID'] = 18\n",
    "# df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ObservationDateTime'].apply()\n",
    "\n",
    "df['ObservationDateTime'] = df['ObservationDateTime'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['BGValue']\n",
    "\n",
    "print(df.shape)\n",
    "df['BGValue'] = pd.to_numeric(df['BGValue'], errors='coerce')\n",
    "df = df[df['BGValue'].notna()].reset_index(drop = True)\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa1b5f",
   "metadata": {},
   "source": [
    "## wearable activity - heart rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b047497",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_file = [i for i in SourceFile_List if 'heart_rate' in i][0]\n",
    "heart_rate_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa414fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(heart_rate_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_heart_rate_json_to_df(json_data):\n",
    "    \"\"\"\n",
    "    Convert heart rate JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    json_data : dict\n",
    "        JSON data containing heart rate measurements\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing heart rate data with columns:\n",
    "        - patient_id: ID of the patient\n",
    "        - heart_rate: Heart rate value in beats/min\n",
    "        - unit: Unit of measurement\n",
    "        - timestamp: Datetime of the measurement\n",
    "    \"\"\"\n",
    "    heart_rate_records = []\n",
    "    \n",
    "    # Get patient ID from the header\n",
    "    patient_id = json_data['header']['uuid']\n",
    "    \n",
    "    # Process each heart rate measurement\n",
    "    for hr_entry in json_data['body']['heart_rate']:\n",
    "        heart_rate_records.append({\n",
    "            'patient_id': patient_id,\n",
    "            'heart_rate': hr_entry['heart_rate']['value'],\n",
    "            'unit': hr_entry['heart_rate']['unit'],\n",
    "            'timestamp': hr_entry['effective_time_frame']['date_time']\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(heart_rate_records)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the heart rate data to DataFrame\n",
    "df_heart_rate = convert_heart_rate_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Heart rate data for patient {df_heart_rate['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_heart_rate)}\")\n",
    "print(f\"Date range: {df_heart_rate['timestamp'].min()} to {df_heart_rate['timestamp'].max()}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_heart_rate.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_heart_rate_json_to_df.fn_string = inspect.getsource(convert_heart_rate_json_to_df)\n",
    "hr_json_list = [i for i in SourceFile_List if 'heart_rate' in i]# [:10]\n",
    "# Test the function with the first CGM JSON file\n",
    "hr_df = convert_heart_rate_json_to_df(data)\n",
    "\n",
    "# Process all CGM files and combine into one DataFrame\n",
    "all_hr_data = []\n",
    "\n",
    "print('The number of heart rate files: ', len(hr_json_list))\n",
    "\n",
    "for file_path in tqdm(hr_json_list, desc=\"Processing HR files\"):\n",
    "    try:\n",
    "        # Extract patient ID from the file path\n",
    "        patient_id = os.path.basename(os.path.dirname(file_path))\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        \n",
    "        # Convert the JSON file to DataFrame\n",
    "        df = convert_heart_rate_json_to_df(data)\n",
    "        \n",
    "        # Add patient ID if not already in the DataFrame\n",
    "        if 'patient_id' not in df.columns:\n",
    "            df['patient_id'] = patient_id\n",
    "            \n",
    "        all_hr_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "final_hr_df = pd.concat(all_hr_data, ignore_index=True)\n",
    "\n",
    "final_hr_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Alina: Rename Columns\n",
    "df = final_hr_df\n",
    "\n",
    "RawName = 'HeartRate'\n",
    "#originalname: 'wearable_blood_glucose'\n",
    "raw_columns =['HREntryID', 'PatientID', 'ObservationDateTime', 'HRValue','HRUnit',\n",
    "              'ObservationEntryDateTime', 'TimezoneOffset',\n",
    "              'Timezone', 'EntryCreatedDateTime'\n",
    "              ]\n",
    "df = df.rename(columns = {'patient_id': 'PatientID', \n",
    "                          'timestamp': 'ObservationDateTime',\n",
    "                          'heart_rate': 'HRValue',\n",
    "                          'unit':'HRUnit',\n",
    "                          })\n",
    "#TODO: What is transmitter_time_unit, source_device_id VS transmitter_id\n",
    "df = df.reindex(columns = raw_columns)\n",
    "df['HREntryID'] = df.index\n",
    "df = pd.merge(df, df_user[['PatientID', 'UserTimeZone']], on = 'PatientID', how = 'left') \n",
    "df['TimezoneOffset'] = df.apply(lambda row: get_timezone_offset_minutes_corrected(\n",
    "                                                    row['ObservationDateTime'], \n",
    "                                                    row['UserTimeZone']), \n",
    "                                                    axis = 1)\n",
    "df['ObservationDateTime'] = df['ObservationDateTime'].dt.tz_localize(None)\n",
    "df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PatientID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert TimezoneOffset to numeric to ensure proper calculation\n",
    "df['TimezoneOffset'] = pd.to_numeric(df['TimezoneOffset'])\n",
    "# Create local datetime by adding the timezone offset\n",
    "df['DT_local'] = df['ObservationDateTime'] + pd.to_timedelta(df['TimezoneOffset'], unit='minutes')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf045dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PatientID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ebcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_value = 'AIREADI-4193'\n",
    "\n",
    "dfx = df[df['PatientID'] == pid_value]\n",
    "\n",
    "# Plot heart rate values over time for patient AIREADI-1080\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(dfx['DT_local'], dfx['HRValue'], '-o', markersize=2)\n",
    "plt.title(f'Heart Rate Over Time for Patient {pid_value}')\n",
    "plt.xlabel('Date Time (Local)')\n",
    "plt.ylabel('Heart Rate (beats/min)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Format x-axis to show dates nicely\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ac95f",
   "metadata": {},
   "source": [
    "## wearable activity - oxygen saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "oxygen_saturation_file_list = [i for i in SourceFile_List if 'oxygen_saturation' in i and 'json' in i]\n",
    "oxygen_saturation_file = oxygen_saturation_file_list[0]\n",
    "oxygen_saturation_file\n",
    "\n",
    "with open(oxygen_saturation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_oxygen_saturation_json_to_df(data):\n",
    "    \"\"\"\n",
    "    Convert oxygen saturation JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON data containing oxygen saturation information\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with oxygen saturation data\n",
    "    \"\"\"\n",
    "    oxygen_records = []\n",
    "    \n",
    "    # Extract patient ID from the header\n",
    "    patient_id = data['header']['uuid']\n",
    "    \n",
    "    # Process each oxygen saturation entry\n",
    "    for entry in data['body']['breathing']:\n",
    "        record = {\n",
    "            'patient_id': patient_id,\n",
    "            'oxygen_saturation': entry['oxygen_saturation']['value'],\n",
    "            'unit': entry['oxygen_saturation']['unit'],\n",
    "            'timestamp': entry['effective_time_frame']['date_time'],\n",
    "            'measurement_method': entry.get('measurement_method', '')\n",
    "        }\n",
    "        oxygen_records.append(record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(oxygen_records)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the oxygen saturation data to DataFrame\n",
    "df_oxygen = convert_oxygen_saturation_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Oxygen saturation data for patient {df_oxygen['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_oxygen)}\")\n",
    "print(f\"Date range: {df_oxygen['timestamp'].min()} to {df_oxygen['timestamp'].max()}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_oxygen.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36143b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all oxygen saturation files\n",
    "oxygen_files = [i for i in SourceFile_List if 'oxygen_saturation' in i and 'json' in i]\n",
    "print(f\"Found {len(oxygen_files)} oxygen saturation files\")\n",
    "\n",
    "# Initialize an empty list to store all oxygen saturation data\n",
    "all_oxygen_data = []\n",
    "\n",
    "# Process each oxygen saturation file\n",
    "for file_path in tqdm(oxygen_files, desc=\"Processing oxygen saturation files\"):\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = convert_oxygen_saturation_json_to_df(data)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_oxygen_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_oxygen_data:\n",
    "    df_oxygen_all = pd.concat(all_oxygen_data, ignore_index=True)\n",
    "    \n",
    "    # Convert timestamp to datetime if not already\n",
    "    df_oxygen_all['timestamp'] = pd.to_datetime(df_oxygen_all['timestamp'])\n",
    "    \n",
    "    # Sort by patient_id and timestamp\n",
    "    df_oxygen_all = df_oxygen_all.sort_values(['patient_id', 'timestamp'])\n",
    "    \n",
    "    # Display information about the combined DataFrame\n",
    "    print(df_oxygen_all.shape)\n",
    "    \n",
    "    # Show the distribution of records by patient\n",
    "    print(df_oxygen_all['patient_id'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"No oxygen saturation data found\")\n",
    "\n",
    "\n",
    "# Create a new DataFrame with standardized column names for oxygen saturation data\n",
    "df_oxygen_all_processed = df_oxygen_all.copy()\n",
    "\n",
    "# Rename columns to match standardized format\n",
    "df_oxygen_all_processed = df_oxygen_all_processed.rename(columns={\n",
    "    'patient_id': 'PatientID',\n",
    "    'oxygen_saturation': 'OxygenValue',\n",
    "    'unit': 'OxygenUnit',\n",
    "    'timestamp': 'ObservationDateTime',\n",
    "    'measurement_method': 'MeasurementMethod'\n",
    "})\n",
    "\n",
    "# Add additional columns\n",
    "df_oxygen_all_processed['OxygenEntryID'] = range(len(df_oxygen_all_processed))\n",
    "df_oxygen_all_processed['ObservationEntryDateTime'] = None\n",
    "df_oxygen_all_processed['TimezoneOffset'] = None\n",
    "df_oxygen_all_processed['Timezone'] = None\n",
    "df_oxygen_all_processed['EntryCreatedDateTime'] = df_oxygen_all_processed['ObservationDateTime']\n",
    "\n",
    "# Map patient IDs to timezones using the existing function\n",
    "# Assuming we have the same timezone distribution as seen in previous cells\n",
    "# Merge with the user timezone dataframe to get the correct timezone for each patient\n",
    "df_oxygen_all_processed = pd.merge(df_oxygen_all_processed, df_user[['PatientID', 'UserTimeZone']], \n",
    "                                  on='PatientID', how='left')\n",
    "\n",
    "# Apply the timezone offset function\n",
    "df_oxygen_all_processed['TimezoneOffset'] = df_oxygen_all_processed.apply(\n",
    "    lambda row: get_timezone_offset_minutes_corrected(row['ObservationDateTime'], row['UserTimeZone']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_oxygen_all_processed['ObservationDateTime'] = df_oxygen_all_processed['ObservationDateTime'].dt.tz_localize(None)\n",
    "df_oxygen_all_processed['EntryCreatedDateTime'] = pd.to_datetime(df_oxygen_all_processed['ObservationDateTime'])\n",
    "\n",
    "# Display the processed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4506b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oxygen_all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37430a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_oxygen_all_processed\n",
    "\n",
    "# df[df['PatientID'] == 'AIREADI-1080']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert TimezoneOffset to numeric to ensure proper calculation\n",
    "# df['TimezoneOffset'] = pd.to_numeric(df['TimezoneOffset'])\n",
    "# # Create local datetime by adding the timezone offset\n",
    "# df['DT_local'] = df['ObservationDateTime'] + pd.to_timedelta(df['TimezoneOffset'], unit='minutes')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfx = df[df['PatientID'] == 'AIREADI-1080']\n",
    "# dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55515854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot oxygen saturation values over time for patient AIREADI-1080\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(dfx['DT_local'], dfx['OxygenValue'], 'o-', color='blue', alpha=0.7)\n",
    "plt.title('Oxygen Saturation Values for Patient AIREADI-1080')\n",
    "plt.xlabel('Local Date/Time')\n",
    "plt.ylabel('Oxygen Saturation (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.ylim(85, 105)  # Typical range for oxygen saturation\n",
    "\n",
    "# Format the x-axis to show dates nicely\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
    "plt.gcf().autofmt_xdate()  # Rotate date labels\n",
    "\n",
    "# Add a horizontal line at 95% (normal oxygen level threshold)\n",
    "plt.axhline(y=95, color='r', linestyle='--', alpha=0.5, label='Normal threshold (95%)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9297d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oxygen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a54407",
   "metadata": {},
   "source": [
    "## wearable activity - physical activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ae1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_file_list = [i for i in SourceFile_List if 'activity.json' in i and 'json' in i]\n",
    "activity_file = activity_file_list[0]\n",
    "activity_file\n",
    "\n",
    "\n",
    "with open(activity_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_activity_json_to_df(data):\n",
    "    \"\"\"\n",
    "    Convert activity JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON data containing activity information\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with activity data\n",
    "    \"\"\"\n",
    "    activity_records = []\n",
    "    \n",
    "    # Extract patient ID from the header\n",
    "    patient_id = data['header']['uuid']\n",
    "    \n",
    "    # Process each activity entry\n",
    "    for activity in data['body']['activity']:\n",
    "        record = {\n",
    "            'patient_id': patient_id,\n",
    "            'activity_name': activity['activity_name'],\n",
    "            'steps': activity['base_movement_quantity']['value'],\n",
    "            'unit': activity['base_movement_quantity']['unit'],\n",
    "            'start_time': activity['effective_time_frame']['time_interval']['start_date_time'],\n",
    "            'end_time': activity['effective_time_frame']['time_interval']['end_date_time']\n",
    "        }\n",
    "        activity_records.append(record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(activity_records)\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    \n",
    "    # Calculate duration in minutes\n",
    "    df['duration_minutes'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # Sort by start time\n",
    "    df = df.sort_values('start_time')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the activity data to DataFrame\n",
    "df_activity = convert_activity_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Activity data for patient {df_activity['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_activity)}\")\n",
    "print(f\"Date range: {df_activity['start_time'].min()} to {df_activity['end_time'].max()}\")\n",
    "print(f\"Activity types: {df_activity['activity_name'].unique()}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_activity.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76bcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each activity type\n",
    "activity_counts = df_activity['activity_name'].value_counts()\n",
    "print(activity_counts)\n",
    "\n",
    "# Find all activity files\n",
    "# activity_files = [i for i in SourceFile_List if 'activity' in i and 'json' in i]\n",
    "activity_files = [i for i in SourceFile_List if 'activity.json' in i and 'json' in i]\n",
    "\n",
    "print(f\"Found {len(activity_files)} activity files\")\n",
    "\n",
    "# Initialize an empty list to store all activity data\n",
    "all_activity_data = []\n",
    "\n",
    "# Process each activity file\n",
    "for file_path in tqdm(activity_files, desc=\"Processing activity files\"):\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = convert_activity_json_to_df(data)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_activity_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_activity_data:\n",
    "    df_activity_all = pd.concat(all_activity_data, ignore_index=True)\n",
    "    \n",
    "    # Convert timestamps to datetime if not already\n",
    "    df_activity_all['start_time'] = pd.to_datetime(df_activity_all['start_time'])\n",
    "    df_activity_all['end_time'] = pd.to_datetime(df_activity_all['end_time'])\n",
    "    \n",
    "    # Sort by patient_id and start_time\n",
    "    df_activity_all = df_activity_all.sort_values(['patient_id', 'start_time'])\n",
    "    \n",
    "    # Display information about the combined DataFrame\n",
    "    print(df_activity_all.shape)\n",
    "    \n",
    "    # Show the distribution of records by patient\n",
    "    print(df_activity_all['patient_id'].value_counts().head(10))\n",
    "    \n",
    "    # Show the distribution of activity types\n",
    "    print(df_activity_all['activity_name'].value_counts())\n",
    "else:\n",
    "    print(\"No activity data found\")\n",
    "\n",
    "# Create a new DataFrame with standardized column names for activity data\n",
    "df_activity_all_processed = df_activity_all.copy()\n",
    "\n",
    "# Rename columns to match standardized format\n",
    "df_activity_all_processed = df_activity_all_processed.rename(columns={\n",
    "    'patient_id': 'PatientID',\n",
    "    'activity_name': 'ActivityName',\n",
    "    'steps': 'StepCount',\n",
    "    'unit': 'StepUnit',\n",
    "    'start_time': 'StartDateTime',\n",
    "    'end_time': 'EndDateTime',\n",
    "    'duration_minutes': 'DurationMinutes'\n",
    "})\n",
    "\n",
    "# Add additional columns\n",
    "df_activity_all_processed['ActivityEntryID'] = range(len(df_activity_all_processed))\n",
    "df_activity_all_processed['EntryCreatedDateTime'] = df_activity_all_processed['StartDateTime']\n",
    "\n",
    "# Merge with the user timezone dataframe to get the correct timezone for each patient\n",
    "df_activity_all_processed = pd.merge(df_activity_all_processed, df_user[['PatientID', 'UserTimeZone']], \n",
    "                                    on='PatientID', how='left')\n",
    "\n",
    "# Apply the timezone offset function\n",
    "df_activity_all_processed['TimezoneOffset'] = df_activity_all_processed.apply(\n",
    "    lambda row: get_timezone_offset_minutes_corrected(row['StartDateTime'], row['UserTimeZone']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert datetime columns to timezone-naive\n",
    "df_activity_all_processed['StartDateTime'] = df_activity_all_processed['StartDateTime'].dt.tz_localize(None)\n",
    "df_activity_all_processed['EndDateTime'] = df_activity_all_processed['EndDateTime'].dt.tz_localize(None)\n",
    "df_activity_all_processed['EntryCreatedDateTime'] = pd.to_datetime(df_activity_all_processed['StartDateTime'])\n",
    "\n",
    "# Display the processed DataFrame\n",
    "df_activity_all_processed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_activity[df_activity['activity_name'] == 'sedentary']\n",
    "\n",
    "\n",
    "\n",
    "# Convert TimezoneOffset to numeric to ensure proper calculation\n",
    "\n",
    "df = df_activity_all_processed\n",
    "df['ObservationDateTime'] = pd.to_datetime(df['StartDateTime'])\n",
    "df['TimezoneOffset'] = pd.to_numeric(df['TimezoneOffset'])\n",
    "# Create local datetime by adding the timezone offset\n",
    "df['DT_local_start'] = df['ObservationDateTime'] + pd.to_timedelta(df['TimezoneOffset'], unit='minutes')\n",
    "df['DT_local_end'] = df['EndDateTime'] + pd.to_timedelta(df['TimezoneOffset'], unit='minutes')\n",
    "\n",
    "dfx = df[df['PatientID'] == 'AIREADI-1080']\n",
    "dfx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c71d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activity data for patient AIREADI-1080 (random full day)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set up the figure with a larger size for better visibility\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get all available dates for this patient\n",
    "available_dates = dfx['DT_local_start'].dt.date.unique()\n",
    "\n",
    "# Select a random full day\n",
    "if len(available_dates) > 0:\n",
    "    random_day = random.choice(available_dates)\n",
    "    dfx_random_day = dfx[dfx['DT_local_start'].dt.date == random_day]\n",
    "    \n",
    "    # Define a color map for different activity types\n",
    "    activity_types = dfx_random_day['ActivityName'].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(activity_types)))\n",
    "    color_map = dict(zip(activity_types, colors))\n",
    "    \n",
    "    # Create a legend handles list\n",
    "    legend_handles = []\n",
    "    \n",
    "    # Plot each activity as a horizontal line from start to end time\n",
    "    for idx, row in dfx_random_day.iterrows():\n",
    "        activity = row['ActivityName']\n",
    "        start = row['DT_local_start']\n",
    "        end = row['DT_local_end']\n",
    "        \n",
    "        # Skip entries with identical start and end times\n",
    "        if start == end:\n",
    "            continue\n",
    "        \n",
    "        # Get color for this activity type\n",
    "        color = color_map[activity]\n",
    "        \n",
    "        # Plot the activity as a horizontal line\n",
    "        line = plt.hlines(y=activity, xmin=start, xmax=end, \n",
    "                          colors=color, linewidth=4, alpha=0.7)\n",
    "        \n",
    "        # Add to legend handles if not already added\n",
    "        if activity not in [h.get_label() for h in legend_handles]:\n",
    "            legend_handles.append(plt.Line2D([0], [0], color=color, lw=4, label=activity))\n",
    "    \n",
    "    # Format the x-axis to show hours nicely for a single day\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Activity Type', fontsize=12)\n",
    "    plt.title(f'Activity Timeline for Patient AIREADI-1080 on {random_day}', fontsize=14)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(handles=legend_handles, loc='upper right')\n",
    "    \n",
    "    # Rotate time labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5a92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b89f3f30",
   "metadata": {},
   "source": [
    "## wearable activity - physicial activity calorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6957256",
   "metadata": {},
   "outputs": [],
   "source": [
    "calorie_file_list = [i for i in SourceFile_List if 'calorie.json' in i and 'json' in i]# [0]\n",
    "calorie_file_list\n",
    "\n",
    "calorie_file = calorie_file_list[0]\n",
    "calorie_file\n",
    "\n",
    "\n",
    "\n",
    "with open(calorie_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_calorie_json_to_df(data):\n",
    "    \"\"\"\n",
    "    Convert calorie JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        The JSON data containing calorie information.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with calorie data.\n",
    "    \"\"\"\n",
    "    # Extract patient ID from the header\n",
    "    patient_id = data['header']['user_id']\n",
    "    \n",
    "    # Initialize lists to store the data\n",
    "    records = []\n",
    "    \n",
    "    # Extract calorie data from the body\n",
    "    for activity in data['body']['activity']:\n",
    "        if activity['activity_name'] == 'kcal_burned':\n",
    "            record = {\n",
    "                'patient_id': patient_id,\n",
    "                'activity_name': activity['activity_name'],\n",
    "                'calories': activity['calories_value']['value'],\n",
    "                'unit': activity['calories_value']['unit'],\n",
    "                'timestamp': activity['effective_time_frame']['date_time']\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the calorie data to DataFrame\n",
    "df_calorie = convert_calorie_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Calorie data for patient {df_calorie['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_calorie)}\")\n",
    "print(f\"Date range: {df_calorie['timestamp'].min()} to {df_calorie['timestamp'].max()}\")\n",
    "print(f\"Calorie range: {df_calorie['calories'].min()} to {df_calorie['calories'].max()} {df_calorie['unit'].iloc[0]}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_calorie.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e64697",
   "metadata": {},
   "source": [
    "## wearable activity - respiratory rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "respiratory_rate_file_list = [i for i in SourceFile_List if 'respiratory_rate' in i and 'json' in i]\n",
    "respiratory_rate_file = respiratory_rate_file_list[0]\n",
    "respiratory_rate_file\n",
    "\n",
    "\n",
    "with open(respiratory_rate_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60623969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_respiratory_rate_json_to_df(data):\n",
    "    \"\"\"\n",
    "    Convert respiratory rate data from JSON format to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): JSON data containing respiratory rate information\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with respiratory rate data\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Extract patient ID from the header\n",
    "    patient_id = data['header']['user_id']\n",
    "    \n",
    "    # Process each respiratory rate record\n",
    "    if 'breathing' in data['body']:\n",
    "        for breathing_record in data['body']['breathing']:\n",
    "            record = {\n",
    "                'patient_id': patient_id,\n",
    "                'respiratory_rate': breathing_record['respiratory_rate']['value'],\n",
    "                'unit': breathing_record['respiratory_rate']['unit'],\n",
    "                'timestamp': breathing_record['effective_time_frame']['date_time']\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the respiratory rate data to DataFrame\n",
    "df_respiratory_rate = convert_respiratory_rate_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Respiratory rate data for patient {df_respiratory_rate['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_respiratory_rate)}\")\n",
    "print(f\"Date range: {df_respiratory_rate['timestamp'].min()} to {df_respiratory_rate['timestamp'].max()}\")\n",
    "print(f\"Respiratory rate range: {df_respiratory_rate['respiratory_rate'].min()} to {df_respiratory_rate['respiratory_rate'].max()} {df_respiratory_rate['unit'].iloc[0]}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_respiratory_rate.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b583492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_respiratory_rate['respiratory_rate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all respiratory rate files\n",
    "respiratory_rate_file_list = [i for i in SourceFile_List if 'respiratory_rate' in i and 'json' in i]\n",
    "respiratory_rate_files = respiratory_rate_file_list # [i for i in SourceFile_List if 'respiratory-rate.json' in i and 'json' in i]\n",
    "\n",
    "print(f\"Found {len(respiratory_rate_files)} respiratory rate files\")\n",
    "\n",
    "# Initialize an empty list to store all respiratory rate data\n",
    "all_respiratory_rate_data = []\n",
    "\n",
    "# Process each respiratory rate file\n",
    "for file_path in tqdm(respiratory_rate_files, desc=\"Processing respiratory rate files\"):\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = convert_respiratory_rate_json_to_df(data)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_respiratory_rate_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_respiratory_rate_data:\n",
    "    df_respiratory_rate_all = pd.concat(all_respiratory_rate_data, ignore_index=True)\n",
    "    \n",
    "    # Convert timestamps to datetime if not already\n",
    "    df_respiratory_rate_all['timestamp'] = pd.to_datetime(df_respiratory_rate_all['timestamp'])\n",
    "    \n",
    "    # Sort by patient_id and timestamp\n",
    "    df_respiratory_rate_all = df_respiratory_rate_all.sort_values(['patient_id', 'timestamp'])\n",
    "    \n",
    "    # Display information about the combined DataFrame\n",
    "    print(df_respiratory_rate_all.shape)\n",
    "    \n",
    "    # Show the distribution of records by patient\n",
    "    print(df_respiratory_rate_all['patient_id'].value_counts().head(10))\n",
    "    \n",
    "    # Show the distribution of respiratory rates\n",
    "    print(df_respiratory_rate_all['respiratory_rate'].value_counts().head(20))\n",
    "else:\n",
    "    print(\"No respiratory rate data found\")\n",
    "\n",
    "# Create a new DataFrame with standardized column names for respiratory rate data\n",
    "df_respiratory_rate_all_processed = df_respiratory_rate_all.copy()\n",
    "\n",
    "# Rename columns to match standardized format\n",
    "df_respiratory_rate_all_processed = df_respiratory_rate_all_processed.rename(columns={\n",
    "    'patient_id': 'PatientID',\n",
    "    'respiratory_rate': 'RespiratoryRate',\n",
    "    'unit': 'RespiratoryUnit',\n",
    "    'timestamp': 'ObservationDateTime'\n",
    "})\n",
    "\n",
    "# Add additional columns\n",
    "df_respiratory_rate_all_processed['EntryCreatedDateTime'] = df_respiratory_rate_all_processed['ObservationDateTime']\n",
    "\n",
    "# Merge with the user timezone dataframe to get the correct timezone for each patient\n",
    "df_respiratory_rate_all_processed = pd.merge(df_respiratory_rate_all_processed, df_user[['PatientID', 'UserTimeZone']], \n",
    "                                    on='PatientID', how='left')\n",
    "\n",
    "# Apply the timezone offset function\n",
    "df_respiratory_rate_all_processed['TimezoneOffset'] = df_respiratory_rate_all_processed.apply(\n",
    "    lambda row: get_timezone_offset_minutes_corrected(row['ObservationDateTime'], row['UserTimeZone']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert datetime columns to timezone-naive\n",
    "df_respiratory_rate_all_processed['ObservationDateTime'] = df_respiratory_rate_all_processed['ObservationDateTime'].dt.tz_localize(None)\n",
    "df_respiratory_rate_all_processed['EntryCreatedDateTime'] = pd.to_datetime(df_respiratory_rate_all_processed['ObservationDateTime'])\n",
    "\n",
    "# Add local datetime column\n",
    "df_respiratory_rate_all_processed['DT_local'] = df_respiratory_rate_all_processed.apply(\n",
    "    lambda row: row['ObservationDateTime'] - pd.Timedelta(minutes=row['TimezoneOffset']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the processed DataFrame\n",
    "df_respiratory_rate_all_processed.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TimezoneOffset to numeric to ensure proper calculation\n",
    "df = df_respiratory_rate_all_processed\n",
    "df['ObservationDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "df['TimezoneOffset'] = pd.to_numeric(df['TimezoneOffset'])\n",
    "\n",
    "# Create local datetime by adding the timezone offset\n",
    "df['DT_local'] = df['ObservationDateTime'] + pd.to_timedelta(df['TimezoneOffset'], unit='minutes')\n",
    "\n",
    "# Filter for a specific patient to examine their data\n",
    "dfx = df[df['PatientID'] == 'AIREADI-1080']\n",
    "dfx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to visualize respiratory rate data for AIREADI-1080\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Filter out invalid respiratory rate values (negative values)\n",
    "valid_data = dfx[dfx['RespiratoryRate'] > 0]\n",
    "\n",
    "# Plot the respiratory rate over time\n",
    "plt.plot(valid_data['DT_local'], valid_data['RespiratoryRate'], 'o-', markersize=4, alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date and Time (Local)', fontsize=12)\n",
    "plt.ylabel('Respiratory Rate (breaths/min)', fontsize=12)\n",
    "plt.title(f'Respiratory Rate Over Time for Patient {valid_data[\"PatientID\"].iloc[0]}', fontsize=14)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis to show dates clearly\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Add a horizontal line at the normal respiratory rate range (12-20 breaths/min)\n",
    "plt.axhline(y=12, color='g', linestyle='--', alpha=0.5, label='Lower normal range (12)')\n",
    "plt.axhline(y=20, color='r', linestyle='--', alpha=0.5, label='Upper normal range (20)')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01baf7d",
   "metadata": {},
   "source": [
    "## wearable activity - stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f71b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_file_list = [i for i in SourceFile_List if 'stress' in i and 'json' in i]\n",
    "stress_file = stress_file_list[0]\n",
    "stress_file\n",
    "\n",
    "\n",
    "with open(stress_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stress_json_to_df(data):\n",
    "    \"\"\"\n",
    "    Convert stress data from JSON format to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): JSON data containing stress information\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with stress data\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Extract patient ID from the header\n",
    "    patient_id = data['header']['user_id']\n",
    "    \n",
    "    # Process each stress record\n",
    "    if 'stress' in data['body']:\n",
    "        for stress_record in data['body']['stress']:\n",
    "            record = {\n",
    "                'patient_id': patient_id,\n",
    "                'stress_level': stress_record['stress']['value'],\n",
    "                'unit': stress_record['stress']['unit'],\n",
    "                'timestamp': stress_record['effective_time_frame']['date_time']\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the stress data to DataFrame\n",
    "df_stress = convert_stress_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Stress data for patient {df_stress['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_stress)}\")\n",
    "print(f\"Date range: {df_stress['timestamp'].min()} to {df_stress['timestamp'].max()}\")\n",
    "print(f\"Stress level range: {df_stress['stress_level'].min()} to {df_stress['stress_level'].max()} {df_stress['unit'].iloc[0]}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_stress.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2005f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all stress files\n",
    "stress_file_list = [i for i in SourceFile_List if 'stress' in i and 'json' in i]\n",
    "stress_files = stress_file_list\n",
    "\n",
    "print(f\"Found {len(stress_files)} stress files\")\n",
    "\n",
    "# Initialize an empty list to store all stress data\n",
    "all_stress_data = []\n",
    "\n",
    "# Process each stress file\n",
    "for file_path in tqdm(stress_files, desc=\"Processing stress files\"):\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = convert_stress_json_to_df(data)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_stress_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_stress_data:\n",
    "    df_stress_all = pd.concat(all_stress_data, ignore_index=True)\n",
    "    \n",
    "    # Convert timestamps to datetime if not already\n",
    "    df_stress_all['timestamp'] = pd.to_datetime(df_stress_all['timestamp'])\n",
    "    \n",
    "    # Sort by patient_id and timestamp\n",
    "    df_stress_all = df_stress_all.sort_values(['patient_id', 'timestamp'])\n",
    "    \n",
    "    # Display information about the combined DataFrame\n",
    "    print(df_stress_all.shape)\n",
    "    \n",
    "    # Show the distribution of records by patient\n",
    "    print(df_stress_all['patient_id'].value_counts().head(10))\n",
    "    \n",
    "    # Show the distribution of stress levels\n",
    "    print(df_stress_all['stress_level'].value_counts().head(20))\n",
    "else:\n",
    "    print(\"No stress data found\")\n",
    "\n",
    "# Create a new DataFrame with standardized column names for stress data\n",
    "df_stress_all_processed = df_stress_all.copy()\n",
    "\n",
    "# Rename columns to match standardized format\n",
    "df_stress_all_processed = df_stress_all_processed.rename(columns={\n",
    "    'patient_id': 'PatientID',\n",
    "    'stress_level': 'StressLevel',\n",
    "    'unit': 'StressUnit',\n",
    "    'timestamp': 'ObservationDateTime'\n",
    "})\n",
    "\n",
    "# Add additional columns\n",
    "df_stress_all_processed['EntryCreatedDateTime'] = df_stress_all_processed['ObservationDateTime']\n",
    "\n",
    "# Merge with the user timezone dataframe to get the correct timezone for each patient\n",
    "df_stress_all_processed = pd.merge(df_stress_all_processed, df_user[['PatientID', 'UserTimeZone']], \n",
    "                                    on='PatientID', how='left')\n",
    "\n",
    "# Apply the timezone offset function\n",
    "df_stress_all_processed['TimezoneOffset'] = df_stress_all_processed.apply(\n",
    "    lambda row: get_timezone_offset_minutes_corrected(row['ObservationDateTime'], row['UserTimeZone']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert datetime columns to timezone-naive\n",
    "df_stress_all_processed['ObservationDateTime'] = df_stress_all_processed['ObservationDateTime'].dt.tz_localize(None)\n",
    "df_stress_all_processed['EntryCreatedDateTime'] = pd.to_datetime(df_stress_all_processed['ObservationDateTime'])\n",
    "\n",
    "# Add local datetime column\n",
    "df_stress_all_processed['DT_local'] = df_stress_all_processed.apply(\n",
    "    lambda row: row['ObservationDateTime'] - pd.Timedelta(minutes=row['TimezoneOffset']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the processed DataFrame\n",
    "df_stress_all_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938c7b3",
   "metadata": {},
   "source": [
    "## wearable activity - sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_file_list = [i for i in SourceFile_List if 'sleep' in i and 'json' in i]\n",
    "sleep_file = sleep_file_list[0]\n",
    "sleep_file\n",
    "\n",
    "\n",
    "with open(sleep_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sleep_json_to_df(data):\n",
    "    \"\"\"\n",
    "    Convert sleep JSON data to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The JSON data containing sleep information\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with sleep stage information\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Extract patient ID from the header\n",
    "    patient_id = data['header']['user_id']\n",
    "    \n",
    "    # Process each sleep record\n",
    "    for sleep_record in data['body']['sleep']:\n",
    "        record = {\n",
    "            'patient_id': patient_id,\n",
    "            'sleep_stage': sleep_record['sleep_stage_state'],\n",
    "            'start_time': sleep_record['sleep_stage_time_frame']['time_interval']['start_date_time'],\n",
    "            'end_time': sleep_record['sleep_stage_time_frame']['time_interval']['end_date_time']\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    \n",
    "    # Calculate duration in minutes\n",
    "    df['duration_minutes'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 60\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert the sleep data to DataFrame\n",
    "df_sleep = convert_sleep_json_to_df(data)\n",
    "\n",
    "# Display information about the DataFrame\n",
    "print(f\"Sleep data for patient {df_sleep['patient_id'].iloc[0]}\")\n",
    "print(f\"Number of records: {len(df_sleep)}\")\n",
    "print(f\"Date range: {df_sleep['start_time'].min()} to {df_sleep['end_time'].max()}\")\n",
    "print(f\"Sleep stages: {df_sleep['sleep_stage'].unique()}\")\n",
    "print(f\"Total sleep duration: {df_sleep['duration_minutes'].sum():.2f} minutes\")\n",
    "print(\"\\nSample data:\")\n",
    "df_sleep.head()\n",
    "\n",
    "# df_sleep['sleep_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2111455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sleep files\n",
    "sleep_file_list = [i for i in SourceFile_List if 'sleep' in i and 'json' in i]\n",
    "sleep_files = sleep_file_list\n",
    "\n",
    "print(f\"Found {len(sleep_files)} sleep files\")\n",
    "\n",
    "# Initialize an empty list to store all sleep data\n",
    "all_sleep_data = []\n",
    "\n",
    "# Process each sleep file\n",
    "for file_path in tqdm(sleep_files, desc=\"Processing sleep files\"):\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = convert_sleep_json_to_df(data)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_sleep_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_sleep_data:\n",
    "    df_sleep_all = pd.concat(all_sleep_data, ignore_index=True)\n",
    "    \n",
    "    # Convert timestamps to datetime if not already\n",
    "    df_sleep_all['start_time'] = pd.to_datetime(df_sleep_all['start_time'])\n",
    "    df_sleep_all['end_time'] = pd.to_datetime(df_sleep_all['end_time'])\n",
    "    \n",
    "    # Sort by patient_id and timestamp\n",
    "    df_sleep_all = df_sleep_all.sort_values(['patient_id', 'start_time'])\n",
    "    \n",
    "    # Display information about the combined DataFrame\n",
    "    print(df_sleep_all.shape)\n",
    "    \n",
    "    # Show the distribution of records by patient\n",
    "    print(df_sleep_all['patient_id'].value_counts().head(10))\n",
    "    \n",
    "    # Show the distribution of sleep stages\n",
    "    print(df_sleep_all['sleep_stage'].value_counts().head(20))\n",
    "else:\n",
    "    print(\"No sleep data found\")\n",
    "\n",
    "# Create a new DataFrame with standardized column names for sleep data\n",
    "df_sleep_all_processed = df_sleep_all.copy()\n",
    "\n",
    "# Rename columns to match standardized format\n",
    "df_sleep_all_processed = df_sleep_all_processed.rename(columns={\n",
    "    'patient_id': 'PatientID',\n",
    "    'sleep_stage': 'SleepStage',\n",
    "    'start_time': 'StartDateTime',\n",
    "    'end_time': 'EndDateTime',\n",
    "    'duration_minutes': 'DurationMinutes'\n",
    "})\n",
    "\n",
    "# Add additional columns\n",
    "df_sleep_all_processed['EntryCreatedDateTime'] = df_sleep_all_processed['StartDateTime']\n",
    "\n",
    "# Merge with the user timezone dataframe to get the correct timezone for each patient\n",
    "df_sleep_all_processed = pd.merge(df_sleep_all_processed, df_user[['PatientID', 'UserTimeZone']], \n",
    "                                  on='PatientID', how='left')\n",
    "\n",
    "# Apply the timezone offset function\n",
    "df_sleep_all_processed['TimezoneOffset'] = df_sleep_all_processed.apply(\n",
    "    lambda row: get_timezone_offset_minutes_corrected(row['StartDateTime'], row['UserTimeZone']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert datetime columns to timezone-naive\n",
    "df_sleep_all_processed['StartDateTime'] = df_sleep_all_processed['StartDateTime'].dt.tz_localize(None)\n",
    "df_sleep_all_processed['EndDateTime'] = df_sleep_all_processed['EndDateTime'].dt.tz_localize(None)\n",
    "df_sleep_all_processed['EntryCreatedDateTime'] = pd.to_datetime(df_sleep_all_processed['StartDateTime'])\n",
    "\n",
    "# Add local datetime column\n",
    "df_sleep_all_processed['DT_local_start'] = df_sleep_all_processed.apply(\n",
    "    lambda row: row['StartDateTime'] - pd.Timedelta(minutes=row['TimezoneOffset']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_sleep_all_processed['DT_local_end'] = df_sleep_all_processed.apply(\n",
    "    lambda row: row['EndDateTime'] - pd.Timedelta(minutes=row['TimezoneOffset']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the processed DataFrame\n",
    "df_sleep_all_processed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd47072",
   "metadata": {},
   "source": [
    "## cardiac ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "ecg_file_list = [i for i in SourceFile_List if 'ecg' in i and 'dat' in i]\n",
    "ecg_file = ecg_file_list[0]\n",
    "ecg_file\n",
    "\n",
    "\n",
    "ecg_file_head = ecg_file.replace('.dat', '.hea')\n",
    "ecg_file_head\n",
    "\n",
    "\n",
    "\n",
    "with open(ecg_file_head, 'r') as f:\n",
    "    ecg_file_head_lines = f.readlines()\n",
    "\n",
    "date = [i for i in ecg_file_head_lines if 'validation_date' in i]\n",
    "date\n",
    "\n",
    "ecg_file_head_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meaningful information from ECG header file\n",
    "def extract_ecg_metadata(header_lines):\n",
    "    \"\"\"\n",
    "    Extract meaningful metadata from ECG header file lines\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # # Extract basic information from the first line\n",
    "    # if len(header_lines) > 0 and ' ' in header_lines[0]:\n",
    "    #     parts = header_lines[0].strip().split(' ')\n",
    "    #     if len(parts) >= 4:\n",
    "    #         metadata['record_name'] = parts[0]\n",
    "    #         metadata['n_channels'] = int(parts[1])\n",
    "    #         metadata['sample_rate'] = int(parts[2])\n",
    "    #         metadata['n_samples'] = int(parts[3])\n",
    "    \n",
    "    # Extract channel information\n",
    "    # channels = []\n",
    "    # for i in range(1, 13):  # Assuming 12-lead ECG\n",
    "    #     if i < len(header_lines) and '.dat' in header_lines[i]:\n",
    "    #         channel_parts = header_lines[i].strip().split(' ')\n",
    "    #         if len(channel_parts) >= 9:\n",
    "    #             channel = {\n",
    "    #                 'file': channel_parts[0],\n",
    "    #                 'format': channel_parts[1],\n",
    "    #                 'gain': channel_parts[2],\n",
    "    #                 'bits': channel_parts[3],\n",
    "    #                 'offset': channel_parts[4],\n",
    "    #                 'initial_value': channel_parts[5],\n",
    "    #                 'checksum': channel_parts[6],\n",
    "    #                 'block_size': channel_parts[7],\n",
    "    #                 'name': channel_parts[8]\n",
    "    #             }\n",
    "    #             channels.append(channel)\n",
    "    \n",
    "    # metadata['channels'] = channels\n",
    "    \n",
    "    # Extract comments and other metadata\n",
    "    for line in header_lines:\n",
    "        if line.startswith('#'):\n",
    "            line = line.strip('# \\n')\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                metadata[key.strip()] = value.strip()\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Extract metadata from the ECG header file\n",
    "ecg_metadata = extract_ecg_metadata(ecg_file_head_lines)\n",
    "ecg_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load ECG data from .dat file\n",
    "def load_ecg_data(file_path):\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"ECG file not found: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Import wfdb library for reading ECG data\n",
    "        import wfdb\n",
    "        \n",
    "        # Get the file path without extension\n",
    "        file_base = os.path.splitext(file_path)[0]\n",
    "        \n",
    "        # Read the ECG record\n",
    "        record = wfdb.rdrecord(file_base)\n",
    "        \n",
    "        # Extract data and metadata\n",
    "        ecg_data = {\n",
    "            'signal': record.p_signal,\n",
    "            'sample_rate': record.fs,\n",
    "            'channels': record.sig_name,\n",
    "            'units': record.units,\n",
    "            'patient_id': record.record_name,\n",
    "            'n_samples': record.sig_len,\n",
    "            'base_time': record.base_time,\n",
    "            'base_date': record.base_date\n",
    "        }\n",
    "        \n",
    "        print(f\"Successfully loaded ECG data with {ecg_data['n_samples']} samples\")\n",
    "        print(f\"Sample rate: {ecg_data['sample_rate']} Hz\")\n",
    "        print(f\"Channels: {ecg_data['channels']}\")\n",
    "        \n",
    "        return ecg_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ECG data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Try to load the ECG data\n",
    "ecg_data = load_ecg_data(ecg_file)\n",
    "\n",
    "# If successful, display a sample of the data\n",
    "if ecg_data is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot a small segment of the first channel\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    time_in_seconds = np.arange(min(1000, ecg_data['n_samples'])) / ecg_data['sample_rate']\n",
    "    plt.plot(time_in_seconds, ecg_data['signal'][:min(1000, ecg_data['n_samples']), 0])\n",
    "    plt.title(f\"ECG Data - Channel: {ecg_data['channels'][0]}\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(f\"Amplitude ({ecg_data['units'][0]})\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d3063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to display all ECG data in a more comprehensive way\n",
    "def display_ecg_data(ecg_data):\n",
    "    \"\"\"\n",
    "    Display all channels of ECG data in a multi-panel plot\n",
    "    \n",
    "    Args:\n",
    "        ecg_data (dict): Dictionary containing ECG data and metadata\n",
    "    \"\"\"\n",
    "    if ecg_data is None:\n",
    "        print(\"No ECG data to display\")\n",
    "        return\n",
    "    \n",
    "    # Get the number of channels\n",
    "    n_channels = len(ecg_data['channels'])\n",
    "    \n",
    "    # Create a figure with subplots arranged in a 6x2 grid\n",
    "    fig, axes = plt.subplots(6, 2, figsize=(15, 24))\n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each channel\n",
    "    for i, channel in enumerate(ecg_data['channels']):\n",
    "        if i < n_channels:  # Make sure we don't exceed the number of channels\n",
    "            # Calculate time in seconds for x-axis\n",
    "            time_in_seconds = np.arange(ecg_data['n_samples']) / ecg_data['sample_rate']\n",
    "            \n",
    "            # Plot the data\n",
    "            axes[i].plot(time_in_seconds, ecg_data['signal'][:, i])\n",
    "            axes[i].set_title(f\"Channel: {channel}\")\n",
    "            axes[i].set_ylabel(f\"Amplitude ({ecg_data['units'][i]})\")\n",
    "            axes[i].grid(True)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_channels, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add a common x-axis label\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display metadata\n",
    "    print(\"\\nECG Metadata:\")\n",
    "    print(f\"Patient ID: {ecg_data['patient_id']}\")\n",
    "    print(f\"Sample Rate: {ecg_data['sample_rate']} Hz\")\n",
    "    print(f\"Number of Samples: {ecg_data['n_samples']}\")\n",
    "    print(f\"Duration: {ecg_data['n_samples']/ecg_data['sample_rate']:.2f} seconds\")\n",
    "    print(f\"Base Time: {ecg_data['base_time']}\")\n",
    "    print(f\"Base Date: {ecg_data['base_date']}\")\n",
    "    print(f\"Channels: {', '.join(ecg_data['channels'])}\")\n",
    "\n",
    "# Display all ECG data\n",
    "display_ecg_data(ecg_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0923e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data['signal'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data#['signal']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c35d2",
   "metadata": {},
   "source": [
    "## process_Source_to_Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c84d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "def process_Source_to_Raw(OneCohort_Args, SourceFile_List, get_RawName_from_SourceFile,SPACE):\n",
    "    \n",
    "\n",
    "    RawName_to_dfRaw = {}\n",
    "\n",
    "\n",
    "    # ---------------------- Patient ----------------------\n",
    "    participants_xml_file = [i for i in SourceFile_List if 'participants' in i][0]\n",
    "    # print(participants_xml_file)\n",
    "\n",
    "    participants_tsv_file = participants_xml_file.replace('.json', '.tsv')\n",
    "\n",
    "    df = pd.read_csv(participants_tsv_file, sep='\\t')\n",
    "    df['YearOfBirth'] = pd.to_datetime(df['study_visit_date']).dt.year - df['age']\n",
    "    # df\n",
    "    # df['study_group'].value_counts()\n",
    "\n",
    "    clinical_site_to_timezone = {\n",
    "        'UAB': 'America/Chicago',\n",
    "        'UW': 'America/Los_Angeles',\n",
    "        'UCSD': 'America/Los_Angeles',\n",
    "    }\n",
    "\n",
    "    df['UserTimeZone'] = df['clinical_site'].map(clinical_site_to_timezone)\n",
    "\n",
    "\n",
    "\n",
    "    df['DiseaseType'] = df['study_group'].map({\n",
    "        'healthy': 0, \n",
    "        'pre_diabetes_lifestyle_controlled': 0.5, \n",
    "        'oral_medication_and_or_non_insulin_injectable_medication_controlled': 2, \n",
    "        'insulin_dependent': 2})\n",
    "\n",
    "    df['MRSegmentID'] = df['study_group']\n",
    "\n",
    "    RawName = 'Patient'\n",
    "    raw_columns = ['PatientID', \n",
    "                'MRSegmentID', 'MRSegmentModifiedDateTime', 'DiseaseType',\n",
    "                                'Gender', 'ActivationDate', 'UserTimeZoneOffset', 'UserTimeZone',\n",
    "                                'Description', 'YearOfBirth']\n",
    "    df = df.rename(columns = {'participant_id': 'PatientID'})\n",
    "    df = df.reindex(columns = raw_columns)\n",
    "    df['UserTimeZoneOffset'] = 0\n",
    "\n",
    "    # person_file = [i for i in SourceFile_List if 'person' in i][0]\n",
    "    # person_file\n",
    "\n",
    "    # df_person = pd.read_csv(person_file)\n",
    "    # df_person.head()\n",
    "\n",
    "    df_user = df\n",
    "    df_user['PatientID'] = 'AIREADI-' + df_user['PatientID'].astype(str)\n",
    "    RawName_to_dfRaw[RawName] = df_user\n",
    "    print(df_user.shape, df_user['PatientID'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------- CGM ----------------------\n",
    "    # all_cgm_data = []\n",
    "    # cgm_json_list = [i for i in SourceFile_List if 'wearable_blood_glucose' in i and  'json' in i]\n",
    "    # print(len(cgm_json_list))\n",
    "\n",
    "    # for file_path in cgm_json_list: # tqdm(, desc=\"Processing CGM files\"):\n",
    "    #     try:\n",
    "    #         # Extract patient ID from the file path\n",
    "    #         patient_id = os.path.basename(os.path.dirname(file_path))\n",
    "            \n",
    "    #         # Convert the JSON file to DataFrame\n",
    "    #         df = convert_cgm_json_to_df(file_path)\n",
    "            \n",
    "    #         # Add patient ID if not already in the DataFrame\n",
    "    #         if 'patient_id' not in df.columns:\n",
    "    #             df['patient_id'] = patient_id\n",
    "                \n",
    "    #         all_cgm_data.append(df)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    # final_cgm_df = pd.concat(all_cgm_data, ignore_index=True)\n",
    "\n",
    "    # RawName = 'ElogBGEntry'\n",
    "    # raw_columns = ['BGEntryID', 'PatientID', 'ObservationDateTime', \n",
    "    #                'BGValue',\n",
    "    #                 # 'IsNormalIndicator', \n",
    "    #                 'ObservationEntryDateTime', 'TimezoneOffset',\n",
    "    #                 'Timezone', 'EntryCreatedDateTime', # 'ActualBGValue',\n",
    "    #                 'ExternalSourceID', \n",
    "    #                 'UserObservationDateTime']\n",
    "\n",
    "    # df = final_cgm_df\n",
    "    # df = df.rename(columns = {'patient_id': 'PatientID', 'glucose_value': 'BGValue', 'timestamp': 'ObservationDateTime'})\n",
    "    # df = df.reindex(columns = raw_columns)\n",
    "    # df = pd.merge(df, df_user[['PatientID', 'UserTimeZone']], on = 'PatientID', how = 'left')\n",
    "    # df['BGEntryID'] = df.index\n",
    "    # df['TimezoneOffset'] = df.apply(lambda row: get_timezone_offset_minutes_corrected(\n",
    "    #                                                 row['ObservationDateTime'], \n",
    "    #                                                 row['UserTimeZone']), \n",
    "    #                                                 axis = 1)\n",
    "    \n",
    "    \n",
    "    # # df['TimezoneOffset'] = None\n",
    "    # df['ExternalSourceID'] = 18\n",
    "    \n",
    "    # df['ObservationDateTime'] = df['ObservationDateTime'].dt.tz_localize(None)\n",
    "    # df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "\n",
    "    # df['BGValue'] = pd.to_numeric(df['BGValue'], errors='coerce')\n",
    "    # df = df[df['BGValue'].notna()].reset_index(drop = True)\n",
    "\n",
    "    # df_cgm = df \n",
    "    # RawName_to_dfRaw[RawName] = df_cgm\n",
    "    # print('cgm patients', df_cgm.shape, df_cgm['PatientID'].nunique())\n",
    "    \n",
    "    \n",
    "    # ---------------------- hr ----------------------\n",
    "    all_hr_data = []\n",
    "    hr_json_list = [i for i in SourceFile_List if 'heart_rate' in i and  'json' in i]\n",
    "    print(len(hr_json_list))\n",
    "\n",
    "\n",
    "    for file_path in hr_json_list: # tqdm(, desc=\"Processing HR files\"):\n",
    "        try:\n",
    "            # Extract patient ID from the file path\n",
    "            patient_id = os.path.basename(os.path.dirname(file_path))\n",
    "            \n",
    "            # Convert the JSON file to DataFrame\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            df = convert_heart_rate_json_to_df(data)\n",
    "            \n",
    "            # Add patient ID if not already in the DataFrame\n",
    "            if 'patient_id' not in df.columns:\n",
    "                df['patient_id'] = patient_id\n",
    "                \n",
    "            all_hr_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    df = pd.concat(all_hr_data, ignore_index=True)\n",
    "\n",
    "    RawName = 'HeartRate'\n",
    "    #originalname: 'wearable_blood_glucose'\n",
    "    raw_columns =['HREntryID', 'PatientID', 'ObservationDateTime', 'HRValue','HRUnit',\n",
    "                'ObservationEntryDateTime', 'TimezoneOffset',\n",
    "                'Timezone', 'EntryCreatedDateTime'\n",
    "                ]\n",
    "    df = df.rename(columns = {'patient_id': 'PatientID', \n",
    "                            'timestamp': 'ObservationDateTime',\n",
    "                            'heart_rate': 'HRValue',\n",
    "                            'unit':'HRUnit',\n",
    "                            })\n",
    "    df = df.reindex(columns = raw_columns)\n",
    "    df['HREntryID'] = df.index\n",
    "    df = pd.merge(df, df_user[['PatientID', 'UserTimeZone']], on = 'PatientID', how = 'left') \n",
    "    df['TimezoneOffset'] = df.apply(lambda row: get_timezone_offset_minutes_corrected(\n",
    "                                                        row['ObservationDateTime'], \n",
    "                                                        row['UserTimeZone']), \n",
    "                                                        axis = 1)\n",
    "    df['ObservationDateTime'] = df['ObservationDateTime'].dt.tz_localize(None)\n",
    "    df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "\n",
    "    df['HRValue'] = pd.to_numeric(df['HRValue'], errors='coerce')\n",
    "    df = df[df['HRValue'].notna()].reset_index(drop = True)\n",
    "\n",
    "    df_hr = df \n",
    "    RawName_to_dfRaw[RawName] = df_hr\n",
    "    print('hr patients',df_hr.shape, df_hr['PatientID'].nunique())\n",
    "\n",
    "    # ---------------------- Save to CSV ----------------------\n",
    "    for RawName, df in RawName_to_dfRaw.items():\n",
    "        print(RawName, df.shape)\n",
    "        print(df.columns)\n",
    "        # display(df.head())\n",
    "\n",
    "        path = os.path.join(OneCohort_Args['FolderPath'], f'processed_RawFile_{RawName}.csv')\n",
    "        print(path)\n",
    "        df.to_csv(path, index=False)\n",
    "        RawName_to_dfRaw[RawName] = path# .replace(SPACE['DATA_RAW'], '$DATA_RAW$')\n",
    "\n",
    "    return RawName_to_dfRaw\n",
    "\n",
    "process_Source_to_Raw.fn_string = inspect.getsource(process_Source_to_Raw)\n",
    "# %%%%%%%%%%%%%%%%%%%%% user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RawName_to_dfRaw = process_Source_to_Raw(OneCohort_Args, SourceFile_List, get_RawName_from_SourceFile,SPACE)    \n",
    "RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113268f8",
   "metadata": {},
   "source": [
    "# Step 5: Save Cohort Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the python file path from the cohort object \n",
    "pypath = cohort.pypath\n",
    "pypath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa05e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = [\n",
    "    'import os',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np'\n",
    "    ]\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to be included in the generated script\n",
    "iterative_variables = [OneCohort_Args, SourceFile_SuffixList]\n",
    "iterative_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from recfldtkn.base import Base\n",
    "# fn_variables = [\n",
    "#     convert_cgm_json_to_df,\n",
    "#     get_RawName_from_SourceFile, \n",
    "#     process_Source_to_Raw,\n",
    "    \n",
    "#     ]\n",
    "# pycode= Base.convert_variables_to_pystirng(iterative_variables = iterative_variables, \n",
    "#                                            fn_variables = fn_variables, \n",
    "#                                            prefix = prefix)\n",
    "# # print(pycode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ded7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import Base \n",
    "# Get the python file path from the cohort object \n",
    "pypath = cohort.pypath\n",
    "\n",
    "# Define the import statements to be included at the begining\n",
    "prefix = [\n",
    "    'import os',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np',\n",
    "    ]\n",
    "# List of variables to be included in the generated script\n",
    "iterative_variables = [OneCohort_Args, SourceFile_SuffixList]\n",
    "# list of the funcitons to be included in the generated script\n",
    "fn_variables = [\n",
    "    convert_cgm_json_to_df,\n",
    "    get_timezone_offset_minutes_corrected,\n",
    "    get_RawName_from_SourceFile, \n",
    "    process_Source_to_Raw]\n",
    "pycode = Base.convert_variables_to_pystirng(iterative_variables = iterative_variables, \n",
    "                                           fn_variables = fn_variables, \n",
    "                                           prefix = prefix)\n",
    "# Create the directory for the Python file if it doesn't exist\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "\n",
    "# print(pypath)\n",
    "with open(pypath, 'w') as file: file.write(pycode)\n",
    "# Create a HTML link and display it\n",
    "full_path = os.path.join(WORKSPACE_PATH, pypath)\n",
    "\n",
    "display(HTML(f'{pypath} <a href=\"{full_path}\" target=\"_blank\">Open File</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f6938",
   "metadata": {},
   "source": [
    "# Step 6: Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25759a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import CohortFn, Cohort\n",
    "from config.config_record.Cohort import CohortName_to_OneCohortArgs\n",
    "CohortNames = [i for i in CohortName_to_OneCohortArgs.keys()]\n",
    "\n",
    "# # %%%%%%%%%%%%%%%%%%%%% user\n",
    "# CohortName = 'WellDoc2022CGM'\n",
    "# CohortName = 'WellDoc2023CVSDeRx'\n",
    "CohortName = 'aireadi-noimage-v2'\n",
    "# # %%%%%%%%%%%%%%%%%%%%% \n",
    "OneCohort_Args = CohortName_to_OneCohortArgs[CohortName]\n",
    "OneCohort_Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source2CohortName = OneCohort_Args['Source2CohortName']\n",
    "cohort_fn = CohortFn(Source2CohortName, SPACE)\n",
    "cohort_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = Cohort(OneCohort_Args, SPACE, cohort_fn)\n",
    "cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.setup_fn(cohort_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21804066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import CohortFn, Cohort\n",
    "\n",
    "Source2CohortName = OneCohort_Args['Source2CohortName']\n",
    "cohort_fn = CohortFn(Source2CohortName, SPACE)\n",
    "cohort = Cohort(OneCohort_Args, SPACE, cohort_fn)\n",
    "cohort.setup_fn(cohort_fn)\n",
    "\n",
    "cohort.initialize_cohort(load_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886af9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d180923",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.SourceFile_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fa5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77119b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14932f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alina_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}