{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab3f242",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80023bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "KEY = '1-WORKSPACE'\n",
    "WORKSPACE_PATH = os.getcwd().split(KEY)[0]\n",
    "print(WORKSPACE_PATH); os.chdir(WORKSPACE_PATH)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s:%(asctime)s:(%(filename)s@%(lineno)d %(name)s)]: %(message)s')\n",
    "\n",
    "SPACE = {\n",
    "    'DATA_RAW': f'./_Data/0-Data_Raw',\n",
    "    'DATA_RFT': f'./_Data/1-Data_RFT',\n",
    "    'DATA_CASE': f'./_Data/2-Data_CASE',\n",
    "    'DATA_AIDATA': f'./_Data/3-Data_AIDATA',\n",
    "    'DATA_EXTERNAL': f'./code/external',\n",
    "    'CODE_FN': f'./code/pipeline', \n",
    "}\n",
    "assert os.path.exists(SPACE['CODE_FN']), f'{SPACE[\"CODE_FN\"]} not found'\n",
    "\n",
    "print(SPACE['CODE_FN'])\n",
    "sys.path.append(SPACE['CODE_FN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704eb394",
   "metadata": {},
   "source": [
    "# Step 1: OneCohort_Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3ed49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This step is foundational for the rest of the notebook, as it ensures that the correct settings and parameters are in place for processing the cohort's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11053dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the cohort configuration dictionary\n",
    "from config.config_record.Cohort import CohortName_to_OneCohortArgs\n",
    "\n",
    "# List available cohort names\n",
    "cohort_names = list(CohortName_to_OneCohortArgs.keys())\n",
    "print(\"Available Cohorts:\", cohort_names)\n",
    "\n",
    "# Select a specific cohort and retrieve its arguments\n",
    "# selected_cohort = 'WellDoc2023CVSDeRx'\n",
    "selected_cohort = 'OhioT1DM'\n",
    "cohort_args = CohortName_to_OneCohortArgs[selected_cohort]\n",
    "print(\"Selected Cohort Arguments:\", cohort_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeaef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import Cohort   # Cohort is a class\n",
    "\n",
    "# Define a placeholder for a cohort funciton \n",
    "cohort_fn = None\n",
    "# Initialize an object of the Cohort class with initial arguments\n",
    "cohort = Cohort(OneCohort_Args, SPACE, cohort_fn)  # cohort is an object of class Cohort, this object  will manage dataset parameters\n",
    "# Update the cohort arguments using the update_cohort_args method\n",
    "# Input is OneCohort_Args and SPACE, which are two dictionaries and the \n",
    "OneCohort_Args = cohort.update_cohort_args(OneCohort_Args, SPACE) # update cohort args\n",
    "# Pretty print the updated cohort argument\n",
    "pprint(OneCohort_Args, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e47628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohort.RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfa716",
   "metadata": {},
   "source": [
    "# Step 2: Get Source Files\n",
    "The purpose of this code segment is to retrieve all files with a specific suffix (in this case, .csv) from a specified folder and list their paths. The folder path and file suffix list are specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "# Define file suffix list to look for .csv files\n",
    "SourceFile_SuffixList = ['xml'] \n",
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "\n",
    "# Get the folder path from OneCohort_Args dictionary (provided by the user)\n",
    "Folder = OneCohort_Args['SourcePath'] \n",
    "\n",
    "# Use the cohort object to get a list of source files with the specified suffix\n",
    "SourceFile_List = cohort.get_SourceFile_List(Folder, SourceFile_SuffixList)\n",
    "\n",
    "# Print the list of source files\n",
    "SourceFile_List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f4c0a",
   "metadata": {},
   "source": [
    "# Step 3: Get RawName from SourceFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "def get_RawName_from_SourceFile(file_path, OneCohort_Args):\n",
    "    \"\"\"\n",
    "    This one is useless \n",
    "    \"\"\"\n",
    "    RawName = file_path.split('_')[-1].split('.')[0]\n",
    "    return RawName\n",
    "\n",
    "get_RawName_from_SourceFile.fn_string = inspect.getsource(get_RawName_from_SourceFile)\n",
    "# %%%%%%%%%%%%%%%%%%%%% \n",
    "\n",
    "file_path = SourceFile_List[0]\n",
    "print(type(file_path))\n",
    "RawName = get_RawName_from_SourceFile(file_path, OneCohort_Args)\n",
    "print(file_path)\n",
    "print(RawName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SourceFile_List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c48a",
   "metadata": {},
   "source": [
    "# Step 4: Process Source to Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab5b3d",
   "metadata": {},
   "source": [
    "## 0. get tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb475e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "xml_file = SourceFile_List[0]\n",
    "\n",
    "\n",
    "# lession, should display the xml file content so the Cursor can understand the data structure\n",
    "# Parse the XML file\n",
    "with open(xml_file, 'r') as file:\n",
    "    xml_content = file.read()\n",
    "    data = xmltodict.parse(xml_content)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23626da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the structure of the XML data\n",
    "def analyze_xml_structure(data):\n",
    "    \"\"\"\n",
    "    Analyze the structure of the XML data to understand its organization and content.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The parsed XML data\n",
    "        \n",
    "    Returns:\n",
    "        dict: A summary of the data structure\n",
    "    \"\"\"\n",
    "    import xmltodict\n",
    "    structure = {}\n",
    "    \n",
    "    # Check if we have patient data\n",
    "    if 'patient' in data:\n",
    "        patient_data = data['patient']\n",
    "        structure['patient_attributes'] = [k for k in patient_data.keys() if k.startswith('@')]\n",
    "        structure['patient_sections'] = [k for k in patient_data.keys() if not k.startswith('@')]\n",
    "        \n",
    "        # Analyze each section in patient data\n",
    "        for section in structure['patient_sections']:\n",
    "            section_data = patient_data[section]\n",
    "            if isinstance(section_data, dict) and 'event' in section_data:\n",
    "                events = section_data['event']\n",
    "                if not isinstance(events, list):\n",
    "                    events = [events]\n",
    "                \n",
    "                # Get sample event to understand structure\n",
    "                if events:\n",
    "                    sample_event = events[0]\n",
    "                    structure[f'{section}_event_attributes'] = list(sample_event.keys())\n",
    "                    structure[f'{section}_event_count'] = len(events)\n",
    "    \n",
    "    # Print summary of the data structure\n",
    "    print(\"XML Data Structure Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if 'patient_attributes' in structure:\n",
    "        print(f\"Patient Attributes: {', '.join(structure['patient_attributes'])}\")\n",
    "    \n",
    "    if 'patient_sections' in structure:\n",
    "        print(f\"\\nPatient Sections: {', '.join(structure['patient_sections'])}\")\n",
    "        \n",
    "        for section in structure['patient_sections']:\n",
    "            if f'{section}_event_count' in structure:\n",
    "                print(f\"\\n  {section.capitalize()} Section:\")\n",
    "                print(f\"    - Events: {structure[f'{section}_event_count']}\")\n",
    "                print(f\"    - Event Attributes: {', '.join(structure[f'{section}_event_attributes'])}\")\n",
    "    \n",
    "    return structure\n",
    "\n",
    "# Execute the analysis\n",
    "# structure_summary = analyze_xml_structure(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "\n",
    "def convert_ohio_xml_to_dataframes(data, xml_path=None):\n",
    "    \"\"\"\n",
    "    Convert OhioT1DM XML data into pandas DataFrames for each section.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): The parsed XML data from OhioT1DM dataset\n",
    "        xml_path (str, optional): Path to the XML file, used to extract year and dataset type\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping section names to pandas DataFrames\n",
    "    \"\"\"\n",
    "    # import xmltodict\n",
    "\n",
    "\n",
    "    dataframes = {}\n",
    "    \n",
    "    if 'patient' not in data:\n",
    "        print(\"No patient data found in the XML\")\n",
    "        return dataframes\n",
    "    \n",
    "    patient = data['patient']\n",
    "    patient_id = patient.get('@id', 'unknown')\n",
    "    \n",
    "    # Extract patient attributes\n",
    "    patient_attrs = {k.replace('@', ''): v for k, v in patient.items() if k.startswith('@')}\n",
    "    \n",
    "    # Add year and dataset type (test/train) information from the file path if available\n",
    "    if xml_path:\n",
    "        # Extract year from the path using regex pattern matching\n",
    "        import re\n",
    "        year_match = re.search(r'/(\\d{4})/', xml_path)\n",
    "        if year_match:\n",
    "            patient_attrs['year'] = year_match.group(1)\n",
    "        else:\n",
    "            patient_attrs['year'] = 'unknown'\n",
    "            \n",
    "        # Extract dataset type (test or train)\n",
    "        if '/test/' in xml_path or '-testing' in xml_path:\n",
    "            patient_attrs['dataset_type'] = 'test'\n",
    "            patient_id = str(patient_id) + '_test'\n",
    "        elif '/train/' in xml_path or '-training' in xml_path:\n",
    "            patient_attrs['dataset_type'] = 'train'\n",
    "            patient_id = str(patient_id) + '_train'\n",
    "        else:\n",
    "            patient_attrs['dataset_type'] = 'unknown'\n",
    "            \n",
    "\n",
    "        patient_attrs['patient_id'] = patient_id\n",
    "        # Add the full file path for reference\n",
    "        patient_attrs['file_path'] = xml_path\n",
    "    \n",
    "    patient_df = pd.DataFrame([patient_attrs])\n",
    "    dataframes['patient_info'] = patient_df\n",
    "    \n",
    "    # Process each section\n",
    "    for section_name, section_data in patient.items():\n",
    "        if section_name.startswith('@') or not isinstance(section_data, dict):\n",
    "            continue\n",
    "            \n",
    "        if 'event' in section_data:\n",
    "            events = section_data['event']\n",
    "            if not isinstance(events, list):\n",
    "                events = [events]  # Convert single event to list\n",
    "                \n",
    "            # Extract all events into a list of dictionaries\n",
    "            events_list = []\n",
    "            for event in events:\n",
    "                event_dict = {k.replace('@', ''): v for k, v in event.items()}\n",
    "                event_dict['patient_id'] = patient_id  # Add patient ID to each event\n",
    "                events_list.append(event_dict)\n",
    "                \n",
    "            # Create DataFrame from events\n",
    "            if events_list:\n",
    "                section_df = pd.DataFrame(events_list)\n",
    "                \n",
    "                # Convert datetime-like columns to datetime\n",
    "                datetime_columns = ['ts', 'tbegin', 'tend']\n",
    "                for col in datetime_columns:\n",
    "                    if col in section_df.columns:\n",
    "                        section_df[col] = pd.to_datetime(section_df[col], format='%d-%m-%Y %H:%M:%S', errors='coerce')\n",
    "                \n",
    "                dataframes[section_name] = section_df\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "convert_ohio_xml_to_dataframes.fn_string = inspect.getsource(convert_ohio_xml_to_dataframes)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# if 'patient' in data:\n",
    "#     print(\"\\nConverting XML data to DataFrames...\")\n",
    "#     dfs = convert_ohio_xml_to_dataframes(data, xml_path=xml_file)\n",
    "    \n",
    "#     # Display summary of created DataFrames\n",
    "#     print(f\"\\nCreated {len(dfs)} DataFrames:\")\n",
    "#     for section_name, df in dfs.items():\n",
    "#         print(f\"  - {section_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "#         if not df.empty:\n",
    "#             print(f\"    Columns: {', '.join(df.columns.tolist())}\")\n",
    "#             print(f\"    Sample data (first 5 row):\")\n",
    "#             display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976790ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SourceFile_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all XML files and combine the data into consolidated DataFrames\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary to store combined DataFrames for each section\n",
    "combined_dfs = defaultdict(list)\n",
    "\n",
    "# print(\"Processing all XML files in the OhioT1DM dataset...\")\n",
    "for xml_file in SourceFile_List:\n",
    "    try:\n",
    "        # Parse XML file\n",
    "        # print(f\"Processing file: {os.path.basename(xml_file)}\")\n",
    "        with open(xml_file, 'r') as f:\n",
    "            xml_content = f.read()\n",
    "        \n",
    "        # Convert XML to dictionary using xmltodict\n",
    "        import xmltodict\n",
    "        data = xmltodict.parse(xml_content)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        if 'patient' in data:\n",
    "            patient_dfs = convert_ohio_xml_to_dataframes(data, xml_path=xml_file)\n",
    "            \n",
    "            # Add each DataFrame to the combined collection\n",
    "            for section_name, df in patient_dfs.items():\n",
    "                if not df.empty:\n",
    "                    combined_dfs[section_name].append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {xml_file}: {str(e)}\")\n",
    "\n",
    "# Concatenate all DataFrames for each section\n",
    "final_dfs = {}\n",
    "for section_name, df_list in combined_dfs.items():\n",
    "    if df_list:\n",
    "        final_dfs[section_name] = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"tableL: {section_name}\")\n",
    "        print(f\"{final_dfs[section_name].shape[0]} rows, \\t{final_dfs[section_name].shape[1]} columns, \\t patient unique number: {final_dfs[section_name]['patient_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b286687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in final_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91aa53",
   "metadata": {},
   "source": [
    "## 1. patient_info --> Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = final_dfs['patient_info']\n",
    "\n",
    "######### deal with the ElogBGEntry\n",
    "df = final_dfs['patient_info']\n",
    "\n",
    "\n",
    "RawName = 'Patient'\n",
    "raw_columns = ['PatientID', 'MRSegmentID', 'MRSegmentModifiedDateTime', 'DiseaseType',\n",
    "                             'Gender', 'ActivationDate', 'UserTimeZoneOffset', 'UserTimeZone',\n",
    "                             'Description', 'YearOfBirth']\n",
    "\n",
    "df = df.rename(columns = {'patient_id': 'PatientID'})\n",
    "df = df.reindex(columns = raw_columns)\n",
    "# df['BGEntryID'] = df.index\n",
    "df['DiseaseType'] = 1\n",
    "df['UserTimeZoneOffset'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbcfff",
   "metadata": {},
   "source": [
    "## 2. exercise --> ELogExerciseEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af933d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = final_dfs['patient_info']\n",
    "\n",
    "######### deal with the ElogBGEntry\n",
    "df = final_dfs['exercise']\n",
    "display(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "# print(df['type'].value_counts())\n",
    "\n",
    "RawName = 'ELogExerciseEntry'\n",
    "\n",
    "raw_columns = ['ExerciseEntryID', 'PatientID', 'EntryID',\n",
    "                'ExerciseDuration', 'ExerciseType', 'ExerciseIntensity',\n",
    "                'TimeSinceExercise', 'EntrySourceID', 'ActivityTypeID',\n",
    "                'ObservationDateTime', 'ObservationEntryDateTime',\n",
    "                'TimezoneOffset', 'Timezone', 'EntryCreatedDateTime',\n",
    "                'ObservationCreatedBy', 'ObservationStatus',\n",
    "                'SourceReferenceID', 'ModifiedDateTime', 'CaloriesBurned',\n",
    "                'DistanceInMeters', 'ExternalEntryID', 'ExternalSourceID']\n",
    "\n",
    "df = df.rename(columns = {\n",
    "    'patient_id': 'PatientID', \n",
    "    'carbs': 'CarbsValue',\n",
    "    'ts': 'ObservationDateTime',\n",
    "    'intensity': 'ExerciseIntensity', \n",
    "    'duration': 'ExerciseDuration',\n",
    "})\n",
    "\n",
    "df = df.reindex(columns = raw_columns)\n",
    "df['ExerciseEntryID'] = df.index\n",
    "df['ObservationEntryDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "# df['DiseaseType'] = 1\n",
    "df['TimezoneOffset'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ffd90",
   "metadata": {},
   "source": [
    "## 3. meal --> ELogCarbsEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = final_dfs['patient_info']\n",
    "\n",
    "######### deal with the ElogBGEntry\n",
    "df = final_dfs['meal']\n",
    "print(df.columns)\n",
    "display(df.head())\n",
    "\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "\n",
    "# TODO: check with Abhi for the Type2ActivityID, where is Breakfast, Lunch, Dinner, etc.\n",
    "Type2ActivityID_string = '''\n",
    "BeforeBreakFast = 1,\n",
    "AfterBreakFast = 2,\n",
    "BeforeLunch = 3,\n",
    "AfterLunch = 4,\n",
    "BeforeDinner = 5,\n",
    "AfterDinner = 6,\n",
    "Bedtime = 7,\n",
    "BeforeExercise = 8,\n",
    "AfterExercise = 9,\n",
    "Snack = 12,\n",
    "Fasting = 14,\n",
    "JustChecking = 31,\n",
    "'''\n",
    "Type2ActivityID = {i.split('=')[0].strip(): int(i.split('=')[1]) for i in Type2ActivityID_string.split(',\\n') if '='  in i}\n",
    "# Type2ActivityID\n",
    "\n",
    "\n",
    "RawName = 'ELogCarbsEntry'\n",
    "raw_columns = ['PatientID', 'CarbsEntryID', 'EntryID', 'CarbsValue',\n",
    "                'EntrySourceID', 'ActivityTypeID', 'ObservationDateTime',\n",
    "                'ObservationEntryDateTime', 'TimezoneOffset', 'Timezone',\n",
    "                'EntryCreatedDateTime', 'ObservationCreatedBy',\n",
    "                'ObservationStatus', 'SourceReferenceID', 'ModifiedDateTime',\n",
    "                'ExternalSourceID', 'ExternalEntryID', 'TotalCalories']\n",
    "\n",
    "df = df.rename(columns = {\n",
    "    'patient_id': 'PatientID', \n",
    "    'carbs': 'CarbsValue',\n",
    "    'ts': 'ObservationDateTime',\n",
    "})\n",
    "df = df.reindex(columns = raw_columns)\n",
    "df['CarbsEntryID'] = df.index\n",
    "df['ObservationDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "# df['DiseaseType'] = 1\n",
    "df['TimezoneOffset'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc2a3a",
   "metadata": {},
   "source": [
    "## 4. glucose_level --> ElogBGEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### deal with the ElogBGEntry\n",
    "df = final_dfs['glucose_level']\n",
    "\n",
    "\n",
    "RawName = 'ElogBGEntry'\n",
    "raw_columns = ['BGEntryID', 'PatientID', 'ObservationDateTime', 'BGValue',\n",
    "                                 'IsNormalIndicator', 'ObservationEntryDateTime', 'TimezoneOffset',\n",
    "                                 'Timezone', 'EntryCreatedDateTime', 'ActualBGValue',\n",
    "                                 'ExternalSourceID', 'UserObservationDateTime']\n",
    "df = df.rename(columns = {'patient_id': 'PatientID', 'value': 'BGValue', 'ts': 'ObservationDateTime'})\n",
    "df = df.reindex(columns = raw_columns)\n",
    "df['BGEntryID'] = df.index\n",
    "df['TimezoneOffset'] = 0 \n",
    "df['ExternalSourceID'] = 18\n",
    "df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316614e6",
   "metadata": {},
   "source": [
    "## process_Source_to_Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c84d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "def process_Source_to_Raw(OneCohort_Args, SourceFile_List, get_RawName_from_SourceFile,SPACE):\n",
    "    \"\"\"\n",
    "    Process source files to raw data files, including renaming columns and merging certain files.\n",
    "\n",
    "    Args:\n",
    "    OneCohort_Args (dict): Dictionary containing processing arguments, including 'FolderPath'.\n",
    "    SourceFile_List (list): List of source file paths.\n",
    "    get_RawName_from_SourceFile (function): Function to extract raw name from file path.\n",
    "\n",
    "    Returns:\n",
    "    dict: Mapping of raw names to processed file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "\n",
    "    # Initialize dictionary to store raw names and their corresponding file paths\n",
    "    RawName_to_dfRaw = {}\n",
    "    # for file_path in SourceFile_List:\n",
    "    #     # Extract the raw name for each file using the function \n",
    "    #     RawName = get_RawName_from_SourceFile(file_path, OneCohort_Args)\n",
    "    #     # Assign value file_path to key RawName\n",
    "    #     RawName_to_dfRaw[RawName] = file_path\n",
    "\n",
    "    # import xmltodict\n",
    "\n",
    "    combined_dfs = defaultdict(list)\n",
    "\n",
    "    # print(\"Processing all XML files in the OhioT1DM dataset...\")\n",
    "    for xml_file in SourceFile_List:\n",
    "        try:\n",
    "            # Parse XML file\n",
    "            # print(f\"Processing file: {os.path.basename(xml_file)}\")\n",
    "            with open(xml_file, 'r') as f:\n",
    "                xml_content = f.read()\n",
    "            \n",
    "            # Convert XML to dictionary using xmltodict\n",
    "            import xmltodict\n",
    "            data = xmltodict.parse(xml_content)\n",
    "            \n",
    "            # Convert to DataFrames\n",
    "            if 'patient' in data:\n",
    "                patient_dfs = convert_ohio_xml_to_dataframes(data, xml_path=xml_file)\n",
    "                \n",
    "                # Add each DataFrame to the combined collection\n",
    "                for section_name, df in patient_dfs.items():\n",
    "                    if not df.empty:\n",
    "                        combined_dfs[section_name].append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {xml_file}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "    final_dfs = {}\n",
    "    for section_name, df_list in combined_dfs.items():\n",
    "        if df_list:\n",
    "            final_dfs[section_name] = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- process patient_info --> Patient\n",
    "    df = final_dfs['patient_info']\n",
    "    RawName = 'Patient'\n",
    "    raw_columns = ['PatientID', 'MRSegmentID', 'MRSegmentModifiedDateTime', 'DiseaseType',\n",
    "                                'Gender', 'ActivationDate', 'UserTimeZoneOffset', 'UserTimeZone',\n",
    "                                'Description', 'YearOfBirth']\n",
    "\n",
    "    df = df.rename(columns = {'patient_id': 'PatientID'})\n",
    "    df = df.reindex(columns = raw_columns)\n",
    "    # df['BGEntryID'] = df.index\n",
    "    df['DiseaseType'] = 1\n",
    "    df['UserTimeZoneOffset'] = 0\n",
    "    RawName_to_dfRaw[RawName] = df \n",
    "\n",
    "\n",
    "\n",
    "    # ---------- process exercise --> ELogExerciseEntry\n",
    "    df = final_dfs['exercise']\n",
    "    # print(df['type'].value_counts())\n",
    "\n",
    "    RawName = 'ELogExerciseEntry'\n",
    "\n",
    "    raw_columns = ['ExerciseEntryID', 'PatientID', 'EntryID',\n",
    "                    'ExerciseDuration', 'ExerciseType', 'ExerciseIntensity',\n",
    "                    'TimeSinceExercise', 'EntrySourceID', 'ActivityTypeID',\n",
    "                    'ObservationDateTime', 'ObservationEntryDateTime',\n",
    "                    'TimezoneOffset', 'Timezone', 'EntryCreatedDateTime',\n",
    "                    'ObservationCreatedBy', 'ObservationStatus',\n",
    "                    'SourceReferenceID', 'ModifiedDateTime', 'CaloriesBurned',\n",
    "                    'DistanceInMeters', 'ExternalEntryID', 'ExternalSourceID']\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'patient_id': 'PatientID', \n",
    "        'carbs': 'CarbsValue',\n",
    "        'ts': 'ObservationDateTime',\n",
    "        'intensity': 'ExerciseIntensity', \n",
    "        'duration': 'ExerciseDuration',\n",
    "    })\n",
    "\n",
    "    df = df.reindex(columns = raw_columns)\n",
    "    df['ExerciseEntryID'] = df.index\n",
    "    # df['DiseaseType'] = 1\n",
    "    df['TimezoneOffset'] = 0\n",
    "    df['ObservationEntryDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "    # df.head()\n",
    "    RawName_to_dfRaw[RawName] = df \n",
    "\n",
    "\n",
    "\n",
    "    # ---------- process meal --> ELogCarbsEntry\n",
    "    # df = final_dfs['patient_info']\n",
    "\n",
    "    ######### deal with the ElogBGEntry\n",
    "    df = final_dfs['meal']\n",
    "    # print(df.columns)\n",
    "    # display(df.head())\n",
    "    # print(df['type'].value_counts())\n",
    "    # TODO: check with Abhi for the Type2ActivityID, where is Breakfast, Lunch, Dinner, etc.\n",
    "\n",
    "    # Type2ActivityID_string = '''\n",
    "    # BeforeBreakFast = 1,\n",
    "    # AfterBreakFast = 2,\n",
    "    # BeforeLunch = 3,\n",
    "    # AfterLunch = 4,\n",
    "    # BeforeDinner = 5,\n",
    "    # AfterDinner = 6,\n",
    "    # Bedtime = 7,\n",
    "    # BeforeExercise = 8,\n",
    "    # AfterExercise = 9,\n",
    "    # Snack = 12,\n",
    "    # Fasting = 14,\n",
    "    # JustChecking = 31,\n",
    "    # '''\n",
    "    # Type2ActivityID = {i.split('=')[0].strip(): int(i.split('=')[1]) for i in Type2ActivityID_string.split(',\\n') if '='  in i}\n",
    "    # Type2ActivityID\n",
    "\n",
    "\n",
    "    RawName = 'ELogCarbsEntry'\n",
    "    raw_columns = ['PatientID', 'CarbsEntryID', 'EntryID', 'CarbsValue',\n",
    "                    'EntrySourceID', 'ActivityTypeID', 'ObservationDateTime',\n",
    "                    'ObservationEntryDateTime', 'TimezoneOffset', 'Timezone',\n",
    "                    'EntryCreatedDateTime', 'ObservationCreatedBy',\n",
    "                    'ObservationStatus', 'SourceReferenceID', 'ModifiedDateTime',\n",
    "                    'ExternalSourceID', 'ExternalEntryID', 'TotalCalories']\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'patient_id': 'PatientID', \n",
    "        'carbs': 'CarbsValue',\n",
    "        'ts': 'ObservationDateTime',\n",
    "    })\n",
    "    df = df.reindex(columns = raw_columns)\n",
    "    df['CarbsEntryID'] = df.index\n",
    "    # df['DiseaseType'] = 1\n",
    "    df['TimezoneOffset'] = 0\n",
    "    df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "    # df.head()\n",
    "    RawName_to_dfRaw[RawName] = df \n",
    "\n",
    "\n",
    "    # ---------- process glucose_level --> ElogBGEntry\n",
    "    df = final_dfs['glucose_level']\n",
    "    RawName = 'ElogBGEntry'\n",
    "    raw_columns = ['BGEntryID', 'PatientID', 'ObservationDateTime', 'BGValue',\n",
    "                                    'IsNormalIndicator', 'ObservationEntryDateTime', 'TimezoneOffset',\n",
    "                                    'Timezone', 'EntryCreatedDateTime', 'ActualBGValue',\n",
    "                                    'ExternalSourceID', 'UserObservationDateTime']\n",
    "    df = df.rename(columns = {'patient_id': 'PatientID', 'value': 'BGValue', 'ts': 'ObservationDateTime'})\n",
    "    df = df.reindex(columns = raw_columns)\n",
    "    df['BGEntryID'] = df.index\n",
    "    df['TimezoneOffset'] = 0 \n",
    "    df['ExternalSourceID'] = 18\n",
    "    df['EntryCreatedDateTime'] = pd.to_datetime(df['ObservationDateTime'])\n",
    "    RawName_to_dfRaw[RawName] = df\n",
    "\n",
    "    for RawName, df in RawName_to_dfRaw.items():\n",
    "        print(RawName, df.shape)\n",
    "        print(df.columns)\n",
    "        # display(df.head())\n",
    "\n",
    "        path = os.path.join(OneCohort_Args['FolderPath'], f'processed_RawFile_{RawName}.csv')\n",
    "        df.to_csv(path, index=False)\n",
    "        RawName_to_dfRaw[RawName] = path# .replace(SPACE['DATA_RAW'], '$DATA_RAW$')\n",
    "        \n",
    "    return RawName_to_dfRaw\n",
    "\n",
    "process_Source_to_Raw.fn_string = inspect.getsource(process_Source_to_Raw)\n",
    "# %%%%%%%%%%%%%%%%%%%%% user\n",
    "\n",
    "RawName_to_dfRaw = process_Source_to_Raw(OneCohort_Args, SourceFile_List, get_RawName_from_SourceFile,SPACE)    \n",
    "RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113268f8",
   "metadata": {},
   "source": [
    "# Step 5: Save Cohort Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the python file path from the cohort object \n",
    "pypath = cohort.pypath\n",
    "pypath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa05e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = [\n",
    "    'import os',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np', \n",
    "    'import json'\n",
    "    ]\n",
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to be included in the generated script\n",
    "iterative_variables = [OneCohort_Args, SourceFile_SuffixList]\n",
    "iterative_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import Base\n",
    "fn_variables = [get_RawName_from_SourceFile, process_Source_to_Raw]\n",
    "pycode= Base.convert_variables_to_pystirng(iterative_variables = iterative_variables, \n",
    "                                           fn_variables = fn_variables, \n",
    "                                           prefix = prefix)\n",
    "# print(pycode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pypath, 'w') as file: file.write(pycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ded7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.base import Base \n",
    "# Get the python file path from the cohort object \n",
    "pypath = cohort.pypath\n",
    "\n",
    "# Define the import statements to be included at the begining\n",
    "prefix = [\n",
    "    'import os',\n",
    "    'import pandas as pd', \n",
    "    'import numpy as np'\n",
    "    ]\n",
    "# List of variables to be included in the generated script\n",
    "iterative_variables = [OneCohort_Args, SourceFile_SuffixList]\n",
    "# list of the funcitons to be included in the generated script\n",
    "fn_variables = [\n",
    "    convert_ohio_xml_to_dataframes,\n",
    "    get_RawName_from_SourceFile, \n",
    "    process_Source_to_Raw\n",
    "    ]\n",
    "pycode = Base.convert_variables_to_pystirng(iterative_variables = iterative_variables, \n",
    "                                           fn_variables = fn_variables, \n",
    "                                           prefix = prefix)\n",
    "# Create the directory for the Python file if it doesn't exist\n",
    "if not os.path.exists(os.path.dirname(pypath)): os.makedirs(os.path.dirname(pypath))\n",
    "\n",
    "# print(pypath)\n",
    "with open(pypath, 'w') as file: file.write(pycode)\n",
    "# Create a HTML link and display it\n",
    "full_path = os.path.join(WORKSPACE_PATH, pypath)\n",
    "\n",
    "display(HTML(f'{pypath} <a href=\"{full_path}\" target=\"_blank\">Open File</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f6938",
   "metadata": {},
   "source": [
    "# Step 6: Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import CohortFn, Cohort\n",
    "from config.config_record.Cohort import CohortName_to_OneCohortArgs\n",
    "CohortNames = [i for i in CohortName_to_OneCohortArgs.keys()]\n",
    "CohortNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25759a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%%%%%%%%%%%%%%%%%%%% user\n",
    "CohortName = 'OhioT1DM'\n",
    "# CohortName = 'WellDoc2023CVSDeRx'\n",
    "# # %%%%%%%%%%%%%%%%%%%%% \n",
    "\n",
    "OneCohort_Args = CohortName_to_OneCohortArgs[CohortName]\n",
    "OneCohort_Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source2CohortName = OneCohort_Args['Source2CohortName']\n",
    "cohort_fn = CohortFn(Source2CohortName, SPACE)\n",
    "cohort_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = Cohort(OneCohort_Args, SPACE, cohort_fn)\n",
    "cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.setup_fn(cohort_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21804066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recfldtkn.record_base.cohort import CohortFn, Cohort\n",
    "\n",
    "\n",
    "Source2CohortName = OneCohort_Args['Source2CohortName']\n",
    "cohort_fn = CohortFn(Source2CohortName, SPACE)\n",
    "cohort = Cohort(OneCohort_Args, SPACE, cohort_fn)\n",
    "cohort.setup_fn(cohort_fn)\n",
    "\n",
    "cohort.initialize_cohort(load_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d180923",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.SourceFile_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fa5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.RawName_to_dfRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77119b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14932f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}